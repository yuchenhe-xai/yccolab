{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/QdAaagVkUhaB0HYEc3BS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuchenhe-xai/yccolab/blob/main/241113_scoring_data_distribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scores for data learnability\n",
        "As discussed in [doc](https://docs.google.com/document/d/1msVlIjZwDUNpwalNuJ60P-9ieSINEVsC5ItyIBPUHPk/edit?tab=t.0#heading=h.y24asf17gdl1) - we want to find the data distributions that worth labeling and RL against. Besides matching the distribution based on clustering and categories, here we are defining a score approximating the learnability. Our assumption is that examples with higher score is more learnable - aka policy would differs before and after training - model could learn from it.\n",
        "\n",
        "Here are 3 different options, note ${a}$ is the LLM token output, $p = p_\\text{target}(a|s)$ is the target policy after RL, $q = p_\\text{ref}(a|s)$ is the original SFT policy.\n",
        "- importance weight (similar as in ppo's off-policy importance weight) : $s = p/q$\n",
        "- cross entropy: $s = - p \\log q$\n",
        "- kl divergence: $s = p \\log (p/q)$\n",
        "\n",
        "Here we wrap this into a reward client to be used downstream."
      ],
      "metadata": {
        "id": "kcwvYJgR3Exa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title imports\n",
        "\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "from enum import Enum\n",
        "from tqdm import tqdm\n",
        "from xlm.config import Config\n",
        "from xlm.config import configclass\n",
        "from xlm.posttrain import utils\n",
        "from xlm.posttrain.data import formatting\n",
        "from xlm.sampling_client import SamplingClient\n",
        "from xlm.tokenizers.constants import get_tokenizer, Tokenizer\n"
      ],
      "metadata": {
        "id": "y0rld9kB-h6w"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3qyQIn328Z5",
        "outputId": "c8a7686b-2a39-4ae6-9e20-1bdd5a663d89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score_type': 'KL',\n",
              " 'gap_score': 0.0001923118433048286,\n",
              " 'target_score': 0.00025645714958583686,\n",
              " 'ref_score': 6.419461970451348e-05}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#@title define scorer\n",
        "DEFAULT_TOKENIZER = get_tokenizer(\"v4\")\n",
        "SFT_MODEL = \"v5l-1010-sft-rubrics-sglang.yuchen.svc.max.x.ai\"\n",
        "POLICY_MODEL = \"v5l-inf-mb-sglang.yuchen.svc.max.x.ai\"\n",
        "\n",
        "class ScoreType(Enum):\n",
        "  P = 0\n",
        "  CE = 1\n",
        "  KL = 2\n",
        "\n",
        "@configclass\n",
        "class RelativeScorer(Config):\n",
        "  tokenizer: Tokenizer = DEFAULT_TOKENIZER\n",
        "  timeout: int = 100\n",
        "  model_addr: str = POLICY_MODEL\n",
        "  model_addr_ref: str = SFT_MODEL\n",
        "  score_type : ScoreType = ScoreType.KL\n",
        "  max_parallel: int = 1024\n",
        "  semaphore = asyncio.Semaphore(value=max_parallel)\n",
        "\n",
        "  async def get_logprob(self, prompt: str, response: str, model_addr: str = None) -> float:\n",
        "      \"\"\"Compute the log probabilities for response\"\"\"\n",
        "      prompt_tokens = self.tokenizer.tokenize_no_eos(prompt)\n",
        "      suffix_prob = np.inf\n",
        "      async with self.semaphore:\n",
        "        async with aiohttp.ClientSession(\n",
        "            timeout=aiohttp.ClientTimeout(total=self.timeout)\n",
        "        ) as session:\n",
        "          logp_response = await session.post(\n",
        "              f\"http://{model_addr}:30000/generate\",\n",
        "              json={\n",
        "                  \"text\": prompt + response,\n",
        "                  \"sampling_params\": {\"temperature\": 0, \"max_new_tokens\": 0},\n",
        "                  \"return_logprob\": True,\n",
        "                  \"logprob_start_len\": len(prompt_tokens) - 1,\n",
        "              },\n",
        "          )\n",
        "          response_json = await logp_response.json()\n",
        "          suffix_logprobs = response_json[\"meta_info\"][\"input_token_logprobs\"]\n",
        "          logprobs = [ri[0] for ri in suffix_logprobs]\n",
        "          suffix_prob = np.exp(np.mean(logprobs)).item()\n",
        "      return suffix_prob\n",
        "\n",
        "  async def get_scores(self, prompt: str, response: str):\n",
        "      target_score = await self.get_logprob(prompt, response, model_addr=self.model_addr)\n",
        "      ref_score = await self.get_logprob(prompt, response, model_addr=self.model_addr_ref)\n",
        "      gap_score = None\n",
        "      match self.score_type :\n",
        "        case ScoreType.P:\n",
        "          gap_score = np.exp(target_score - ref_score)\n",
        "        case ScoreType.CE:\n",
        "          gap_score =  - target_score * np.exp(ref_score)\n",
        "        case ScoreType.KL:\n",
        "          gap_score = np.exp(target_score) * (target_score - ref_score)\n",
        "      scores = {\"score_type\": self.score_type.name, \"gap_score\": gap_score, \"target_score\": target_score, \"ref_score\": ref_score}\n",
        "      # we cares larger gap - more negative the more learning\n",
        "      return scores\n",
        "\n",
        "scorer = RelativeScorer()\n",
        "await scorer.get_scores(prompt=\"asdfs\", response=\"dx\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title a new reward client that does scoring\n",
        "from xlm.reward_client import RewardClient\n",
        "from dataclasses import field\n",
        "\n",
        "@configclass\n",
        "class RelativeRewardClient(RewardClient):\n",
        "    model_addr: str = POLICY_MODEL\n",
        "    model_addr_ref: str = SFT_MODEL\n",
        "    score_type: ScoreType = ScoreType.KL\n",
        "    scorer: RelativeScorer = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "\n",
        "        # if response is not provided or we want to use a specific model response, we set up the sampling client for it\n",
        "        if self.address.endswith(\"/\"):\n",
        "          self.address = self.address[:-1]\n",
        "        self.sampling_client = SamplingClient()\n",
        "        self.model = self.address.split(\"/\")[0]\n",
        "        self.sampling_client._grok_client.register_model(\n",
        "           model=self.model, model_address=self.address, max_parallel=self.max_parallel\n",
        "        )\n",
        "        self.semaphore = asyncio.Semaphore(value=self.max_parallel)\n",
        "        if self.scorer is None:\n",
        "          self.scorer = RelativeScorer(\n",
        "              model_addr=self.model_addr, model_addr_ref=self.model_addr_ref, score_type=self.score_type\n",
        "          )\n",
        "\n",
        "\n",
        "    async def _generate(\n",
        "        self, messages: list[dict[str, str]] = None, example: dict[str, Any] = None\n",
        "    ) -> dict[str, Any]:\n",
        "        try:\n",
        "          async with self.semaphore:\n",
        "            prompt = example.get(\"prompt\", None)\n",
        "            response = example.get(\"response\", None)\n",
        "            if messages is not None:\n",
        "              if messages[-1][\"role\"] in [\"ASSIS\", \"assistant\"]:\n",
        "                  prompt = formatting.render_conversation(\n",
        "                      name=\"grok\", messages=messages[:-1]\n",
        "                  )\n",
        "                  response = messages[-1][\"content\"]\n",
        "              else:\n",
        "                  prompt = formatting.render_conversation(\n",
        "                      name=\"grok\", messages=messages\n",
        "                  )\n",
        "            if response is None:\n",
        "              response = await self.sampling_client.generate(prompt=prompt)\n",
        "            scores = await self.scorer.get_scores(prompt=prompt, response=response)\n",
        "            example[\"scores\"] = scores\n",
        "            return scores\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n"
      ],
      "metadata": {
        "id": "tQv9deQ164rP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Human: You will classify what genre each book is based on its summary.  When answering give both the genre name and a short (1-2 sentences) justification for why it is that genre.  If a book could be classified as multiple genres pick the one that fits the best. Here are the genres:\n",
        "\n",
        "[Fantasy]: A genre of imaginative fiction involving magic and adventure, especially in a setting other than the real world\n",
        "[Science Fiction]: A form of fiction that deals principally with the impact of actual or imagined science upon society or individuals.\n",
        "[Dystopian]: A popular genre of science fiction, dystopian novels offer a bleak and frightening vision of the future.\n",
        "[Action & Adventure]: The protagonist has a very important goal to achieve, but they’re going to have to go through the wringer first! The hero experiences obstacle after obstacle and goes through downright dangerous situations but eventually, they triumph and return home transformed.\n",
        "[Mystery]: Also called detective fiction, this book genre is characterized by a gripping plot that revolves around a mystery.\n",
        " [Historical Fiction]: This book genre encompasses fictional stories in a historical setting, carefully balancing creativity and facts. In most cases, the characters and events are imagined by the author and enriched with historically accurate details from a specific time period.\n",
        "[Romance]:  The key thing to remember is that the romantic relationship must be the center point of the plot. (Other giveaways include a “happily ever after” ending and the warm fuzzies.)\n",
        "[Contemporary Fiction]: This book genre is occasionally lumped in with others to indicate that the book takes place in the present day. But in its simplest form, contemporary fiction is better understood as the absence of a genre. Your book doesn’t need tropes and trappings, monsters and mysteries, when its tension, drama, and conflict lies in the quirks and quandaries of your protagonist’s everyday life: work, politics, relationships, and the struggles of the modern era.\n",
        "[Literary Fiction]: Like contemporary fiction, books considered literary fiction can’t be neatly filed under any other genre. What distinguishes this genre from contemporary fiction is that works of literary fiction are thought to have considerable artistic value. If your prose is meant to engage the reader in thought, if your narrative is character-driven and introspective, and if you provide personal or social commentary on a “serious” theme, then chances are you’re writing lit-fic\n",
        "\n",
        "Your output should consist of the name of the genre in bold followed by the 1-2 sentence justification for why the book is considered to be that genre.\n",
        "\n",
        "Classify this book based on the summary:\n",
        "\n",
        "Crown of Secrets by Melanie Cellier\n",
        "Verene is a disappointment to her entire kingdom--the first royal ever born without power, despite her mother being the most powerful mage in history. So when she's sent to the Academy in neighboring Kallorway to forge ties with her people's traditional enemies, she's determined to succeed and prove she can still be of value to her kingdom.\n",
        "\n",
        "Prince Darius of Kallorway is the strongest mage in his family--and the only reason his weak father is still clinging to his throne. Starting at the Academy at the same time as Verene, the crown prince is cold and distant and shows no desire to connect with her. Instead he seems suspicious of both her presence and her claimed lack of power.\n",
        "\n",
        "Surrounded by unfamiliar politics and long-held enemies, Verene discovers that some at the Academy want her gone by whatever means necessary. As the threats grow ever more sinister, she starts to question all of her assumptions. The hardened prince might just be her best hope of survival and--even more shockingly--he might be right about her power. If Verene wants to survive Kallorway and the Academy, she must uncover her hidden powers and take her true place among the mages.<|separator|>\n",
        "\n",
        "Assistant:\"\"\".strip()\n",
        "response = \"This book is classified as fantasy because it involves a world with magic and mages, which is a classic element of fantasy settings, and the plot revolves around magical powers and political intrigue in an imagined kingdom.\"\n",
        "example = {\"prompt\": prompt, \"response\": response}\n",
        "\n",
        "rr = RelativeRewardClient(address=\"v5l-1101-crm.yuchen.svc.max.x.ai\")\n",
        "await rr._generate(example=example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLayEhGvAVve",
        "outputId": "ee09f2ab-e22d-4b8c-dc0f-ec00e3dbded8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-11-14 19:21:24,078 E] \u001b[2;36m[colabbox-0:1618030] sampling_client:1010:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score_type': 'KL',\n",
              " 'gap_score': 0.007686059452784876,\n",
              " 'target_score': 0.16672462110759176,\n",
              " 'ref_score': 0.1602188892913625}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title example of wrapping the scoring into a data processing pipeline\n",
        "\n",
        "\n",
        "# for new policy answer - if we dont have a model generated response yet\n",
        "DEFAULT_MODEL = \"v5l-1101-crm.yuchen.svc.max.x.ai\"\n",
        "\n",
        "sampling_client = SamplingClient()\n",
        "sampling_client._grok_client.register_model(\n",
        "    model=DEFAULT_MODEL, model_address=DEFAULT_MODEL\n",
        ")\n",
        "scorer = RelativeScorer()\n",
        "\n",
        "async def process_convo(\n",
        "    input_path, output_path, sampling_client = None, semaphore = None, pbar=None, scorer: RelativeScorer = scorer\n",
        "):\n",
        "    df1 = utils.read_df(glob.glob(input_path))\n",
        "    row_pbar = tqdm(total=len(df1), desc=f\"Processing {input_path}\", leave=False)\n",
        "    async def process_row(example):\n",
        "        example = example.to_dict() if not isinstance(example, dict) else example\n",
        "        reward = None\n",
        "        conv = example.get('conversation0')[:-1]\n",
        "        prompt = formatting.render_conversation(name=\"grok\", messages=conv) if conv is not None else None\n",
        "        if sampling_client:\n",
        "          response = sampling_client.generate(prompt=prompt)\n",
        "        else:\n",
        "          model_id_0 = example.get(\"model_0_id\")\n",
        "          model_id_1 =  example.get(\"model_1_id\")\n",
        "          if \"grok\" in model_id_0 and \"grok\" in model_id_1:\n",
        "            chosen_model = 0 if example.get(\"model_0_id\") > example.get(\"model_1_id\") else 1\n",
        "          elif \"grok\" in model_id_0:\n",
        "            chosen_model = 0\n",
        "          elif \"grok\" in model_id_1:\n",
        "            chosen_model = 1\n",
        "          else:\n",
        "            chosen_model = 0\n",
        "          response = example.get(f'conversation{chosen_model}', [])[-1][\"content\"]\n",
        "        scores = await scorer.get_scores(prompt, response)\n",
        "        example.update({\"entropy_scores\": scores})\n",
        "        row_pbar.update(1)\n",
        "        return example\n",
        "    # Process in batches\n",
        "    batch_size = 10_000  # len(df1) #  1000  # Adjust this based on your system's capacity\n",
        "    mapped_data = []\n",
        "    for start in range(0, len(df1), batch_size):\n",
        "        batch = df1.iloc[start : start + batch_size]\n",
        "        batch_results = await asyncio.gather(\n",
        "            *(process_row(example) for _, example in batch.iterrows())\n",
        "        )\n",
        "        mapped_data.extend(batch_results)\n",
        "\n",
        "    df2 = pd.DataFrame(mapped_data)\n",
        "    def getentropygap(row):\n",
        "        return row['entropy_scores']['gap_scores']\n",
        "\n",
        "    # Apply the function to create a new column or use directly for sorting\n",
        "    # Here, we create a temporary column for clarity, but you can also sort directly\n",
        "    df2['temp_entropy_gap'] = df2.apply(getentropygap, axis=1)\n",
        "\n",
        "    # Now sort the DataFrame by the temporary column\n",
        "    df2_sorted = df2.sort_values(by='temp_entropy_gap', ascending=True)\n",
        "\n",
        "    utils.df_to_parquet(df2_sorted, output_path)\n",
        "    return df2_sorted\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m91yKZlx3Ilm",
        "outputId": "ef7c9fee-4c1b-40fc-86a5-edde436cbb0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-11-14 19:21:27,245 E] \u001b[2;36m[colabbox-0:1618030] sampling_client:1010:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = \"/data/datasets/preferences-v2/filtered/xrenewNONE/train/surge_api_1/data/*.parquet\"\n",
        "output_path = \"/data/datasets/preferences-v2/filtered/xrenewNONEentropy/surge_api_1/\"\n",
        "newdata = await process_convo(input_folder, output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "sHh0lEOR6ejf",
        "outputId": "55add2aa-0b4c-426c-cee2-b9935089218a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 26.11it/s]\n",
            "Processing /data/datasets/preferences-v2/filtered/xrenewNONE/train/surge_api_1/data/*.parquet:  99%|████████████████████████████▋| 755/762 [01:02<00:00, 21.77it/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'gap_scores'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m input_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/datasets/preferences-v2/filtered/xrenewNONE/train/surge_api_1/data/*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/datasets/preferences-v2/filtered/xrenewNONEentropy/surge_api_1/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m newdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_convo(input_folder, output_path)\n",
            "Cell \u001b[0;32mIn[5], line 57\u001b[0m, in \u001b[0;36mprocess_convo\u001b[0;34m(input_path, output_path, sampling_client, semaphore, pbar, scorer)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy_scores\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgap_scores\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Apply the function to create a new column or use directly for sorting\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Here, we create a temporary column for clarity, but you can also sort directly\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_entropy_gap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgetentropygap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Now sort the DataFrame by the temporary column\u001b[39;00m\n\u001b[1;32m     60\u001b[0m df2_sorted \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_entropy_gap\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[5], line 53\u001b[0m, in \u001b[0;36mprocess_convo.<locals>.getentropygap\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetentropygap\u001b[39m(row):\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentropy_scores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgap_scores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'gap_scores'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RC3uL6aP6nYt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}