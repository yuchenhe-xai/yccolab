{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4zkiVT35yN7s",
        "MSEJownmyV4x",
        "q42mC0jByahI",
        "HDDFE2IYylWq",
        "9nFpVNt_NnvX",
        "FKLj2P2Vqhd8",
        "NofHL13xtjKm",
        "u8Z9hdGxAdZh",
        "wkC8geNloScv"
      ],
      "authorship_tag": "ABX9TyOtWO89PXcopkIKJwG7QYvo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuchenhe-xai/yccolab/blob/main/241015_exp_runs_rubrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "s74aq1H8yLe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import asyncio\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "from typing import Any\n",
        "import pandas as pd\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "import uuid\n",
        "from xlm.config import configclass\n",
        "from xlm.posttrain import utils\n",
        "from xlm.posttrain.data import formatting\n",
        "from xlm.posttrain.evals.eval_base import SamplerEvalBase\n",
        "from xlm.sampling_client import SamplingClient"
      ],
      "metadata": {
        "id": "_8xPl66DG3Ss"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import markdown\n",
        "from markdown.extensions.tables import TableExtension\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def display_conversation(conversation, conv_loss_mask=None):\n",
        "    \"\"\"\n",
        "    Display a conversation in a notebook with different colors for the 'user' and 'assistant' roles,\n",
        "    supporting Markdown formatting with correct table rendering.\n",
        "\n",
        "    Parameters:\n",
        "    - conversation (list of dict): List of dictionaries containing 'role' and 'content'.\n",
        "    - conv_loss_mask (list of bool): List of boolean values indicating whether to highlight the message.\n",
        "    \"\"\"\n",
        "    user_bg_color = \"#d0e7ff\"  # light blue for user messages\n",
        "    assistant_bg_color = \"#d4edda\"  # light green for assistant messages\n",
        "    user_text_color = \"#084298\"  # darker blue for user text\n",
        "    assistant_text_color = \"#155724\"  # darker green for assistant text\n",
        "    html_output = \"\"\n",
        "\n",
        "    # Setting up Markdown with table support\n",
        "    md = markdown.Markdown(extensions=[TableExtension(use_align_attribute=True)])\n",
        "\n",
        "    for i, entry in enumerate(conversation):\n",
        "        role = entry[\"role\"]\n",
        "        content = entry[\"content\"]\n",
        "        is_user = role.lower() in [\"user\", \"human\"]\n",
        "\n",
        "        # Set color based on role\n",
        "        bg_color = user_bg_color if is_user else assistant_bg_color\n",
        "        text_color = user_text_color if is_user else assistant_text_color\n",
        "        role_display = \"User\" if is_user else \"Assistant\"\n",
        "\n",
        "        # Convert Markdown to HTML, ensuring tables are rendered correctly\n",
        "        content_html = md.convert(content)\n",
        "\n",
        "        # Highlight if conv_loss_mask is True for this message\n",
        "        highlight_style = \"font-weight: bold; text-decoration: underline;\" if conv_loss_mask and conv_loss_mask[i] else \"font-weight: normal;\"\n",
        "\n",
        "        # Append each entry to HTML string with styling\n",
        "        html_output += f\"\"\"\n",
        "        <div style='background-color: {bg_color}; color: {text_color};\n",
        "                    border-radius: 8px; padding: 10px; margin: 8px 0;'>\n",
        "            <strong style='{highlight_style}'>{role_display}:</strong>\n",
        "            <div style='{highlight_style}'>{content_html}</div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    display(HTML(html_output))"
      ],
      "metadata": {
        "id": "SPYdjBXZPyY3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect Preference Data 10/15\n"
      ],
      "metadata": {
        "id": "4zkiVT35yN7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/data/datasets/preferences-v2/mixture/surge1t10/train/*.parquet\"\n",
        "df_data = utils.read_df(glob.glob(data_path))"
      ],
      "metadata": {
        "id": "AHBk0V2jHGg_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ba4e87-3c51-4fe0-d3c6-4b078fa102f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 67/67 [00:06<00:00, 11.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (_, row) in enumerate(df_data.iterrows()):\n",
        "  example = row.to_dict()\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QOYbVy07uzK",
        "outputId": "8e742050-0979-4239-bbb1-00b0f635efcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "905568"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data"
      ],
      "metadata": {
        "id": "EsjLDtXgrgRS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f16506e-49c3-4e2e-93ca-cb4897f8d486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   data0  \\\n",
              "0      Human: Based off of this text, tell me in a si...   \n",
              "1      Human: You are a wizard from the wizarding wor...   \n",
              "2      Human: Please write three separate description...   \n",
              "3      Human: Please act as a psychiatrist having a c...   \n",
              "4      Human: I am searching for phd programs to appl...   \n",
              "...                                                  ...   \n",
              "56593  Human: Me and my sister are going to make cook...   \n",
              "56594  Human: Please provide me advice on how to disc...   \n",
              "56595  Human: Brainstorm some birthday party ideas fo...   \n",
              "56596  Human: in my mac playgorunds app i cannot run ...   \n",
              "56597  Human: Make a bulleted list all of the given i...   \n",
              "\n",
              "                                                   data1  \\\n",
              "0      Human: Based off of this text, tell me in a si...   \n",
              "1      Human: You are a wizard from the wizarding wor...   \n",
              "2      Human: Please write three separate description...   \n",
              "3      Human: Please act as a psychiatrist having a c...   \n",
              "4      Human: I am searching for phd programs to appl...   \n",
              "...                                                  ...   \n",
              "56593  Human: Me and my sister are going to make cook...   \n",
              "56594  Human: Please provide me advice on how to disc...   \n",
              "56595  Human: Brainstorm some birthday party ideas fo...   \n",
              "56596  Human: in my mac playgorunds app i cannot run ...   \n",
              "56597  Human: Make a bulleted list all of the given i...   \n",
              "\n",
              "                                           conversation0  \\\n",
              "0      [{'content': 'Based off of this text, tell me ...   \n",
              "1      [{'content': 'You are a wizard from the wizard...   \n",
              "2      [{'content': 'Please write three separate desc...   \n",
              "3      [{'content': 'Please act as a psychiatrist hav...   \n",
              "4      [{'content': 'I am searching for phd programs ...   \n",
              "...                                                  ...   \n",
              "56593  [{'content': 'Me and my sister are going to ma...   \n",
              "56594  [{'content': 'Please provide me advice on how ...   \n",
              "56595  [{'content': 'Brainstorm some birthday party i...   \n",
              "56596  [{'content': 'in my mac playgorunds app i cann...   \n",
              "56597  [{'content': 'Make a bulleted list all of the ...   \n",
              "\n",
              "                                           conversation1  \\\n",
              "0      [{'content': 'Based off of this text, tell me ...   \n",
              "1      [{'content': 'You are a wizard from the wizard...   \n",
              "2      [{'content': 'Please write three separate desc...   \n",
              "3      [{'content': 'Please act as a psychiatrist hav...   \n",
              "4      [{'content': 'I am searching for phd programs ...   \n",
              "...                                                  ...   \n",
              "56593  [{'content': 'Me and my sister are going to ma...   \n",
              "56594  [{'content': 'Please provide me advice on how ...   \n",
              "56595  [{'content': 'Brainstorm some birthday party i...   \n",
              "56596  [{'content': 'in my mac playgorunds app i cann...   \n",
              "56597  [{'content': 'Make a bulleted list all of the ...   \n",
              "\n",
              "                                                  prompt  preference_overall  \\\n",
              "0      Human: Based off of this text, tell me in a si...                0.50   \n",
              "1      Human: You are a wizard from the wizarding wor...                0.50   \n",
              "2      Human: Please write three separate description...                0.66   \n",
              "3      Human: Please act as a psychiatrist having a c...                0.33   \n",
              "4      Human: I am searching for phd programs to appl...                0.83   \n",
              "...                                                  ...                 ...   \n",
              "56593  Human: Me and my sister are going to make cook...                0.33   \n",
              "56594  Human: Please provide me advice on how to disc...                0.66   \n",
              "56595  Human: Brainstorm some birthday party ideas fo...                0.66   \n",
              "56596  Human: in my mac playgorunds app i cannot run ...                0.83   \n",
              "56597  Human: Make a bulleted list all of the given i...                0.16   \n",
              "\n",
              "                                           id  \\\n",
              "0      785b1621-4081-4c1a-96d1-72db350f3c66:1   \n",
              "1      dd04e1f6-c670-4c45-aa32-4b3206033a35:2   \n",
              "2      184cf9c8-a3a8-4412-8bfe-039a83cc520f:3   \n",
              "3      5045df3d-54b4-4255-bc29-c7c7fab17530:1   \n",
              "4      c701c6d5-f4ff-4bd3-8971-df01d112f8ec:2   \n",
              "...                                       ...   \n",
              "56593  6b7691a2-588d-485a-8da9-d2d142bc0a8a:3   \n",
              "56594  47ef3c8a-18ea-47f3-b730-3192fb010a53:2   \n",
              "56595  8dd04d7a-3771-4ee1-8406-3b570682abb0:2   \n",
              "56596  2deb52a9-0ce2-4930-9388-81a448b89661:2   \n",
              "56597  2a6c3d39-cd0b-4ddc-a808-0c27bb1e44aa:3   \n",
              "\n",
              "                                                src_path  \\\n",
              "0      /data/datasets/preferences-v2/raw/surge_finegr...   \n",
              "1      /data/datasets/preferences-v2/raw/surge_finegr...   \n",
              "2      /data/datasets/preferences-v2/raw/surge_finegr...   \n",
              "3      /data/datasets/preferences-v2/raw/surge_finegr...   \n",
              "4      /data/datasets/preferences-v2/raw/surge_finegr...   \n",
              "...                                                  ...   \n",
              "56593  /data/datasets/preferences-v2/raw/surge_finegr...   \n",
              "56594  /data/datasets/preferences-v2/raw/surge_finegr...   \n",
              "56595  /data/datasets/preferences-v2/raw/surge_finegr...   \n",
              "56596  /data/datasets/preferences-v2/raw/surge_finegr...   \n",
              "56597  /data/datasets/preferences-v2/raw/surge_finegr...   \n",
              "\n",
              "                         dataset                                       domain  \\\n",
              "0      surge_finegrain_general_6                                   Closed Q&A   \n",
              "1      surge_finegrain_general_8                                     Open Q&A   \n",
              "2      surge_finegrain_general_4                                Summarization   \n",
              "3      surge_finegrain_general_5                                      Persona   \n",
              "4      surge_finegrain_general_4                                   Extraction   \n",
              "...                          ...                                          ...   \n",
              "56593  surge_finegrain_general_3                               Classification   \n",
              "56594  surge_finegrain_general_5                                     Open Q&A   \n",
              "56595  surge_finegrain_general_4                                     Open Q&A   \n",
              "56596  surge_finegrain_coding_10  Translation to Another Programming Language   \n",
              "56597  surge_finegrain_general_8                                      Writing   \n",
              "\n",
              "       preference_quality  preference_instruction-following  \\\n",
              "0                   0.500                            -100.0   \n",
              "1                   0.375                            -100.0   \n",
              "2                   0.500                            -100.0   \n",
              "3                   0.500                            -100.0   \n",
              "4                   0.625                            -100.0   \n",
              "...                   ...                               ...   \n",
              "56593            -100.000                            -100.0   \n",
              "56594               0.500                            -100.0   \n",
              "56595               0.625                            -100.0   \n",
              "56596            -100.000                            -100.0   \n",
              "56597               0.375                            -100.0   \n",
              "\n",
              "       preference_correctness  preference_writing  \\\n",
              "0                      -100.0              -100.0   \n",
              "1                         0.5              -100.0   \n",
              "2                      -100.0              -100.0   \n",
              "3                      -100.0              -100.0   \n",
              "4                      -100.0              -100.0   \n",
              "...                       ...                 ...   \n",
              "56593                     0.5              -100.0   \n",
              "56594                  -100.0              -100.0   \n",
              "56595                  -100.0              -100.0   \n",
              "56596                  -100.0              -100.0   \n",
              "56597                  -100.0              -100.0   \n",
              "\n",
              "                                                  critic  preference_verbosity  \n",
              "0      Both responses answer the prompt according to ...              1.000000  \n",
              "1      Both responses have major inaccuracies because...           -100.000000  \n",
              "2      Model B remembered to bold the athlete or team...              0.500000  \n",
              "3      Model A does a better job of fulfilling the fi...              0.500000  \n",
              "4      Response B is much better. Both models extract...              0.833333  \n",
              "...                                                  ...                   ...  \n",
              "56593  Although, the original recipe was a whoopie pi...              0.500000  \n",
              "56594  While Model A had great movie options, Model B...              0.500000  \n",
              "56595  Both responses follow the requested parameters...              0.500000  \n",
              "56596  The prompt implies that the user is targeting ...           -100.000000  \n",
              "56597  Both models wrote great pieces, but Model A is...           -100.000000  \n",
              "\n",
              "[56598 rows x 16 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data0</th>\n",
              "      <th>data1</th>\n",
              "      <th>conversation0</th>\n",
              "      <th>conversation1</th>\n",
              "      <th>prompt</th>\n",
              "      <th>preference_overall</th>\n",
              "      <th>id</th>\n",
              "      <th>src_path</th>\n",
              "      <th>dataset</th>\n",
              "      <th>domain</th>\n",
              "      <th>preference_quality</th>\n",
              "      <th>preference_instruction-following</th>\n",
              "      <th>preference_correctness</th>\n",
              "      <th>preference_writing</th>\n",
              "      <th>critic</th>\n",
              "      <th>preference_verbosity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Human: Based off of this text, tell me in a si...</td>\n",
              "      <td>Human: Based off of this text, tell me in a si...</td>\n",
              "      <td>[{'content': 'Based off of this text, tell me ...</td>\n",
              "      <td>[{'content': 'Based off of this text, tell me ...</td>\n",
              "      <td>Human: Based off of this text, tell me in a si...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>785b1621-4081-4c1a-96d1-72db350f3c66:1</td>\n",
              "      <td>/data/datasets/preferences-v2/raw/surge_finegr...</td>\n",
              "      <td>surge_finegrain_general_6</td>\n",
              "      <td>Closed Q&amp;A</td>\n",
              "      <td>0.500</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>Both responses answer the prompt according to ...</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Human: You are a wizard from the wizarding wor...</td>\n",
              "      <td>Human: You are a wizard from the wizarding wor...</td>\n",
              "      <td>[{'content': 'You are a wizard from the wizard...</td>\n",
              "      <td>[{'content': 'You are a wizard from the wizard...</td>\n",
              "      <td>Human: You are a wizard from the wizarding wor...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>dd04e1f6-c670-4c45-aa32-4b3206033a35:2</td>\n",
              "      <td>/data/datasets/preferences-v2/raw/surge_finegr...</td>\n",
              "      <td>surge_finegrain_general_8</td>\n",
              "      <td>Open Q&amp;A</td>\n",
              "      <td>0.375</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>Both responses have major inaccuracies because...</td>\n",
              "      <td>-100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Human: Please write three separate description...</td>\n",
              "      <td>Human: Please write three separate description...</td>\n",
              "      <td>[{'content': 'Please write three separate desc...</td>\n",
              "      <td>[{'content': 'Please write three separate desc...</td>\n",
              "      <td>Human: Please write three separate description...</td>\n",
              "      <td>0.66</td>\n",
              "      <td>184cf9c8-a3a8-4412-8bfe-039a83cc520f:3</td>\n",
              "      <td>/data/datasets/preferences-v2/raw/surge_finegr...</td>\n",
              "      <td>surge_finegrain_general_4</td>\n",
              "      <td>Summarization</td>\n",
              "      <td>0.500</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>Model B remembered to bold the athlete or team...</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Human: Please act as a psychiatrist having a c...</td>\n",
              "      <td>Human: Please act as a psychiatrist having a c...</td>\n",
              "      <td>[{'content': 'Please act as a psychiatrist hav...</td>\n",
              "      <td>[{'content': 'Please act as a psychiatrist hav...</td>\n",
              "      <td>Human: Please act as a psychiatrist having a c...</td>\n",
              "      <td>0.33</td>\n",
              "      <td>5045df3d-54b4-4255-bc29-c7c7fab17530:1</td>\n",
              "      <td>/data/datasets/preferences-v2/raw/surge_finegr...</td>\n",
              "      <td>surge_finegrain_general_5</td>\n",
              "      <td>Persona</td>\n",
              "      <td>0.500</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>Model A does a better job of fulfilling the fi...</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Human: I am searching for phd programs to appl...</td>\n",
              "      <td>Human: I am searching for phd programs to appl...</td>\n",
              "      <td>[{'content': 'I am searching for phd programs ...</td>\n",
              "      <td>[{'content': 'I am searching for phd programs ...</td>\n",
              "      <td>Human: I am searching for phd programs to appl...</td>\n",
              "      <td>0.83</td>\n",
              "      <td>c701c6d5-f4ff-4bd3-8971-df01d112f8ec:2</td>\n",
              "      <td>/data/datasets/preferences-v2/raw/surge_finegr...</td>\n",
              "      <td>surge_finegrain_general_4</td>\n",
              "      <td>Extraction</td>\n",
              "      <td>0.625</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>Response B is much better. Both models extract...</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56593</th>\n",
              "      <td>Human: Me and my sister are going to make cook...</td>\n",
              "      <td>Human: Me and my sister are going to make cook...</td>\n",
              "      <td>[{'content': 'Me and my sister are going to ma...</td>\n",
              "      <td>[{'content': 'Me and my sister are going to ma...</td>\n",
              "      <td>Human: Me and my sister are going to make cook...</td>\n",
              "      <td>0.33</td>\n",
              "      <td>6b7691a2-588d-485a-8da9-d2d142bc0a8a:3</td>\n",
              "      <td>/data/datasets/preferences-v2/raw/surge_finegr...</td>\n",
              "      <td>surge_finegrain_general_3</td>\n",
              "      <td>Classification</td>\n",
              "      <td>-100.000</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>Although, the original recipe was a whoopie pi...</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56594</th>\n",
              "      <td>Human: Please provide me advice on how to disc...</td>\n",
              "      <td>Human: Please provide me advice on how to disc...</td>\n",
              "      <td>[{'content': 'Please provide me advice on how ...</td>\n",
              "      <td>[{'content': 'Please provide me advice on how ...</td>\n",
              "      <td>Human: Please provide me advice on how to disc...</td>\n",
              "      <td>0.66</td>\n",
              "      <td>47ef3c8a-18ea-47f3-b730-3192fb010a53:2</td>\n",
              "      <td>/data/datasets/preferences-v2/raw/surge_finegr...</td>\n",
              "      <td>surge_finegrain_general_5</td>\n",
              "      <td>Open Q&amp;A</td>\n",
              "      <td>0.500</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>While Model A had great movie options, Model B...</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56595</th>\n",
              "      <td>Human: Brainstorm some birthday party ideas fo...</td>\n",
              "      <td>Human: Brainstorm some birthday party ideas fo...</td>\n",
              "      <td>[{'content': 'Brainstorm some birthday party i...</td>\n",
              "      <td>[{'content': 'Brainstorm some birthday party i...</td>\n",
              "      <td>Human: Brainstorm some birthday party ideas fo...</td>\n",
              "      <td>0.66</td>\n",
              "      <td>8dd04d7a-3771-4ee1-8406-3b570682abb0:2</td>\n",
              "      <td>/data/datasets/preferences-v2/raw/surge_finegr...</td>\n",
              "      <td>surge_finegrain_general_4</td>\n",
              "      <td>Open Q&amp;A</td>\n",
              "      <td>0.625</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>Both responses follow the requested parameters...</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56596</th>\n",
              "      <td>Human: in my mac playgorunds app i cannot run ...</td>\n",
              "      <td>Human: in my mac playgorunds app i cannot run ...</td>\n",
              "      <td>[{'content': 'in my mac playgorunds app i cann...</td>\n",
              "      <td>[{'content': 'in my mac playgorunds app i cann...</td>\n",
              "      <td>Human: in my mac playgorunds app i cannot run ...</td>\n",
              "      <td>0.83</td>\n",
              "      <td>2deb52a9-0ce2-4930-9388-81a448b89661:2</td>\n",
              "      <td>/data/datasets/preferences-v2/raw/surge_finegr...</td>\n",
              "      <td>surge_finegrain_coding_10</td>\n",
              "      <td>Translation to Another Programming Language</td>\n",
              "      <td>-100.000</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>The prompt implies that the user is targeting ...</td>\n",
              "      <td>-100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56597</th>\n",
              "      <td>Human: Make a bulleted list all of the given i...</td>\n",
              "      <td>Human: Make a bulleted list all of the given i...</td>\n",
              "      <td>[{'content': 'Make a bulleted list all of the ...</td>\n",
              "      <td>[{'content': 'Make a bulleted list all of the ...</td>\n",
              "      <td>Human: Make a bulleted list all of the given i...</td>\n",
              "      <td>0.16</td>\n",
              "      <td>2a6c3d39-cd0b-4ddc-a808-0c27bb1e44aa:3</td>\n",
              "      <td>/data/datasets/preferences-v2/raw/surge_finegr...</td>\n",
              "      <td>surge_finegrain_general_8</td>\n",
              "      <td>Writing</td>\n",
              "      <td>0.375</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>Both models wrote great pieces, but Model A is...</td>\n",
              "      <td>-100.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>56598 rows × 16 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJQcPiQa74_c",
        "outputId": "344e92c5-fb75-4b4d-cbf4-f33d4461b906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data0', 'data1', 'conversation0', 'conversation1', 'prompt', 'preference_overall', 'id', 'src_path', 'dataset', 'domain', 'preference_quality', 'preference_instruction-following', 'preference_correctness', 'preference_writing', 'critic', 'preference_verbosity'])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbJ5DANXyVFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rubric Data 10/18\n"
      ],
      "metadata": {
        "id": "MSEJownmyV4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RUBRIC_PROPOSER_PROMPT = \"\"\"\\\n",
        "You are an educational expert to design detailed rubrics for evaluating responses to given queries:\n",
        "\n",
        "---\n",
        "\n",
        "Please create a set of detailed rubrics to evaluate how well an AI responds to a series of questions or commands. Here's what you need to do:\n",
        "\n",
        "1. **Understand and Analyze the Context**: You'll be provided query to help guide your rubric creation. Please read them carefully and provide a detailed analysis report before you write any rubrics.\n",
        "\n",
        "2. **Rubric Structure**:\n",
        "   - Each rubric item should be a simple, binary (True or False) criterion focusing on one specific aspect of the response.\n",
        "   - Ensure that the criteria are:\n",
        "     - **Specific**: Clearly define what's being evaluated.\n",
        "     - **Comprehensiveness**: Cover most significant aspects required by query including Instruction_following, Correctness, Reasoning, Clarity, Conciseness, Writing and more.\n",
        "     - **Objective**: A third party should be able to unambiguously determine if the criterion is met.\n",
        "\n",
        "3. **Example**:\n",
        "- Input is:\n",
        "<conversation-history>\n",
        "Human: I am going to visit my college roommate Zach soon and I want to get him a gift. We both love going to Taco Bell during the school year, so I would like to get him a small gift that is Taco Bell-themed. Can you come up with 5 fun gift ideas that incorporate Taco Bell?<|separator|>\n",
        "\n",
        "Assistant: Here are 5 fun Taco Bell-themed gift ideas for your college roommate Zach:\n",
        "\n",
        "    1. Taco Bell Gift Card: A Taco Bell gift card is a perfect gift for any Taco Bell enthusiast. This way, Zach can enjoy his favorite menu items whenever he wants.\n",
        "\n",
        "    2. Taco Bell Hot Sauce Packet Keychain: You can find keychains online that are designed to look like Taco Bell's iconic hot sauce packets. This is a fun and quirky gift that Zach can carry around with him.\n",
        "\n",
        "    3. Taco Bell T-Shirt or Hoodie: There are many Taco Bell-themed t-shirts and hoodies available online, featuring the brand's logo, slogans, or even vintage designs. Pick one that you think Zach would like and wear.\n",
        "\n",
        "    4. Taco Bell Socks: Novelty socks are always a fun gift, and you can find Taco Bell-themed socks online that feature the brand's colors, logo, or even tacos and burritos.\n",
        "\n",
        "    5. Taco Bell Cookbook: If Zach enjoys cooking, get him a Taco Bell-inspired cookbook that features recipes for recreating his favorite menu items at home. This gift combines his love for Taco Bell with a fun culinary experience.\n",
        "\n",
        "These gifts are all relatively small and affordable, making them perfect for a college roommate. Enjoy your visit with Zach and your shared love for Taco Bell!\n",
        "</conversation-history>\n",
        "<query> Can you classify these gift ideas into the following categories: clothing items and non-clothing items? </query>\n",
        "- Output is ```\n",
        "<analysis>\n",
        "   The query requests the 5 gift ideas into 2 categories: clothing items and non-clothing items.\n",
        "   The output is in the format of a table, which shows the 5 gifts and their corresponding category.\n",
        "</analysis>\n",
        "<rubrics>\n",
        "    {{\n",
        "        \"criterion_1\": \"All five gift ideas are included in the classification.\",\n",
        "        \"criterion_2\": \"Each item's classification is unambiguous and without error.\",\n",
        "        \"criterion_3\": \"Taco Bell T-Shirt or Hoodie and Taco Bell Socks are classified as clothing items.\",\n",
        "        \"criterion_4\": \"Taco Bell Gift Card, Taco Bell Hot Sauce Packet Keychain and Taco Bell Cookbook are classified as non-clothing items.\"\n",
        "        ...\n",
        "    }}\n",
        "</rubrics>\n",
        "    ```\n",
        "\n",
        "4. **Rubric Writing Tips**:\n",
        "   - If possible, directly cite the phrase or words from original query.\n",
        "   - Formulate each criterion as a clear and specific statement that can be answered with a yes or no. But citeria itself should not be \"Yes\" or \"No\".\n",
        "   - Only cover most significant aspects expressed in query. Do not make duplicated criteria.\n",
        "   - If a prompt asks for 2 pieces of information, each piece should be a separate criterion.\n",
        "   - Avoid clever or indirect phrasing. Avoid unnecessary synonyms that may confuse the verifier and be less faithful to the prompt. Stick to what's straightforward.\n",
        "   - Criteria should be concrete and specific to the user prompt.\n",
        "\n",
        "You will be provided a conversation history and a final query.\n",
        "Now your input is:\n",
        "<conversation-history> {history} </conversation-history>\n",
        "<query> {prompt} </query>\n",
        "\n",
        "Please think step by step, first provide detailed analysis on the <query> and list what is the most important requirements.\n",
        "If the <conversation-history> is presented, you do not need to consider the requirements of the conversation-history, but only use it as more context for the requirements for the <query>.\n",
        "Finally, based on the analysis of <query>, write specific and detailed rubrics. Do not generate rubrics not required in query.\n",
        "\n",
        "The analysis shall be enclosed in <analysis></analysis>.\n",
        "The rubrics shall be enclosed in <rubrics></rubrics>. The final rubrics shall be written in dictionary format, and must be json parsable.\n",
        "<analysis> </analysis>\n",
        "<rubrics>\n",
        "{{\n",
        "    'criterion_1': xxx,\n",
        "    'criterion_2': xxx,\n",
        "    ......\n",
        "}}\n",
        "</rubrics>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UPPFoU-k0ee8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split = \"valid\"\n",
        "human_rubric_path = f'/data/datasets/GIF/merged_1015/rl/{split}.parquet'\n",
        "\n",
        "df_data = utils.read_df(human_rubric_path)\n",
        "print(\"\\nsize\", df_data.size)\n",
        "new_df_data = []\n",
        "for idx, (_, row) in enumerate(df_data.iterrows()):\n",
        "    example = row.to_dict()\n",
        "    # print(example.keys(), example['prompt'])\n",
        "    msgs = formatting.derender_conversation(\n",
        "        name=\"grok\",\n",
        "        rendered_conv=formatting.strip_last_generation_prompt(example[\"prompt\"]),\n",
        "    )\n",
        "    query = msgs[-1]\n",
        "    history = \"\"\n",
        "    if len(msgs) > 1:\n",
        "        history = formatting.render_conversation(name=\"grok\", messages=msgs[:-1])\n",
        "        if history.endswith(\"<|separator|>\"):\n",
        "            history = history[: -len(\"<|separator|>\")]\n",
        "    # example['id'] = str(uuid.uuid4())\n",
        "    user_prompt = RUBRIC_PROPOSER_PROMPT.format(history=history, prompt=query[\"content\"])\n",
        "    model_reponse_target = json.dumps(example['rubric'].tolist())\n",
        "    conversation = [{\"role\": \"user\", \"content\": user_prompt.replace(\"<|separator|>\", \"\")}, {\"role\": \"assistant\", \"content\": model_reponse_target}]\n",
        "    example['conversation'] = conversation\n",
        "    conversation_str = formatting.render_conversation(name=\"grok\", messages=example['conversation'])\n",
        "    example['conversation_str'] = conversation_str\n",
        "    example[\"name\"] = \"mixture_rubrics_proposal_train-8k-v4-pack1\"\n",
        "\n",
        "    new_df_data.append(example)\n",
        "new_df_data = pd.DataFrame.from_records(new_df_data)\n",
        "new_df_data.to_parquet(f\"/data/datasets/GIF/merged_1015/sft/{split}.parquet\")\n",
        "new_df_data"
      ],
      "metadata": {
        "id": "QR0D4XRj_JR8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "db4fe81f-0ec6-470c-c2b7-2dc0835ca130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "size 3360\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    source                               task_id             category  \\\n",
              "0    surge  1637a65c-caa2-4637-a122-8acbf1a86fa4            [Open QA]   \n",
              "1    surge  def3d136-2fdb-4f23-b94d-b8f25dde9ad0              [Other]   \n",
              "2    surge  a5a9df45-e868-4c96-94d7-e175eb8eb913              [Other]   \n",
              "3    surge  cd319c82-0b04-4906-9eb3-a8282164fceb              [Other]   \n",
              "4    surge  d6030193-0ded-4987-aa4f-4ec01d515fba  [Asking for advice]   \n",
              "..     ...                                   ...                  ...   \n",
              "555  tutor                 mvMFphUimxtlnDeNroOL3            [Persona]   \n",
              "556  tutor                 xPAdVxaanoti7BxQ6WUTc     [Conversational]   \n",
              "557  tutor                 OWeJ52FRgOo8kEFHmnSkk     [Conversational]   \n",
              "558  tutor                 IUfONyP9mEkn0WjFlBkGE     [Conversational]   \n",
              "559  tutor                 tL9eXtTPondjZARON5oOE     [Conversational]   \n",
              "\n",
              "                                                rubric  \\\n",
              "0    [The motion examples in the response should us...   \n",
              "1    [The model should include 15 perks from this l...   \n",
              "2    [The response should list the items the user w...   \n",
              "3    [The schedule should allocate 8 uninterrupted ...   \n",
              "4    [The table should include the price of the iPh...   \n",
              "..                                                 ...   \n",
              "555  [Response has between 166 and 183 words., Resp...   \n",
              "556  [The response provides 3 jokes about guns and ...   \n",
              "557  [The response lists tasks that would need to b...   \n",
              "558  [The response writes a joke about fairies. , T...   \n",
              "559  [The response's jokes are all dad jokes., The ...   \n",
              "\n",
              "                                                prompt  \\\n",
              "0    Human: I'm performing a signal test to be a fo...   \n",
              "1    Human: I want to learn some of the history of ...   \n",
              "2    Human: I want to make meal for the next 3 days...   \n",
              "3    Human: I have a very important and busy week c...   \n",
              "4    Human: I need advice on whether or not I shoul...   \n",
              "..                                                 ...   \n",
              "555  Human: It's 1978.  Write a three paragraph a l...   \n",
              "556  Human: my dad and i constantly argue about pol...   \n",
              "557  Human: Give me a list of the 3 best quarterbac...   \n",
              "558  Human: Tell me a joke for each topic I come up...   \n",
              "559  Human: I need some great Dad jokes for my youn...   \n",
              "\n",
              "                                              messages  \\\n",
              "0    [{'content': 'I'm performing a signal test to ...   \n",
              "1    [{'content': 'I want to learn some of the hist...   \n",
              "2    [{'content': 'I want to make meal for the next...   \n",
              "3    [{'content': 'I have a very important and busy...   \n",
              "4    [{'content': 'I need advice on whether or not ...   \n",
              "..                                                 ...   \n",
              "555  [{'content': 'It's 1978.  Write a three paragr...   \n",
              "556  [{'content': 'my dad and i constantly argue ab...   \n",
              "557  [{'content': 'Give me a list of the 3 best qua...   \n",
              "558  [{'content': 'Tell me a joke for each topic I ...   \n",
              "559  [{'content': 'I need some great Dad jokes for ...   \n",
              "\n",
              "                                          conversation  \\\n",
              "0    [{'role': 'user', 'content': 'You are an educa...   \n",
              "1    [{'role': 'user', 'content': 'You are an educa...   \n",
              "2    [{'role': 'user', 'content': 'You are an educa...   \n",
              "3    [{'role': 'user', 'content': 'You are an educa...   \n",
              "4    [{'role': 'user', 'content': 'You are an educa...   \n",
              "..                                                 ...   \n",
              "555  [{'role': 'user', 'content': 'You are an educa...   \n",
              "556  [{'role': 'user', 'content': 'You are an educa...   \n",
              "557  [{'role': 'user', 'content': 'You are an educa...   \n",
              "558  [{'role': 'user', 'content': 'You are an educa...   \n",
              "559  [{'role': 'user', 'content': 'You are an educa...   \n",
              "\n",
              "                                      conversation_str  \\\n",
              "0    Human: You are an educational expert to design...   \n",
              "1    Human: You are an educational expert to design...   \n",
              "2    Human: You are an educational expert to design...   \n",
              "3    Human: You are an educational expert to design...   \n",
              "4    Human: You are an educational expert to design...   \n",
              "..                                                 ...   \n",
              "555  Human: You are an educational expert to design...   \n",
              "556  Human: You are an educational expert to design...   \n",
              "557  Human: You are an educational expert to design...   \n",
              "558  Human: You are an educational expert to design...   \n",
              "559  Human: You are an educational expert to design...   \n",
              "\n",
              "                                           name  \n",
              "0    mixture_rubrics_proposal_train-8k-v4-pack1  \n",
              "1    mixture_rubrics_proposal_train-8k-v4-pack1  \n",
              "2    mixture_rubrics_proposal_train-8k-v4-pack1  \n",
              "3    mixture_rubrics_proposal_train-8k-v4-pack1  \n",
              "4    mixture_rubrics_proposal_train-8k-v4-pack1  \n",
              "..                                          ...  \n",
              "555  mixture_rubrics_proposal_train-8k-v4-pack1  \n",
              "556  mixture_rubrics_proposal_train-8k-v4-pack1  \n",
              "557  mixture_rubrics_proposal_train-8k-v4-pack1  \n",
              "558  mixture_rubrics_proposal_train-8k-v4-pack1  \n",
              "559  mixture_rubrics_proposal_train-8k-v4-pack1  \n",
              "\n",
              "[560 rows x 9 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>task_id</th>\n",
              "      <th>category</th>\n",
              "      <th>rubric</th>\n",
              "      <th>prompt</th>\n",
              "      <th>messages</th>\n",
              "      <th>conversation</th>\n",
              "      <th>conversation_str</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>surge</td>\n",
              "      <td>1637a65c-caa2-4637-a122-8acbf1a86fa4</td>\n",
              "      <td>[Open QA]</td>\n",
              "      <td>[The motion examples in the response should us...</td>\n",
              "      <td>Human: I'm performing a signal test to be a fo...</td>\n",
              "      <td>[{'content': 'I'm performing a signal test to ...</td>\n",
              "      <td>[{'role': 'user', 'content': 'You are an educa...</td>\n",
              "      <td>Human: You are an educational expert to design...</td>\n",
              "      <td>mixture_rubrics_proposal_train-8k-v4-pack1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>surge</td>\n",
              "      <td>def3d136-2fdb-4f23-b94d-b8f25dde9ad0</td>\n",
              "      <td>[Other]</td>\n",
              "      <td>[The model should include 15 perks from this l...</td>\n",
              "      <td>Human: I want to learn some of the history of ...</td>\n",
              "      <td>[{'content': 'I want to learn some of the hist...</td>\n",
              "      <td>[{'role': 'user', 'content': 'You are an educa...</td>\n",
              "      <td>Human: You are an educational expert to design...</td>\n",
              "      <td>mixture_rubrics_proposal_train-8k-v4-pack1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>surge</td>\n",
              "      <td>a5a9df45-e868-4c96-94d7-e175eb8eb913</td>\n",
              "      <td>[Other]</td>\n",
              "      <td>[The response should list the items the user w...</td>\n",
              "      <td>Human: I want to make meal for the next 3 days...</td>\n",
              "      <td>[{'content': 'I want to make meal for the next...</td>\n",
              "      <td>[{'role': 'user', 'content': 'You are an educa...</td>\n",
              "      <td>Human: You are an educational expert to design...</td>\n",
              "      <td>mixture_rubrics_proposal_train-8k-v4-pack1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>surge</td>\n",
              "      <td>cd319c82-0b04-4906-9eb3-a8282164fceb</td>\n",
              "      <td>[Other]</td>\n",
              "      <td>[The schedule should allocate 8 uninterrupted ...</td>\n",
              "      <td>Human: I have a very important and busy week c...</td>\n",
              "      <td>[{'content': 'I have a very important and busy...</td>\n",
              "      <td>[{'role': 'user', 'content': 'You are an educa...</td>\n",
              "      <td>Human: You are an educational expert to design...</td>\n",
              "      <td>mixture_rubrics_proposal_train-8k-v4-pack1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>surge</td>\n",
              "      <td>d6030193-0ded-4987-aa4f-4ec01d515fba</td>\n",
              "      <td>[Asking for advice]</td>\n",
              "      <td>[The table should include the price of the iPh...</td>\n",
              "      <td>Human: I need advice on whether or not I shoul...</td>\n",
              "      <td>[{'content': 'I need advice on whether or not ...</td>\n",
              "      <td>[{'role': 'user', 'content': 'You are an educa...</td>\n",
              "      <td>Human: You are an educational expert to design...</td>\n",
              "      <td>mixture_rubrics_proposal_train-8k-v4-pack1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>tutor</td>\n",
              "      <td>mvMFphUimxtlnDeNroOL3</td>\n",
              "      <td>[Persona]</td>\n",
              "      <td>[Response has between 166 and 183 words., Resp...</td>\n",
              "      <td>Human: It's 1978.  Write a three paragraph a l...</td>\n",
              "      <td>[{'content': 'It's 1978.  Write a three paragr...</td>\n",
              "      <td>[{'role': 'user', 'content': 'You are an educa...</td>\n",
              "      <td>Human: You are an educational expert to design...</td>\n",
              "      <td>mixture_rubrics_proposal_train-8k-v4-pack1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556</th>\n",
              "      <td>tutor</td>\n",
              "      <td>xPAdVxaanoti7BxQ6WUTc</td>\n",
              "      <td>[Conversational]</td>\n",
              "      <td>[The response provides 3 jokes about guns and ...</td>\n",
              "      <td>Human: my dad and i constantly argue about pol...</td>\n",
              "      <td>[{'content': 'my dad and i constantly argue ab...</td>\n",
              "      <td>[{'role': 'user', 'content': 'You are an educa...</td>\n",
              "      <td>Human: You are an educational expert to design...</td>\n",
              "      <td>mixture_rubrics_proposal_train-8k-v4-pack1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>tutor</td>\n",
              "      <td>OWeJ52FRgOo8kEFHmnSkk</td>\n",
              "      <td>[Conversational]</td>\n",
              "      <td>[The response lists tasks that would need to b...</td>\n",
              "      <td>Human: Give me a list of the 3 best quarterbac...</td>\n",
              "      <td>[{'content': 'Give me a list of the 3 best qua...</td>\n",
              "      <td>[{'role': 'user', 'content': 'You are an educa...</td>\n",
              "      <td>Human: You are an educational expert to design...</td>\n",
              "      <td>mixture_rubrics_proposal_train-8k-v4-pack1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>tutor</td>\n",
              "      <td>IUfONyP9mEkn0WjFlBkGE</td>\n",
              "      <td>[Conversational]</td>\n",
              "      <td>[The response writes a joke about fairies. , T...</td>\n",
              "      <td>Human: Tell me a joke for each topic I come up...</td>\n",
              "      <td>[{'content': 'Tell me a joke for each topic I ...</td>\n",
              "      <td>[{'role': 'user', 'content': 'You are an educa...</td>\n",
              "      <td>Human: You are an educational expert to design...</td>\n",
              "      <td>mixture_rubrics_proposal_train-8k-v4-pack1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559</th>\n",
              "      <td>tutor</td>\n",
              "      <td>tL9eXtTPondjZARON5oOE</td>\n",
              "      <td>[Conversational]</td>\n",
              "      <td>[The response's jokes are all dad jokes., The ...</td>\n",
              "      <td>Human: I need some great Dad jokes for my youn...</td>\n",
              "      <td>[{'content': 'I need some great Dad jokes for ...</td>\n",
              "      <td>[{'role': 'user', 'content': 'You are an educa...</td>\n",
              "      <td>Human: You are an educational expert to design...</td>\n",
              "      <td>mixture_rubrics_proposal_train-8k-v4-pack1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>560 rows × 9 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast, re\n",
        "response = '''\n",
        " ### Analysis:\n",
        "The query requests two primary actions:\n",
        "1. **Rewrite and Categorize the List:** The original list of Chrome extensions provided in the conversation history needs to be grouped by categories like Time Tracking, Productivity Tools, etc., to make the information more navigable for online business owners.\n",
        "\n",
        "2. **Grammar and Spelling Check:** The provided text should be reviewed for any spelling or grammatical issues to ensure clarity and professionalism in communication.\n",
        "\n",
        "Here are the most important requirements based on the query:\n",
        "\n",
        "- **Categorization:** The list must be categorized into meaningful groups related to the functionalities of the extensions, enhancing user accessibility.\n",
        "\n",
        "- **Consistency and Accuracy in Titles and Descriptions:** When rewritten, the titles and descriptions should remain true to the original information provided. This includes accurately matching the extensions to their intended use categories.\n",
        "\n",
        "- **Attention to Grammar:** The descriptions should be free from grammatical and spelling errors to uphold the professionalism of the list.\n",
        "\n",
        "- **Clarity and Conciseness:** The rewritten response should be clearly presented so that users can quickly understand what each extension does.\n",
        "\n",
        "Based on these requirements, the following rubrics will focus on these key aspects:\n",
        "\n",
        "### Rubrics:\n",
        "```json\n",
        "{\n",
        "    \"criterion_1\": \"Each extension is categorized under a relevant category\",\n",
        "    \"criterion_2\": \"The categories are Time-Tracking, Productivity Tools, Writing & Editing, Communication, Organization, Password Management, Focus, Browser Management, Scheduling, Email, and Lead Generation\",\n",
        "    \"criterion_3\": \"The titles and descriptions of the extensions remain accurate to the original list\",\n",
        "    \"criterion_4\": \"There are no spelling or grammatical errors in the revised descriptions\",\n",
        "    \"criterion_5\": \"The extension titles and categories are consistently formatted\",\n",
        "    \"criterion_6\": \"Each category includes an introduction or heading\",\n",
        "    \"criterion_7\": \"The descriptions are clear and concise, enabling easy comprehension\",\n",
        "    \"criterion_8\": \"The list is formatted in an organized manner that is visually navigable\"\n",
        "}\n",
        "```\n",
        "'''\n",
        "m = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response, re.DOTALL)\n",
        "res = ast.literal_eval(m.group(1).strip())\n",
        "res\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAALjQz8OhhP",
        "outputId": "ea3e17e0-0e67-4f12-b218-07b19111c987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'criterion_1': 'Each extension is categorized under a relevant category',\n",
              " 'criterion_2': 'The categories are Time-Tracking, Productivity Tools, Writing & Editing, Communication, Organization, Password Management, Focus, Browser Management, Scheduling, Email, and Lead Generation',\n",
              " 'criterion_3': 'The titles and descriptions of the extensions remain accurate to the original list',\n",
              " 'criterion_4': 'There are no spelling or grammatical errors in the revised descriptions',\n",
              " 'criterion_5': 'The extension titles and categories are consistently formatted',\n",
              " 'criterion_6': 'Each category includes an introduction or heading',\n",
              " 'criterion_7': 'The descriptions are clear and concise, enabling easy comprehension',\n",
              " 'criterion_8': 'The list is formatted in an organized manner that is visually navigable'}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsWOoF3gOv-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd $XAI_ROOT/x/ziniu/scripts && python evaluate_rubric_rm.py\n",
        "import asyncio\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "from xlm.posttrain import utils\n",
        "from xlm.reward_client import SGLangRewardClient\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from xlm.sampling_client import GrokSamplingClient, SamplingClient\n",
        "from xlm.posttrain.data import formatting\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "import json\n",
        "from xlm.reward_client import RubricCriticRewardClient\n",
        "import argparse\n",
        "from xlm.posttrain.launch_utils import create_json\n",
        "\n",
        "\n",
        "class RMEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        reward_model_address: str,\n",
        "        max_parallel: int = 64,\n",
        "    ):\n",
        "        self.reward_model_address = reward_model_address\n",
        "        print(reward_model_address)\n",
        "        self.max_parallel = max_parallel\n",
        "        self.verifier_version = \"v0\"\n",
        "\n",
        "    async def setup_clients(self):\n",
        "        self.client = RubricCriticRewardClient(\n",
        "            address=self.reward_model_address,\n",
        "            max_parallel=self.max_parallel,\n",
        "            verifier_version=self.verifier_version,\n",
        "        )\n",
        "\n",
        "    async def process(self):\n",
        "        print(\"\\n=================================================\")\n",
        "        paths = glob.glob(\"/data/ziniu/rubric_datasets_merge/rewrite_rubrics.parquet\")\n",
        "        df = utils.read_df(paths, verbose=False)\n",
        "        reward_tasks = []\n",
        "        for _, example in df.iterrows():\n",
        "            text = example[\"prompt\"] + \" \" + example[\"response_0\"]\n",
        "            msgs = formatting.derender_conversation(name=\"grok\", rendered_conv=text)\n",
        "            rubrics = list(json.loads(example[\"rubric\"]).values())\n",
        "            reward_tasks.append(self.client.generate(messages=msgs, rubric_list=rubrics))\n",
        "        rewards_0 = await tqdm_asyncio.gather(*reward_tasks)\n",
        "        reward_tasks = []\n",
        "        for _, example in df.iterrows():\n",
        "            text = example[\"prompt\"] + \" \" + example[\"response_1\"]\n",
        "            msgs = formatting.derender_conversation(name=\"grok\", rendered_conv=text)\n",
        "            rubrics = list(json.loads(example[\"rubric\"]).values())\n",
        "            reward_tasks.append(self.client.generate(messages=msgs, rubric_list=rubrics))\n",
        "        rewards_1 = await tqdm_asyncio.gather(*reward_tasks)\n",
        "        df[\"reward_0\"] = [json.dumps(r, indent=2) for r in rewards_0]\n",
        "        df[\"reward_1\"] = [json.dumps(r, indent=2) for r in rewards_1]\n",
        "        df[\"reward_0_avg\"] = [r[\"reward\"] for r in rewards_0]\n",
        "        df[\"reward_1_avg\"] = [r[\"reward\"] for r in rewards_1]\n",
        "        df.to_parquet(\"/data/ziniu/rubric_datasets_merge/rewrite_rubrics_w_reward.parquet\")\n",
        "\n",
        "\n",
        "async def main(*args):\n",
        "    sampler = RMEvaluator(\n",
        "        reward_model_address=\"v5l-16.zhu.svc.kiy.x.ai\",\n",
        "        max_parallel=512,\n",
        "    )\n",
        "    await sampler.setup_clients()\n",
        "    await sampler.process()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ],
      "metadata": {
        "id": "PSU4ovQoRcuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verifier Results 10/19"
      ],
      "metadata": {
        "id": "q42mC0jByahI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths = glob.glob(\"/data/yuchen/rubric_datasets_human/rewrite_rubrics_w_reward_1000.parquet\")\n",
        "\n",
        "# paths = glob.glob(\"/data/yuchen/rubric_datasets_human/kiy-infer.qian.svc.kiy.x.ai_zero_shot_rubrics_reward.parquet\")\n",
        "\n",
        "paths = glob.glob(\"/data/yuchen/rubric_datasets_human/zero_shot_rubrics_v1_sft_ref_200_reward_verifier_no_cot.parquet\")\n",
        "\n",
        "df = utils.read_df(paths, verbose=False)"
      ],
      "metadata": {
        "id": "HjcdnHZ2OFDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "j68btEixOdgC",
        "outputId": "ca4940cf-f2e1-437d-ffe7-865a2a5610da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                prompt  \\\n",
              "0    Human: I'm getting my scuba certification in a...   \n",
              "1    Human: Given an article from the user, your jo...   \n",
              "2    Human: I want a summary of this article, title...   \n",
              "3    Human: I have a csv file containing weather da...   \n",
              "4    Human: The practice of referring to underwear ...   \n",
              "..                                                 ...   \n",
              "192  Human: What is the pole that the author is ref...   \n",
              "193  Human: Create 10 options for brief email bodie...   \n",
              "194  Human: I am planning to throw a gender reveal ...   \n",
              "195  Human: I am very interested in astronomy, and ...   \n",
              "196  System: Format every answer like an email to s...   \n",
              "\n",
              "                                                critic  \\\n",
              "0    Response B included locations that have flight...   \n",
              "1    *Note - I chose Complex Extraction because the...   \n",
              "2    Neither response does an amazing job of summar...   \n",
              "3    Model A assumes the data format of the file an...   \n",
              "4    While both responses successfully enhanced the...   \n",
              "..                                                 ...   \n",
              "192  Both responses get the answer to the question ...   \n",
              "193  Model B offers a better-formatted response giv...   \n",
              "194  Response B is much better for a few reasons. F...   \n",
              "195  Model A impressively notes the potential class...   \n",
              "196  Response A Evaluation Justification:\\n\\nTurn 3...   \n",
              "\n",
              "                             dataset          domain  \\\n",
              "0          surge_finegrain_general_5   Brainstorming   \n",
              "1              surge_finegrain_api_1      Extraction   \n",
              "2          surge_finegrain_general_4   Summarization   \n",
              "3           surge_finegrain_coding_5          coding   \n",
              "4          surge_finegrain_general_8       Rewriting   \n",
              "..                               ...             ...   \n",
              "192        surge_finegrain_general_5      Closed Q&A   \n",
              "193        surge_finegrain_general_7         Writing   \n",
              "194        surge_finegrain_general_3   Brainstorming   \n",
              "195        surge_finegrain_general_3  Classification   \n",
              "196  surge_finegrain_system_prompt_1         Writing   \n",
              "\n",
              "                                            response_0  \\\n",
              "0    Congratulations on getting your scuba certific...   \n",
              "1    Here is the analysis of the business advice pr...   \n",
              "2    Here's a summary of the article \"5 Signs of Lo...   \n",
              "3    Creating animations in Python involves using l...   \n",
              "4    **CENTURIES**\\n\\n- **20th** - \\n  - Mass produ...   \n",
              "..                                                 ...   \n",
              "192  The pole referred to in this excerpt is the No...   \n",
              "193  Subject: Schedule Your Call - Program Details\\...   \n",
              "194  Here are some ideas for the Shrek-themed gende...   \n",
              "195  The short story can be classified under the ge...   \n",
              "196  Subject: Did You Feng Shui? Plus, A Special Of...   \n",
              "\n",
              "                                            response_1  preference_overall  \\\n",
              "0    Here are 5 domestic and 5 international diving...                0.83   \n",
              "1    Here is the analysis of the credibility of the...                0.66   \n",
              "2    Title: 5 Signs Someone Might Need to Grow Up a...                0.33   \n",
              "3    Creating an animation in Python that reflects ...                1.00   \n",
              "4    **CENTURIES**\\n\\n- **20th** - \\n  - Mass produ...                0.33   \n",
              "..                                                 ...                 ...   \n",
              "192  The pole referred to by the author in this exc...                0.83   \n",
              "193  Subject: Schedule a Call - After School Progra...                0.83   \n",
              "194  Here are some ideas for creating humorous and ...                1.00   \n",
              "195  ### Evaluation of the Short Story:\\n\\n**Genre*...                0.33   \n",
              "196  Subject: ✨ Did Your Bedroom Get Its Feng Shui ...                0.66   \n",
              "\n",
              "                                                rubric  \\\n",
              "0    [\\n  \"The response should produce a list of at...   \n",
              "1    [\\n  \"The response should include advice title...   \n",
              "2    [\\n  \"There should be 5 main points to the art...   \n",
              "3    [\\n  \"The response identifies the need to incl...   \n",
              "4    [\\n  \"Response is placed in reverse chronologi...   \n",
              "..                                                 ...   \n",
              "192  [\\n  \"The response states the pole being refer...   \n",
              "193  [\\n  \"The response contains an email no more t...   \n",
              "194  [\\n  \"The response provides a list of ideas fo...   \n",
              "195  [\\n  \"Response classifies the story by genre.\"...   \n",
              "196  [\\n  \"The response includes the subject line.\"...   \n",
              "\n",
              "                                         orig_response  \\\n",
              "0     [\"The response should produce a list of at le...   \n",
              "1     [\"The response should include advice titles a...   \n",
              "2     [\"There should be 5 main points to the articl...   \n",
              "3     [\"The response identifies the need to include...   \n",
              "4     [\"Response is placed in reverse chronological...   \n",
              "..                                                 ...   \n",
              "192   [\"The response states the pole being referred...   \n",
              "193   [\"The response contains an email no more than...   \n",
              "194   [\"The response provides a list of ideas for w...   \n",
              "195   [\"Response classifies the story by genre.\", \"...   \n",
              "196   [\"The response includes the subject line.\", \"...   \n",
              "\n",
              "                                              messages  \\\n",
              "0    [{'content': 'You are an educational expert to...   \n",
              "1    [{'content': 'You are an educational expert to...   \n",
              "2    [{'content': 'You are an educational expert to...   \n",
              "3    [{'content': 'You are an educational expert to...   \n",
              "4    [{'content': 'You are an educational expert to...   \n",
              "..                                                 ...   \n",
              "192  [{'content': 'You are an educational expert to...   \n",
              "193  [{'content': 'You are an educational expert to...   \n",
              "194  [{'content': 'You are an educational expert to...   \n",
              "195  [{'content': 'You are an educational expert to...   \n",
              "196  [{'content': 'You are an educational expert to...   \n",
              "\n",
              "                                              reward_0  \\\n",
              "0    {\\n  \"verifications\": [\\n    {\\n      \"outputs...   \n",
              "1    {\\n  \"verifications\": [\\n    {\\n      \"outputs...   \n",
              "2    {\\n  \"verifications\": [\\n    {\\n      \"outputs...   \n",
              "3    {\\n  \"verifications\": [\\n    {\\n      \"outputs...   \n",
              "4    {\\n  \"verifications\": [\\n    {\\n      \"outputs...   \n",
              "..                                                 ...   \n",
              "192  {\\n  \"verifications\": [\\n    {\\n      \"outputs...   \n",
              "193  {\\n  \"verifications\": [\\n    {\\n      \"outputs...   \n",
              "194  {\\n  \"verifications\": [\\n    {\\n      \"outputs...   \n",
              "195  {\\n  \"verifications\": [\\n    {\\n      \"outputs...   \n",
              "196  {\\n  \"verifications\": [\\n    {\\n      \"outputs...   \n",
              "\n",
              "                                              reward_1  reward_0_avg  \\\n",
              "0    {\\n  \"verifications\": [\\n    {\\n      \"outputs...      0.500000   \n",
              "1    {\\n  \"verifications\": [\\n    {\\n      \"outputs...      0.384615   \n",
              "2    {\\n  \"verifications\": [\\n    {\\n      \"outputs...      1.000000   \n",
              "3    {\\n  \"verifications\": [\\n    {\\n      \"outputs...      0.714286   \n",
              "4    {\\n  \"verifications\": [\\n    {\\n      \"outputs...      1.000000   \n",
              "..                                                 ...           ...   \n",
              "192  {\\n  \"verifications\": [\\n    {\\n      \"outputs...      0.500000   \n",
              "193  {\\n  \"verifications\": [\\n    {\\n      \"outputs...      1.000000   \n",
              "194  {\\n  \"verifications\": [\\n    {\\n      \"outputs...      0.857143   \n",
              "195  {\\n  \"verifications\": [\\n    {\\n      \"outputs...      1.000000   \n",
              "196  {\\n  \"verifications\": [\\n    {\\n      \"outputs...      0.666667   \n",
              "\n",
              "     reward_1_avg  \n",
              "0        0.500000  \n",
              "1        0.307692  \n",
              "2        1.000000  \n",
              "3        0.714286  \n",
              "4        1.000000  \n",
              "..            ...  \n",
              "192      1.000000  \n",
              "193      1.000000  \n",
              "194      0.714286  \n",
              "195      1.000000  \n",
              "196      0.666667  \n",
              "\n",
              "[197 rows x 14 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>critic</th>\n",
              "      <th>dataset</th>\n",
              "      <th>domain</th>\n",
              "      <th>response_0</th>\n",
              "      <th>response_1</th>\n",
              "      <th>preference_overall</th>\n",
              "      <th>rubric</th>\n",
              "      <th>orig_response</th>\n",
              "      <th>messages</th>\n",
              "      <th>reward_0</th>\n",
              "      <th>reward_1</th>\n",
              "      <th>reward_0_avg</th>\n",
              "      <th>reward_1_avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Human: I'm getting my scuba certification in a...</td>\n",
              "      <td>Response B included locations that have flight...</td>\n",
              "      <td>surge_finegrain_general_5</td>\n",
              "      <td>Brainstorming</td>\n",
              "      <td>Congratulations on getting your scuba certific...</td>\n",
              "      <td>Here are 5 domestic and 5 international diving...</td>\n",
              "      <td>0.83</td>\n",
              "      <td>[\\n  \"The response should produce a list of at...</td>\n",
              "      <td>[\"The response should produce a list of at le...</td>\n",
              "      <td>[{'content': 'You are an educational expert to...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Human: Given an article from the user, your jo...</td>\n",
              "      <td>*Note - I chose Complex Extraction because the...</td>\n",
              "      <td>surge_finegrain_api_1</td>\n",
              "      <td>Extraction</td>\n",
              "      <td>Here is the analysis of the business advice pr...</td>\n",
              "      <td>Here is the analysis of the credibility of the...</td>\n",
              "      <td>0.66</td>\n",
              "      <td>[\\n  \"The response should include advice title...</td>\n",
              "      <td>[\"The response should include advice titles a...</td>\n",
              "      <td>[{'content': 'You are an educational expert to...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>0.307692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Human: I want a summary of this article, title...</td>\n",
              "      <td>Neither response does an amazing job of summar...</td>\n",
              "      <td>surge_finegrain_general_4</td>\n",
              "      <td>Summarization</td>\n",
              "      <td>Here's a summary of the article \"5 Signs of Lo...</td>\n",
              "      <td>Title: 5 Signs Someone Might Need to Grow Up a...</td>\n",
              "      <td>0.33</td>\n",
              "      <td>[\\n  \"There should be 5 main points to the art...</td>\n",
              "      <td>[\"There should be 5 main points to the articl...</td>\n",
              "      <td>[{'content': 'You are an educational expert to...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Human: I have a csv file containing weather da...</td>\n",
              "      <td>Model A assumes the data format of the file an...</td>\n",
              "      <td>surge_finegrain_coding_5</td>\n",
              "      <td>coding</td>\n",
              "      <td>Creating animations in Python involves using l...</td>\n",
              "      <td>Creating an animation in Python that reflects ...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>[\\n  \"The response identifies the need to incl...</td>\n",
              "      <td>[\"The response identifies the need to include...</td>\n",
              "      <td>[{'content': 'You are an educational expert to...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.714286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Human: The practice of referring to underwear ...</td>\n",
              "      <td>While both responses successfully enhanced the...</td>\n",
              "      <td>surge_finegrain_general_8</td>\n",
              "      <td>Rewriting</td>\n",
              "      <td>**CENTURIES**\\n\\n- **20th** - \\n  - Mass produ...</td>\n",
              "      <td>**CENTURIES**\\n\\n- **20th** - \\n  - Mass produ...</td>\n",
              "      <td>0.33</td>\n",
              "      <td>[\\n  \"Response is placed in reverse chronologi...</td>\n",
              "      <td>[\"Response is placed in reverse chronological...</td>\n",
              "      <td>[{'content': 'You are an educational expert to...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>Human: What is the pole that the author is ref...</td>\n",
              "      <td>Both responses get the answer to the question ...</td>\n",
              "      <td>surge_finegrain_general_5</td>\n",
              "      <td>Closed Q&amp;A</td>\n",
              "      <td>The pole referred to in this excerpt is the No...</td>\n",
              "      <td>The pole referred to by the author in this exc...</td>\n",
              "      <td>0.83</td>\n",
              "      <td>[\\n  \"The response states the pole being refer...</td>\n",
              "      <td>[\"The response states the pole being referred...</td>\n",
              "      <td>[{'content': 'You are an educational expert to...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>Human: Create 10 options for brief email bodie...</td>\n",
              "      <td>Model B offers a better-formatted response giv...</td>\n",
              "      <td>surge_finegrain_general_7</td>\n",
              "      <td>Writing</td>\n",
              "      <td>Subject: Schedule Your Call - Program Details\\...</td>\n",
              "      <td>Subject: Schedule a Call - After School Progra...</td>\n",
              "      <td>0.83</td>\n",
              "      <td>[\\n  \"The response contains an email no more t...</td>\n",
              "      <td>[\"The response contains an email no more than...</td>\n",
              "      <td>[{'content': 'You are an educational expert to...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>Human: I am planning to throw a gender reveal ...</td>\n",
              "      <td>Response B is much better for a few reasons. F...</td>\n",
              "      <td>surge_finegrain_general_3</td>\n",
              "      <td>Brainstorming</td>\n",
              "      <td>Here are some ideas for the Shrek-themed gende...</td>\n",
              "      <td>Here are some ideas for creating humorous and ...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>[\\n  \"The response provides a list of ideas fo...</td>\n",
              "      <td>[\"The response provides a list of ideas for w...</td>\n",
              "      <td>[{'content': 'You are an educational expert to...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.714286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>Human: I am very interested in astronomy, and ...</td>\n",
              "      <td>Model A impressively notes the potential class...</td>\n",
              "      <td>surge_finegrain_general_3</td>\n",
              "      <td>Classification</td>\n",
              "      <td>The short story can be classified under the ge...</td>\n",
              "      <td>### Evaluation of the Short Story:\\n\\n**Genre*...</td>\n",
              "      <td>0.33</td>\n",
              "      <td>[\\n  \"Response classifies the story by genre.\"...</td>\n",
              "      <td>[\"Response classifies the story by genre.\", \"...</td>\n",
              "      <td>[{'content': 'You are an educational expert to...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>System: Format every answer like an email to s...</td>\n",
              "      <td>Response A Evaluation Justification:\\n\\nTurn 3...</td>\n",
              "      <td>surge_finegrain_system_prompt_1</td>\n",
              "      <td>Writing</td>\n",
              "      <td>Subject: Did You Feng Shui? Plus, A Special Of...</td>\n",
              "      <td>Subject: ✨ Did Your Bedroom Get Its Feng Shui ...</td>\n",
              "      <td>0.66</td>\n",
              "      <td>[\\n  \"The response includes the subject line.\"...</td>\n",
              "      <td>[\"The response includes the subject line.\", \"...</td>\n",
              "      <td>[{'content': 'You are an educational expert to...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>{\\n  \"verifications\": [\\n    {\\n      \"outputs...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>197 rows × 14 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import spearmanr, kendalltau\n",
        "\n",
        "\n",
        "def calculate_acc(df):\n",
        "  import numpy as np\n",
        "  from scipy.stats import spearmanr, kendalltau\n",
        "  score_human = np.stack([1 - df['preference_overall'],  df['preference_overall']], axis=1)\n",
        "  score_rubric_verifier = np.stack([df['reward_0_avg'], df['reward_1_avg']], axis=1)\n",
        "\n",
        "  valid_indices = (score_human[: , 0] != score_human[:, 1]) & (score_rubric_verifier[:, 0]!= score_rubric_verifier[:, 1])\n",
        "  score_human = score_human[valid_indices, :]\n",
        "  score_rubric_verifier = score_rubric_verifier[valid_indices, :]\n",
        "\n",
        "  # Sample preference scores and true labels\n",
        "  label_human = score_human[: , 0] < score_human[:, 1]\n",
        "  label_rubric_verifier = score_rubric_verifier[:, 0] < score_rubric_verifier[: , 1]\n",
        "  print(label_human.shape, label_rubric_verifier.shape)\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = np.sum(label_human == label_rubric_verifier) / len(label_human)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "\n",
        "  alllen = len(label_human)\n",
        "  # # Calculate Spearman's correlation\n",
        "  spearman_corrs = np.array([spearmanr(score_human[i], score_rubric_verifier[i])[0] for i in range(alllen)])\n",
        "  spearman_corrs = np.mean(spearman_corrs[~np.isnan(spearman_corrs)])\n",
        "  print(\"Spearman's Correlation:\", spearman_corrs)\n",
        "\n",
        "  # # Calculate Kendall's Tau correlation\n",
        "  kendall_corr, _ = kendalltau(label_rubric_verifier, label_human)\n",
        "  print(\"Kendall's Tau Correlation:\", kendall_corr)\n",
        "\n",
        "  metrics = {\"count\" : label_human.shape[0], \"accuracy\": accuracy, \"spearman_corrs\": spearman_corrs,  \"kendall_corr\": kendall_corr}\n",
        "  return metrics\n",
        "\n",
        "all_metrics = calculate_acc(df)\n",
        "results = [{\"domain\": \"ALL\"} | all_metrics]\n",
        "all_cat = df['domain'].unique()\n",
        "for cat in all_cat:\n",
        "  print(cat)\n",
        "  metrics = calculate_acc(df[df['domain'] == cat])\n",
        "  metrics.update({\"domain\": cat})\n",
        "  results.append(metrics)\n",
        "\n",
        "\n",
        "result_df = pd.DataFrame(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPw1-3xlPZdS",
        "outputId": "ec9407b7-a853-49c9-b86d-53d329bc13a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(63,) (63,)\n",
            "Accuracy: 0.6507936507936508\n",
            "Spearman's Correlation: 0.3015873015873015\n",
            "Kendall's Tau Correlation: 0.30879642674374563\n",
            "Brainstorming\n",
            "(9,) (9,)\n",
            "Accuracy: 0.5555555555555556\n",
            "Spearman's Correlation: 0.1111111111111111\n",
            "Kendall's Tau Correlation: -0.25\n",
            "Extraction\n",
            "(4,) (4,)\n",
            "Accuracy: 0.5\n",
            "Spearman's Correlation: 0.0\n",
            "Kendall's Tau Correlation: nan\n",
            "Summarization\n",
            "(6,) (6,)\n",
            "Accuracy: 0.16666666666666666\n",
            "Spearman's Correlation: -0.6666666666666666\n",
            "Kendall's Tau Correlation: -0.6324555320336758\n",
            "coding\n",
            "(4,) (4,)\n",
            "Accuracy: 1.0\n",
            "Spearman's Correlation: 0.9999999999999999\n",
            "Kendall's Tau Correlation: nan\n",
            "Rewriting\n",
            "(3,) (3,)\n",
            "Accuracy: 0.6666666666666666\n",
            "Spearman's Correlation: 0.3333333333333333\n",
            "Kendall's Tau Correlation: nan\n",
            "Open Q&A\n",
            "(10,) (10,)\n",
            "Accuracy: 0.7\n",
            "Spearman's Correlation: 0.39999999999999997\n",
            "Kendall's Tau Correlation: 0.5345224838248489\n",
            "Classification\n",
            "(6,) (6,)\n",
            "Accuracy: 0.6666666666666666\n",
            "Spearman's Correlation: 0.3333333333333333\n",
            "Kendall's Tau Correlation: 0.24999999999999997\n",
            "Writing\n",
            "(5,) (5,)\n",
            "Accuracy: 1.0\n",
            "Spearman's Correlation: 0.9999999999999998\n",
            "Kendall's Tau Correlation: 1.0\n",
            "Persona\n",
            "(2,) (2,)\n",
            "Accuracy: 0.5\n",
            "Spearman's Correlation: 0.0\n",
            "Kendall's Tau Correlation: nan\n",
            "Conversational\n",
            "(2,) (2,)\n",
            "Accuracy: 1.0\n",
            "Spearman's Correlation: 0.9999999999999999\n",
            "Kendall's Tau Correlation: 1.0\n",
            "Advice\n",
            "(4,) (4,)\n",
            "Accuracy: 0.5\n",
            "Spearman's Correlation: 0.0\n",
            "Kendall's Tau Correlation: -0.33333333333333337\n",
            "Code Recommendation\n",
            "(1,) (1,)\n",
            "Accuracy: 1.0\n",
            "Spearman's Correlation: 0.9999999999999999\n",
            "Kendall's Tau Correlation: nan\n",
            "Closed Q&A\n",
            "(2,) (2,)\n",
            "Accuracy: 1.0\n",
            "Spearman's Correlation: 0.9999999999999999\n",
            "Kendall's Tau Correlation: 1.0\n",
            "Help Learn Programming Concepts\n",
            "(2,) (2,)\n",
            "Accuracy: 0.5\n",
            "Spearman's Correlation: 0.0\n",
            "Kendall's Tau Correlation: nan\n",
            "Other\n",
            "(1,) (1,)\n",
            "Accuracy: 1.0\n",
            "Spearman's Correlation: 0.9999999999999999\n",
            "Kendall's Tau Correlation: nan\n",
            "Code Modification\n",
            "(0,) (0,)\n",
            "Accuracy: nan\n",
            "Spearman's Correlation: nan\n",
            "Kendall's Tau Correlation: nan\n",
            "Code Creation\n",
            "(1,) (1,)\n",
            "Accuracy: 0.0\n",
            "Spearman's Correlation: -0.9999999999999999\n",
            "Kendall's Tau Correlation: nan\n",
            "Help Understand Given Code\n",
            "(1,) (1,)\n",
            "Accuracy: 1.0\n",
            "Spearman's Correlation: 0.9999999999999999\n",
            "Kendall's Tau Correlation: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_92588/1931376049.py:21: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  accuracy = np.sum(label_human == label_rubric_verifier) / len(label_human)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result_df.sort_values('count', ascending=False)\n",
        "# df[(df[\"preference_overall\"] - 0.5) * (df[\"reward_0_avg\"] - df[\"reward_1_avg\"]) > 0 ]\n"
      ],
      "metadata": {
        "id": "bi98Xu85dOXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "6qjZY5TNlI7I",
        "outputId": "44699e3b-d3af-440c-e1c0-15244af630f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             domain  count  accuracy  spearman_corrs  \\\n",
              "0                               ALL     63  0.650794        0.301587   \n",
              "6                          Open Q&A     10  0.700000        0.400000   \n",
              "1                     Brainstorming      9  0.555556        0.111111   \n",
              "3                     Summarization      6  0.166667       -0.666667   \n",
              "7                    Classification      6  0.666667        0.333333   \n",
              "8                           Writing      5  1.000000        1.000000   \n",
              "2                        Extraction      4  0.500000        0.000000   \n",
              "4                            coding      4  1.000000        1.000000   \n",
              "11                           Advice      4  0.500000        0.000000   \n",
              "5                         Rewriting      3  0.666667        0.333333   \n",
              "13                       Closed Q&A      2  1.000000        1.000000   \n",
              "14  Help Learn Programming Concepts      2  0.500000        0.000000   \n",
              "9                           Persona      2  0.500000        0.000000   \n",
              "10                   Conversational      2  1.000000        1.000000   \n",
              "12              Code Recommendation      1  1.000000        1.000000   \n",
              "15                            Other      1  1.000000        1.000000   \n",
              "17                    Code Creation      1  0.000000       -1.000000   \n",
              "18       Help Understand Given Code      1  1.000000        1.000000   \n",
              "16                Code Modification      0       NaN             NaN   \n",
              "\n",
              "    kendall_corr  \n",
              "0       0.308796  \n",
              "6       0.534522  \n",
              "1      -0.250000  \n",
              "3      -0.632456  \n",
              "7       0.250000  \n",
              "8       1.000000  \n",
              "2            NaN  \n",
              "4            NaN  \n",
              "11     -0.333333  \n",
              "5            NaN  \n",
              "13      1.000000  \n",
              "14           NaN  \n",
              "9            NaN  \n",
              "10      1.000000  \n",
              "12           NaN  \n",
              "15           NaN  \n",
              "17           NaN  \n",
              "18           NaN  \n",
              "16           NaN  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>domain</th>\n",
              "      <th>count</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>spearman_corrs</th>\n",
              "      <th>kendall_corr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ALL</td>\n",
              "      <td>63</td>\n",
              "      <td>0.650794</td>\n",
              "      <td>0.301587</td>\n",
              "      <td>0.308796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Open Q&amp;A</td>\n",
              "      <td>10</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.534522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Brainstorming</td>\n",
              "      <td>9</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>-0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Summarization</td>\n",
              "      <td>6</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>-0.666667</td>\n",
              "      <td>-0.632456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Classification</td>\n",
              "      <td>6</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Writing</td>\n",
              "      <td>5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Extraction</td>\n",
              "      <td>4</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>coding</td>\n",
              "      <td>4</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Advice</td>\n",
              "      <td>4</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Rewriting</td>\n",
              "      <td>3</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Closed Q&amp;A</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Help Learn Programming Concepts</td>\n",
              "      <td>2</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Persona</td>\n",
              "      <td>2</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Conversational</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Code Recommendation</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Other</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Code Creation</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Help Understand Given Code</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Code Modification</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "df1 = result_df_ref\n",
        "df2 = result_df\n",
        "df1 = df1[df1['domain'] != 'ALL']\n",
        "df2 = df2[df2['domain'] != 'ALL']\n",
        "\n",
        "# Merge both dataframes on domain\n",
        "merged_df = pd.merge(df1, df2, on='domain', suffixes=('_df1', '_df2'))\n",
        "\n",
        "# Calculate percentiles for width scaling (you can adjust the percentile scaling as needed)\n",
        "merged_df['percentile_width_df1'] = merged_df['count_df1'] / merged_df['count_df1'].sum()\n",
        "merged_df['percentile_width_df2'] = merged_df['count_df2'] / merged_df['count_df2'].sum()\n",
        "\n",
        "# Define bar positions\n",
        "index = np.arange(len(merged_df))\n",
        "bar_width_df1 = merged_df['percentile_width_df1'] * 2.0  # Scaling the width for better visualization\n",
        "bar_width_df2 = merged_df['percentile_width_df2'] * 2.0\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(20, 6))\n",
        "\n",
        "# Plot bars\n",
        "bars1 = ax.bar(index - bar_width_df1 / 2, merged_df['accuracy_df1'], width=bar_width_df1, label='Prompt+Critic Accurac: 0.80', align='center')\n",
        "bars2 = ax.bar(index + bar_width_df2 / 2, merged_df['accuracy_df2'], width=bar_width_df2, label='Prompt SFT Accuracy: 0.65', align='center')\n",
        "\n",
        "# Labeling\n",
        "ax.set_xlabel('Domain')\n",
        "ax.set_ylabel('Rubric Verification Accuracy')\n",
        "ax.set_title('Rubric Verification Accuracy by Domain, Bar Width by Percentile of Count')\n",
        "ax.set_xticks(index)\n",
        "ax.set_xticklabels(merged_df['domain'])\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "Gd58niTo3YWL",
        "outputId": "f31ff3bb-6c2f-4fd0-ef39-f87d43fff845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x600 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, ast, json\n",
        "def parse_verify(response: str) -> float | None:\n",
        "    pattern = r\"<rubrics\\b[^>]*>(.*?)<\\/rubrics>\"\n",
        "\n",
        "    m = re.search(pattern, response, re.DOTALL)\n",
        "    if m:\n",
        "        rubrics_content = m.group(1).strip()\n",
        "        res = ast.literal_eval(rubrics_content)\n",
        "        return json.dumps(res, indent=2)\n",
        "    return None\n",
        "\n",
        "\n",
        "response = \"\"\"\n",
        "<analysis>The query \"Diamond League sounds like a pokemon game. Are there other leagues like this?\" requires a casual response. The key requirements are:\n",
        "\n",
        "1. **Similarity in Name**: Look for leagues with names that sound like they could be from a video game or entertainment context, similar to \"Diamond League\".\n",
        "\n",
        "2. **Comparison**: The response should acknowledge or implicitly recognize the Diamond League's existence while introducing similar leagues in athletics.\n",
        "\n",
        "3. **Limited Response**: The response must be concise and limited in nature (3 sentences as per query), mimicking casual chat dialogue.\n",
        "\n",
        "4. **Existence**: Confirm if there are other athletic leagues or series with quirky or game-like sounding names.\n",
        "\n",
        "These criteria ensure the answer addresses the playful nature of the question while providing useful information within the constraints provided.\n",
        "</analysis>\n",
        "\n",
        "<rubrics>\n",
        "{\n",
        "    \"criterion_1\": \"The response includes leagues with names that could be perceived as playful or game-like.\",\n",
        "    \"criterion_2\": \"The response hints at or directly acknowledges the existence of the Diamond League.\",\n",
        "    \"criterion_3\": \"The response does not exceed 3 sentences in length.\",\n",
        "    \"criterion_4\": \"Leagues introduced are relevant to athletics.\"\n",
        "}\n",
        "</rubrics>\n",
        "\"\"\"\n",
        "\n",
        "# parse_verify(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5cfMxaSXZ1w",
        "outputId": "dc23ad0e-d3b4-4a3f-8b03-5a8123dea7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"criterion_1\": \"The response includes leagues with names that could be perceived as playful or game-like.\",\\n  \"criterion_2\": \"The response hints at or directly acknowledges the existence of the Diamond League.\",\\n  \"criterion_3\": \"The response does not exceed 3 sentences in length.\",\\n  \"criterion_4\": \"Leagues introduced are relevant to athletics.\"\\n}'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import ast\n",
        "# import json\n",
        "# import logging\n",
        "# import re\n",
        "# from abc import ABC, abstractmethod\n",
        "# from typing import Any\n",
        "\n",
        "# import markdown\n",
        "\n",
        "# from xlm.config import Config, configclass\n",
        "# from xlm.posttrain.data import formatting\n",
        "# from xlm.reward_client import RubricCriticRewardClient\n",
        "# from xlm.utils import configure_logging\n",
        "\n",
        "\n",
        "# from xlm.sampling_client import SamplingClient\n",
        "\n",
        "# from xlm.reward_client import RubricCriticRewardClient\n",
        "\n",
        "# from xlm.posttrain.data.preprocess.filter_custom import CustomFilter\n",
        "\n",
        "\n",
        "# class VerifierRubricsFilter(CustomFilter):\n",
        "#     \"\"\"Removes verifier bad results according to synthetic rubrics.\"\"\"\n",
        "#     rubric_gen_templates: str\n",
        "#     rubric_gen_model: str\n",
        "#     rubric_verifier_model : str\n",
        "#     rubric_gen_client: SamplingClient = None\n",
        "#     verifier_client: RubricCriticRewardClient = None\n",
        "#     max_retry: int = 2\n",
        "\n",
        "#     def __post_init__(self):\n",
        "#         self.rubric_threshold = 0.5\n",
        "#         self.max_parallel = 512\n",
        "#         self.max_retry = 2\n",
        "\n",
        "#         # rubric_gen model\n",
        "#         from xlm.posttrain.evals import rubric_proposer\n",
        "#         self.rubric_gen_templates = rubric_proposer.RUBRIC_PROPOSER_PROMPT_V1 + rubric_proposer.RUBRIC_PROPOSER_PROMPT_no_response\n",
        "\n",
        "#         self.rubric_gen_model = \"v5l-1010-sft-full.yuchen.svc.kiy.x.ai\"\n",
        "#         self.rubric_gen_client = SamplingClient(oai_max_parallel=1, llama3_max_parallel=1024)\n",
        "#         self.rubric_gen_client._grok_client.register_model(\n",
        "#             model=\"rubric-gen-model\",\n",
        "#             model_address=self.rubric_gen_model,\n",
        "#             max_parallel=self.max_parallel,\n",
        "#         )\n",
        "#         self.rubric_gen_sampling_args = {\n",
        "#             \"seed\": 0,\n",
        "#             \"temperature\": 1.0,\n",
        "#             \"top_p\": 1.0,\n",
        "#             \"max_tokens\": 4096,\n",
        "#         }\n",
        "\n",
        "#         # rubric verifier model\n",
        "#         self.rubric_verifier_model = \"v5l-1010-sft-full.yuchen.svc.kiy.x.ai\"\n",
        "#         self.verifier_version = \"no_cot\"\n",
        "#         self.verifier_client = RubricCriticRewardClient(\n",
        "#             address=self.rubric_verifier_model,\n",
        "#             max_parallel=self.max_parallel,\n",
        "#             verifier_version=self.verifier_version,\n",
        "#         )\n",
        "\n",
        "#     def rubrics_gen_message(self, messages: list[str]) -> list[str]:\n",
        "#         # rubric_gen model\n",
        "#         rubrics_msgs = messages[: -1]\n",
        "#         query = rubrics_msgs[-1]\n",
        "#         history = \"\"\n",
        "#         if len(rubrics_msgs) > 1:\n",
        "#             history = formatting.render_conversation(name=\"grok\", messages=rubrics_msgs[:-1])\n",
        "#             if history.endswith(\"<|separator|>\"):\n",
        "#                 history = history[: -len(\"<|separator|>\")]\n",
        "#         rubrics_gen_messages = [\n",
        "#             {\n",
        "#                 \"role\": \"user\",\n",
        "#                 \"content\": self.rubric_gen_templates.format(\n",
        "#                     history=history,\n",
        "#                     prompt=query[\"content\"],\n",
        "#                     # critic=example[\"critic\"],\n",
        "#                     # response0=example[\"conversation0\"][-1][\"content\"],\n",
        "#                     # response1=example[\"conversation1\"][-1][\"content\"],\n",
        "#                 ),\n",
        "#             }\n",
        "#         ]\n",
        "#         return rubrics_gen_messages\n",
        "\n",
        "#     def parse_rubrics(self, response: str) -> float | None:\n",
        "#         response = response.replace(\"*\", \"\")\n",
        "#         m = re.search(r\"<rubrics\\b[^>]*>(.*?)<\\/rubrics>\", response, re.DOTALL)\n",
        "#         if m:\n",
        "#             rubrics_content = m.group(1).strip()\n",
        "#             if \"```json\" in rubrics_content:\n",
        "#                 m = re.search(r\"\\s*```json\\s*(.*?)\\s*```\", rubrics_content, re.DOTALL)\n",
        "#                 if m:\n",
        "#                     rubrics_content = m.group(1).strip()\n",
        "#                 rubrics_content = rubrics_content.replace(\"```python\", \"\").replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "#             res = ast.literal_eval(rubrics_content)\n",
        "#             return json.dumps(res, indent=2)\n",
        "#         m = re.search(r\"\\s*```json\\s*(.*?)\\s*```\", response, re.DOTALL)\n",
        "#         if m:\n",
        "#             res = m.group(1).strip()\n",
        "#             res = res.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "#             res = ast.literal_eval(res)\n",
        "#             return json.dumps(res, indent=2)\n",
        "#         m = re.search(r\"\\{.*?\\}\", response, re.DOTALL)\n",
        "#         if m:\n",
        "#             res = ast.literal_eval(m.group())\n",
        "#             return json.dumps(res, indent=2)\n",
        "#         # else we just directly output res? it shall be json parsable?\n",
        "#         return None\n",
        "\n",
        "#     async def generate_rubrics(self, messages: list[dict[str, Any]]) -> dict[str, Any]:\n",
        "#         idx = 0\n",
        "#         answer = None\n",
        "#         while idx < self.max_retry + 1:\n",
        "#             response = await self.rubric_client.generate(\n",
        "#                 model=self.rubric_gen_model, messages=messages, **self.rubric_gen_sampling_args\n",
        "#             )\n",
        "#             answer = self.parse_rubrics(response)\n",
        "#             if answer is not None:\n",
        "#                 break\n",
        "#             idx += 1\n",
        "#         return answer\n",
        "\n",
        "#     async def verify_rubrics(self, verifier_msgs: list[str], rubrics: list[str] = []) -> float:\n",
        "#         rubric_verifiers = await self.verifier_client.generate(messages=verifier_msgs, rubric_list=rubrics)\n",
        "#         return rubric_verifiers\n",
        "\n",
        "#     async def filter(self, message: str,  messages: list[str ] = None) -> bool:\n",
        "#         rubrics = [] # list of str\n",
        "\n",
        "#         rubrics_gen_messages = self.rubrics_gen_message(messages=messages)\n",
        "\n",
        "#         rubrics = await self.generate_rubrics(rubrics_gen_messages)\n",
        "\n",
        "#         rubrics_results_answers = await self.verify_rubrics(messages, rubrics=rubrics)\n",
        "\n",
        "#         rubrics_len = len(rubrics)\n",
        "#         reward = rubrics_results_answers[\"reward\"]\n",
        "#         if rubrics_len > 0:\n",
        "#             return reward > self.rubric_threshold * rubrics_len\n",
        "#         return True\n",
        "\n",
        "#     def __repr__(self):\n",
        "#         return f\"Bad verifier results for: {self.role}\""
      ],
      "metadata": {
        "id": "Npei5u-H3noF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1t7Sgly5eI84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rubric Filters 10/21"
      ],
      "metadata": {
        "id": "HDDFE2IYylWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import asyncio\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "from typing import Any\n",
        "import pandas as pd\n",
        "\n",
        "import uuid\n",
        "from xlm.config import configclass\n",
        "from xlm.posttrain import utils\n",
        "from xlm.posttrain.data import formatting\n",
        "from xlm.posttrain.evals.eval_base import SamplerEvalBase\n",
        "from xlm.sampling_client import SamplingClient"
      ],
      "metadata": {
        "id": "nXz6-HfOwDF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/data/datasets/preferences-v2/mixture/surge1t10/train/part-00001-of-00067.parquet\"\n",
        "df_data = utils.read_df(glob.glob(data_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lJFfXDlzQcN",
        "outputId": "5ca96e74-dbe3-4b15-f121-8bb55ffbe2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xlm.posttrain.data.preprocess.filter_custom import VerifierRubricsFilter\n",
        "rubric_filters = VerifierRubricsFilter(role=\"assistant\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf2aI7Z00B-r",
        "outputId": "de726c09-e52f-42bd-a470-9b3caa3b2cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-10-18 00:58:16,355 W] \u001b[2;36m[boxn-0:78741] sampling_client:1066:\u001b[0m Expecting 1 or more API keys in the OPENAI_API_KEY environment variable, but got: None\n",
            "\u001b[2m[2024-10-18 00:58:16,356 W] \u001b[2;36m[boxn-0:78741] sampling_client:871:\u001b[0m Expecting 1 or more API keys in the ANTHROPIC_API_KEY environment variable, but got: None\n",
            "\u001b[2m[2024-10-18 00:58:16,356 W] \u001b[2;36m[boxn-0:78741] sampling_client:740:\u001b[0m Environment var `GEMINI_API_KEY` not set.\n",
            "\u001b[2m[2024-10-18 00:58:16,411 E] \u001b[2;36m[boxn-0:78741] sampling_client:988:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n",
            "\u001b[2m[2024-10-18 00:58:16,564 W] \u001b[2;36m[boxn-0:78741] sampling_client:1874:\u001b[0m Expecting 1 or more API keys in the XAI_API_KEY environment variable, but got: None\n",
            "\u001b[2m[2024-10-18 00:58:16,567 W] \u001b[2;36m[boxn-0:78741] sampling_client:1066:\u001b[0m Expecting 1 or more API keys in the OPENAI_API_KEY environment variable, but got: None\n",
            "\u001b[2m[2024-10-18 00:58:16,567 W] \u001b[2;36m[boxn-0:78741] sampling_client:871:\u001b[0m Expecting 1 or more API keys in the ANTHROPIC_API_KEY environment variable, but got: None\n",
            "\u001b[2m[2024-10-18 00:58:16,568 W] \u001b[2;36m[boxn-0:78741] sampling_client:740:\u001b[0m Environment var `GEMINI_API_KEY` not set.\n",
            "\u001b[2m[2024-10-18 00:58:16,621 E] \u001b[2;36m[boxn-0:78741] sampling_client:988:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n",
            "\u001b[2m[2024-10-18 00:58:16,764 W] \u001b[2;36m[boxn-0:78741] sampling_client:1874:\u001b[0m Expecting 1 or more API keys in the XAI_API_KEY environment variable, but got: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages_examples = df_data['conversation0']\n",
        "for i in range(10):\n",
        "  messages = messages_examples[i].tolist()\n",
        "  print(json.dumps(messages, indent=2))\n",
        "  filter = await rubric_filters.filter(message=messages[-1], messages=messages)\n",
        "  print(filter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHWZC-_93R7i",
        "outputId": "4c230bd2-baba-4c57-85b1-60b6e3ff3a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"content\": \"What are the differences in the types of pareto distributions? Which is most often used in actuarial science? \",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Pareto distributions are continuous probability distributions that are often used to model heavy-tailed phenomena like income distribution, wealth distribution, sizes of cities, insurance claims, etc. There are several types of Pareto distributions, each with slightly different mathematical properties. Here are some main types:\\n\\n1. **Pareto Type I (The Classic Pareto Distribution)**:\\n   - It is defined by the scale parameter (\\u03b1 or xm) and shape parameter (\\u03b1).\\n   - Probability Density Function (PDF): \\\\(f(x) = \\\\frac{\\\\alpha x_m^{\\\\alpha}}{x^{\\\\alpha+1}}\\\\) for \\\\(x \\\\geq x_m\\\\)\\n   - Cumulative Distribution Function (CDF): \\\\(F(x) = 1 - \\\\left(\\\\frac{x_m}{x}\\\\right)^\\\\alpha\\\\)\\n\\n2. **Pareto Type II (also known as Lomax Distribution)**:\\n   - Adds a location parameter (\\u03bc), shifting the distribution.\\n   - PDF: \\\\(f(x) = \\\\frac{\\\\alpha}{\\\\sigma}\\\\left(1 + \\\\frac{x - \\\\mu}{\\\\sigma}\\\\right)^{-(\\\\alpha + 1)}\\\\)\\n   - This can be useful when the phenomenon does not start at a fixed minimum value.\\n\\n3. **Pareto Type III**:\\n   - Includes a further shape parameter that allows for more flexibility in the tail behavior, particularly useful for modeling very heavy-tailed data.\\n\\n4. **Pareto Type IV**:\\n   - This version includes inequality parameters allowing for a more flexible shape, particularly at the lower end of the scale.\\n\\n5. **Generalized Pareto Distribution (GPD)**:\\n   - This is commonly used in extreme value theory for modeling exceedances over a threshold, which is different from modeling the entire distribution of a dataset but closely related to Pareto distributions. GPD has three parameters: location (\\u03bc), scale (\\u03c3), and shape (\\u03be).\\n   - PDF: \\\\(f(x) = \\\\frac{1}{\\\\sigma}\\\\left(1 + \\\\xi \\\\frac{x - \\\\mu}{\\\\sigma}\\\\right)^{-(1/\\\\xi + 1)}\\\\) for \\\\(x > \\\\mu\\\\) when \\\\(\\\\xi \\\\geq 0\\\\), and \\\\(\\\\mu \\\\leq x \\\\leq \\\\mu - \\\\sigma / \\\\xi\\\\) when \\\\(\\\\xi < 0\\\\)\\n\\nIn actuarial science, where the focus is often on modeling large, rare events (like major insurance claims), the most commonly used are:\\n\\n- **Pareto Type II (Lomax Distribution)**: \\n  - It's particularly useful in insurance because it can model the entire range of claim sizes with the addition of a location parameter, thus accommodating both the body and the tail of the distribution effectively.\\n\\n- **Generalized Pareto Distribution (GPD)**:\\n  - In the context of extreme value theory, GPD is extensively used in actuarial science for modeling the size of large claims over high thresholds (excess-of-loss), which is crucial for reinsurance and setting premium rates for catastrophic events.\\n\\nThe choice between these distributions would depend on the specific requirements of the analysis, such as whether you're looking at all claims or only those above a certain threshold (where GPD might be more appropriate). Actuaries might use these distributions to model fire insurance claims, earthquake damage, or any scenario where most losses are small, but occasional extreme events cause very large losses.\",\n",
            "    \"role\": \"assistant\"\n",
            "  }\n",
            "]\n",
            "\u001b[2m[2024-10-18 00:58:21,954 I] \u001b[2;36m[boxn-0:78741] filter_custom:335:\u001b[0m rubrics:\n",
            "['The response distinguishes between different types of Pareto distributions.', 'The differences between each type of Pareto distribution are explained.', 'The response identifies one single type of Pareto distribution as the most used in actuarial science.', 'The response provide a proper reason why this type is widely used in actuarial science.']\n",
            "\u001b[2m[2024-10-18 00:58:22,237 I] \u001b[2;36m[boxn-0:78741] filter_custom:340:\u001b[0m raw verifier output:\n",
            "{\n",
            "  \"The response distinguishes between different types of Pareto distributions.\": \"yes\",\n",
            "  \"The differences between each type of Pareto distribution are explained.\": \"yes\",\n",
            "  \"The response identifies one single type of Pareto distribution as the most used in actuarial science.\": \"no\",\n",
            "  \"The response provide a proper reason why this type is widely used in actuarial science.\": \"yes\"\n",
            "}\n",
            "\u001b[2m[2024-10-18 00:58:22,238 I] \u001b[2;36m[boxn-0:78741] filter_custom:344:\u001b[0m reward: 0.75\n",
            "False\n",
            "[\n",
            "  {\n",
            "    \"content\": \"Give me a list of one line joke to say on a very hot day so that a pest control sales man can use for his opening ice breaker\",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"1. \\\"Is it hot enough for you, or should I bring a flamethrower to this conversation?\\\"\\n\\n2. \\\"I'm not saying it's hot outside, but I just saw a mirage of an ice cream truck.\\\"\\n\\n3. \\\"It's so hot today, I'm considering moving to the Arctic just for the breeze.\\\"\\n\\n4. \\\"I hope you're not afraid of a little sweat because I'm about to melt like an ice cube in a microwave.\\\"\\n\\n5. \\\"Is it just me, or does it feel like we're standing in the middle of a giant oven?\\\"\\n\\n6. \\\"I'm not sure what's hotter, the weather or my sales pitch. Let me know after I'm done.\\\"\\n\\n7. \\\"It's so hot, I feel like I'm auditioning for a part in a desert movie.\\\"\\n\\n8. \\\"I know it's a hot day, but don't worry, my pest control services are even hotter.\\\"\\n\\n9. \\\"If you think this heat is intense, wait until you see how we exterminate those pests.\\\"\\n\\n10. \\\"It's a scorcher today, but I promise my services will leave you feeling as cool as a cucumber.\\\"\",\n",
            "    \"role\": \"assistant\"\n",
            "  }\n",
            "]\n",
            "\u001b[2m[2024-10-18 00:58:27,137 I] \u001b[2;36m[boxn-0:78741] filter_custom:335:\u001b[0m rubrics:\n",
            "['Each joke provided is listed as a single line.', 'Each line provided contains a sufficient humorous content to be considered a one-liner joke.', 'All jokes mention, imply, or are clearly relevant to hot weather.', 'All jokes would not cause offence or upset when used in a jest.', \"All of the jokes are specific to the salesman's context of pest control.\"]\n",
            "\u001b[2m[2024-10-18 00:58:27,311 I] \u001b[2;36m[boxn-0:78741] filter_custom:340:\u001b[0m raw verifier output:\n",
            "{\n",
            "  \"Each joke provided is listed as a single line.\": \"yes\",\n",
            "  \"Each line provided contains a sufficient humorous content to be considered a one-liner joke.\": \"yes\",\n",
            "  \"All jokes mention, imply, or are clearly relevant to hot weather.\": \"yes\",\n",
            "  \"All jokes would not cause offence or upset when used in a jest.\": \"yes\",\n",
            "  \"All of the jokes are specific to the salesman's context of pest control.\": \"no\"\n",
            "}\n",
            "\u001b[2m[2024-10-18 00:58:27,312 I] \u001b[2;36m[boxn-0:78741] filter_custom:344:\u001b[0m reward: 0.8\n",
            "False\n",
            "[\n",
            "  {\n",
            "    \"content\": \"Can sit-ups cause pain on the tailbone? What are some common home remedies for tailbone pain? Do not suggest any medication. \",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Yes, sit-ups can cause pain on the tailbone, especially if they are not performed correctly or if there is an existing injury. The tailbone, or coccyx, is the small bone at the base of the spine, and it can become bruised, strained, or even fractured due to improper exercise techniques or excessive pressure.\\n\\nSome common home remedies for tailbone pain include:\\n\\n1. Ice: Applying ice to the affected area for 15-20 minutes several times a day can help reduce inflammation and numb the pain.\\n\\n2. Heat: After the initial swelling has gone down, applying heat through a heating pad or warm compress can help relax the muscles and promote blood flow to the area.\\n\\n3. Cushioning: Using a soft cushion or a doughnut-shaped pillow when sitting can help alleviate pressure on the tailbone and provide relief.\\n\\n4. Proper posture: Maintaining good posture while sitting and standing can help reduce strain on the tailbone and promote healing.\\n\\n5. Gentle stretching: Performing gentle stretching exercises, such as pelvic tilts, can help relieve tightness and discomfort in the lower back and tailbone area.\\n\\n6. Yoga: Practicing yoga poses that focus on stretching and strengthening the muscles around the tailbone, such as child's pose and cat-cow stretch, can provide relief and support healing.\\n\\n7. Epsom salt baths: Soaking in a warm bath with Epsom salts can help relax muscles and reduce inflammation.\\n\\n8. Avoid aggravating activities: Refraining from activities that cause pain or discomfort, such as sitting for extended periods, can help prevent further strain and allow the tailbone to heal.\\n\\n9. Maintain a healthy weight: Excess weight can put additional strain on the tailbone, so maintaining a healthy weight can help reduce the risk of tailbone pain.\\n\\n10. Stay hydrated: Drinking plenty of water can help keep the muscles and tissues around the tailbone healthy and flexible, reducing the risk of injury and pain.\",\n",
            "    \"role\": \"assistant\"\n",
            "  }\n",
            "]\n",
            "\u001b[2m[2024-10-18 00:58:34,437 I] \u001b[2;36m[boxn-0:78741] filter_custom:335:\u001b[0m rubrics:\n",
            "['The response explicitly states whether sit-ups can cause pain on the tailbone.', 'All listed home remedies are non-medication based.', 'Tailbone pain question are addressed separately for better readability.', 'The response provides at least 3 distinct home remedy options for tailbone pain.', 'The response includes all 3 parts with no format error, as requested.', 'The response has no spelling and grammar errors.', 'The response was well-organized and clear to read.', 'The response addresses the query in an appropriate length.']\n",
            "\u001b[2m[2024-10-18 00:58:34,563 I] \u001b[2;36m[boxn-0:78741] filter_custom:340:\u001b[0m raw verifier output:\n",
            "{\n",
            "  \"The response explicitly states whether sit-ups can cause pain on the tailbone.\": \"yes\",\n",
            "  \"All listed home remedies are non-medication based.\": \"yes\",\n",
            "  \"Tailbone pain question are addressed separately for better readability.\": \"yes\",\n",
            "  \"The response provides at least 3 distinct home remedy options for tailbone pain.\": \"yes\",\n",
            "  \"The response includes all 3 parts with no format error, as requested.\": \"yes\",\n",
            "  \"The response has no spelling and grammar errors.\": \"yes\",\n",
            "  \"The response was well-organized and clear to read.\": \"yes\",\n",
            "  \"The response addresses the query in an appropriate length.\": \"yes\"\n",
            "}\n",
            "\u001b[2m[2024-10-18 00:58:34,563 I] \u001b[2;36m[boxn-0:78741] filter_custom:344:\u001b[0m reward: 1.0\n",
            "False\n",
            "[\n",
            "  {\n",
            "    \"content\": \"Can you provide me with a list of 6 similar games to Dave the Diver? \",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Here are 6 similar games to Dave the Diver:\\n\\n1. ABZ\\u00db - A beautiful underwater adventure game where explore the ocean and uncover the secrets of a mysterious world.\\n\\n2. Subnautica - An open-world survival game set on an alien ocean planet where players must collect resources, build bases, and explore the depths of the sea.\\n\\n3. Raft - A survival game where players are stranded on a small raft in the middle of the ocean and must gather resources, expand their raft, and survive against the elements.\\n\\n4. Stranded Deep - Another survival game set in the open ocean, where players must manage their resources, craft tools, and build shelter to survive after a plane crash.\\n\\n5. Depth - A multiplayer game that pits a team of divers against a team of sharks in an intense battle for survival in the depths of the ocean.\\n\\n6. Barotrauma - A 2D co-op survival game set on a submarine in the depths of an alien ocean, where players must manage the submarine's systems, fight off hostile creatures, and explore the ocean floor.\",\n",
            "    \"role\": \"assistant\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Can you give me more information on the game Stranded Deep? Provide 5 bullet points of facts about the game. Additionally, let me know if the game is coop, provide how many players, and list what consoles the game is on. \",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Certainly! Here are five bullet points about Stranded Deep along with details about co-op gameplay and available platforms:\\n\\n- **Survival Gameplay**: In Stranded Deep, players find themselves stranded in the Pacific Ocean after a plane crash. Gameplay focuses on survival: managing health, hunger, thirst, and exposure while exploring procedurally generated islands and the ocean floor.\\n\\n- **Crafting and Building**: Players can craft a variety of tools, weapons, and shelter from resources found on the islands or in the ocean. Building a raft for travel between islands and creating a sustainable living environment are key components of the gameplay.\\n\\n- **Dynamic Weather and Day-Night Cycle**: The game features a realistic day-night cycle and dynamic weather, both of which affect gameplay. For example, weather conditions can change drastically, affecting visibility, raft stability, and the player's body temperature.\\n\\n- **Underwater Exploration**: Players can dive underwater to explore coral reefs, shipwrecks, and ocean trenches. These locations contain resources and also pose dangers like sharks, which players must manage to avoid or confront.\\n\\n- **Fauna Interactions**: The game includes a variety of ocean wildlife, from benign fish that can be hunted for food to dangerous predators like sharks. Encounters require strategy, as some creatures can damage your raft or injure you.\\n\\nRegarding cooperative gameplay:\\n\\n- **Co-op Mode**: Stranded Deep has introduced a split-screen co-op mode for consoles, which allows two players to play together on the same screen. However, there isn't an online multiplayer feature, so co-op is limited to local play.\\n\\n- **Player Limit**: In co-op mode, the game supports 2 players.\\n\\nAs for console availability:\\n\\n- **Consoles**: Stranded Deep is available on:\\n  - PlayStation 4\\n  - Xbox One\\n  - Nintendo Switch (as of March 2022)\\n\\nIt's worth noting that Stranded Deep has also been released on PC (via Steam) where players have access to mods, which can enhance or alter the gameplay experience. Additionally, while not available at launch, the developers have mentioned the potential for future online multiplayer capabilities, but as of my last update, no such feature has been implemented. Always check the most recent updates or official sources for the latest features and platform availability.\",\n",
            "    \"role\": \"assistant\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Someone recommended the game Dredge to me if I liked Dave the Diver. Tell me about this game. Provide a 1-paragraph plot summary and then give me 6 bullet points of game features. \",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Dredge is an indie fishing adventure game with a sinister twist, developed by Black Salt Games. In Dredge, players step into the role of a down-on-their-luck fisherman, exploring a series of remote islands and their surrounding depths. The game combines elements of exploration, fishing, and a narrative filled with mystery and danger. The plot revolves around uncovering the secrets of the mysterious archipelago where the game is set, as players meet various characters, fulfill quests, and delve into the dark waters that hold more than just fish.\\n\\nHere are six key features of the game:\\n\\n- **Fishing Mechanics**: At its core, Dredge offers a unique fishing experience where players can fish in various locations, each with its own set of fish species to catch. The fishing mechanics are simple yet satisfying, involving a timing-based minigame to successfully reel in your catch.\\n\\n- **Exploration**: Players can explore a beautifully rendered, atmospheric world filled with secrets. The game encourages exploration both above and below the water, where players can discover new fishing spots, hidden treasures, and pieces of the story.\\n\\n- **Upgrade System**: By selling their catch, players can earn money to upgrade their boat and equipment. Upgrades can enhance fishing capabilities, increase storage, or allow access to new areas, adding a layer of strategy to what players prioritize in their upgrades.\\n\\n- **Day-Night Cycle and Weather Effects**: The game features a dynamic day-night cycle and weather system that affects gameplay. Certain fish are more active at specific times or under particular weather conditions, adding a layer of realism and challenge.\\n\\n- **Narrative Elements**: Dredge weaves an intriguing narrative throughout the gameplay, with players encountering various characters and uncovering the lore of the islands. This narrative progression is often tied to gameplay milestones and exploration.\\n\\n- **Atmospheric Setting**: The game sets a mysterious and sometimes eerie atmosphere, with its visually distinctive style and ambient sound design. This atmosphere is heightened by encounters with otherworldly creatures and the unfolding mystery of the islands and their past.\\n\\nDredge offers a blend of relaxing gameplay through its fishing mechanics and a compelling narrative that drives the player to explore and uncover the mysteries of its world. It's this unique combination that has drawn comparisons to games like Dave the Diver, appealing to players who enjoy exploration, narrative-driven gameplay, and a touch of the unknown in their gaming experience.\",\n",
            "    \"role\": \"assistant\"\n",
            "  }\n",
            "]\n",
            "\u001b[2m[2024-10-18 00:58:41,193 I] \u001b[2;36m[boxn-0:78741] filter_custom:335:\u001b[0m rubrics:\n",
            "['Plot summary is provided.', 'Plot summary is only a single paragraph.', 'Plot summary mentions key plot points such as the discovery of cursed fish and the changing nature of the game as it unfolds.', 'There are Six bullet points provided.', 'Bullet points are well-detailed and descriptive.', 'Bullet points do not repeat each other and are separate items.', 'Each bullet point focuses on one specific aspect of the game.']\n",
            "\u001b[2m[2024-10-18 00:58:41,439 I] \u001b[2;36m[boxn-0:78741] filter_custom:340:\u001b[0m raw verifier output:\n",
            "{\n",
            "  \"Plot summary is provided.\": \"yes\",\n",
            "  \"Plot summary is only a single paragraph.\": \"yes\",\n",
            "  \"Plot summary mentions key plot points such as the discovery of cursed fish and the changing nature of the game as it unfolds.\": \"no\",\n",
            "  \"There are Six bullet points provided.\": \"yes\",\n",
            "  \"Bullet points are well-detailed and descriptive.\": \"yes\",\n",
            "  \"Bullet points do not repeat each other and are separate items.\": \"yes\",\n",
            "  \"Each bullet point focuses on one specific aspect of the game.\": \"yes\"\n",
            "}\n",
            "\u001b[2m[2024-10-18 00:58:41,440 I] \u001b[2;36m[boxn-0:78741] filter_custom:344:\u001b[0m reward: 0.8571428571428571\n",
            "False\n",
            "[\n",
            "  {\n",
            "    \"content\": \"Tell me about the downfall of Lizzo as if you are writing an article for Us Weekly. Make sure to include claims that are backed up by evidence. Include cancel culture in there and how it applies to this situation. Please make sure the article is no more than 500 words. \",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Title: The Fall from Grace: Lizzo's Cancelation and Its Cultural Context\\n\\nIn an era where every tweet, video, and soundbite can be scrutinized to the nth degree, the meteoric rise of an artist can be matched only by their plummet due to cancel culture. Lizzo, the once-celebrated body-positive singer, rapper, and flutist known for her empowering anthems like \\\"Truth Hurts\\\" and \\\"Good as Hell,\\\" has found herself at the center of controversy, leading to a swift backlash from fans and observers alike.\\n\\nIt all spiraled downward when several former employees, including backup dancers and tour staff, came forward with allegations against Lizzo. They accused her of fostering a hostile work environment, including claims of sexual harassment, body-shaming, and false imprisonment. These allegations starkly contrasted the inclusive, uplifting persona Lizzo had cultivated. Evidence emerged in the form of legal documents, as well as corroborative statements from individuals associated with Lizzo's Big Grrrl Big Touring company.\\n\\nSocial media, once the launchpad for Lizzo's success, became the very platform that accelerated her downfall. #LizzoIsOverParty trended as users unearthed old tweets and videos of Lizzo that they deemed problematic, contributing to the narrative of hypocrisy. The cancel culture, a modern phenomenon where public figures face a form of ostracism for perceived wrongdoings, had now turned its gaze towards her. \\n\\nCritics argue that cancel culture leaves little room for nuance or redemption. However, in Lizzo's case, the mounting evidence and her initial response \\u2014 a Notes app apology deemed insincere by many \\u2014 did little to quell the uproar. Rather than address specific allegations, Lizzo's statement generalized the issue, which only fueled further discontent among her fanbase and the general public.\\n\\nIn an interview with Us Weekly, a former dancer for Lizzo stated, \\\"We were excited to work with someone who preaches body positivity and inclusion. But behind the scenes, it was different. The contradiction was stark and hurtful.\\\" This sentiment was echoed across social platforms, where fans expressed betrayal by the singer's alleged off-stage behavior.\\n\\nBrands started to distance themselves from Lizzo, with some collaborations being quietly shelved. Her music streams and sales saw a noticeable dip, signifying that her once-loyal fanbase was turning away. The situation was exacerbated when Lizzo's attempt to regain control of her narrative through social media fell flat, with many interpreting her posts as tone-deaf amidst the serious accusations.\\n\\nThe cultural impact of Lizzo's downfall serves as a cautionary tale within the music industry. It highlights the precariousness of celebrity, particularly for those who build their brand on authenticity and moral virtue. In Lizzo's case, cancel culture acted swiftly, holding her accountable in a very public trial by social media.\\n\\nAs the dust continues to settle, discussions about the fairness and efficacy of cancel culture persist. Some argue it's a tool for justice, while others view it as mob mentality lacking in due process. For Lizzo, the road to redemption, if there is one, looks to be long and fraught with the challenge of regaining trust once broken. What remains to be seen is whether Lizzo can navigate her way back into the public's good graces or if this marks the irrevocable decline of a once shining star.\",\n",
            "    \"role\": \"assistant\"\n",
            "  }\n",
            "]\n",
            "\u001b[2m[2024-10-18 00:58:49,079 I] \u001b[2;36m[boxn-0:78741] filter_custom:335:\u001b[0m rubrics:\n",
            "[\"The response is written in a style that matches Us Weekly magazine's typical tone.\", 'The article covers the downfall of Lizzo.', \"Claims about Lizzo's downfall are substantiated with evidence within the article.\", \"The role and impact of cancel culture related to Lizzo's situation are discussed.\", 'The word count of the article is no greater than 500 words.', 'The response is clear and written in a coherent and engaging manner.']\n",
            "\u001b[2m[2024-10-18 00:58:49,205 I] \u001b[2;36m[boxn-0:78741] filter_custom:340:\u001b[0m raw verifier output:\n",
            "{\n",
            "  \"The response is written in a style that matches Us Weekly magazine's typical tone.\": \"yes\",\n",
            "  \"The article covers the downfall of Lizzo.\": \"yes\",\n",
            "  \"Claims about Lizzo's downfall are substantiated with evidence within the article.\": \"yes\",\n",
            "  \"The role and impact of cancel culture related to Lizzo's situation are discussed.\": \"yes\",\n",
            "  \"The word count of the article is no greater than 500 words.\": \"no\",\n",
            "  \"The response is clear and written in a coherent and engaging manner.\": \"yes\"\n",
            "}\n",
            "\u001b[2m[2024-10-18 00:58:49,205 I] \u001b[2;36m[boxn-0:78741] filter_custom:344:\u001b[0m reward: 0.8333333333333334\n",
            "False\n",
            "[\n",
            "  {\n",
            "    \"content\": \"I need some help! I'm a highschool English teacher getting ready for a new class of freshman students. I'm thinking of having them do a writing project using the same formatting and style of Edgar Allan Poe. What would be some good poems to reference for writing style and would be easy to understand by 14 year old students?\",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Great idea! Edgar Allan Poe's works can be engaging and inspiring for students. Here are some poems that are suitable for 14-year-olds and can serve as good references for writing style:\\n\\n1. \\\"The Raven\\\" - This poem is one of Poe's most famous works and has a captivating, rhythmic quality. The narrative style and the theme of lost love can resonate with young readers.\\n\\n2. \\\"Annabel Lee\\\" - This poem also deals with the theme of lost love and has a hauntingly beautiful quality. The language is relatively simple, and the story is easy to follow.\\n\\n3. \\\"The Bells\\\" - This poem showcases Poe's mastery of sound devices like onomatopoeia, alliteration, and assonance. Students can analyze the poem's structure and the way Poe uses language to evoke different moods.\\n\\n4. \\\"A Dream Within a Dream\\\" - This short poem explores the nature of reality and dreams, a theme that can be intriguing for young readers. The language is accessible, and the poem's brevity makes it easy to analyze.\\n\\n5. \\\"The Tell-Tale Heart\\\" (short story) - While not a poem, this famous short story is an excellent example of Poe's unique writing style. It can help students understand his use of first-person narration, suspense, and psychological horror.\\n\\nWhen introducing Poe's works to your students, consider providing some background information on his life and the historical context in which he wrote. This can help students better understand and appreciate his unique style.\",\n",
            "    \"role\": \"assistant\"\n",
            "  }\n",
            "]\n",
            "\u001b[2m[2024-10-18 00:58:55,114 I] \u001b[2;36m[boxn-0:78741] filter_custom:335:\u001b[0m rubrics:\n",
            "[\"Edgar Allan Poe's poems are exclusively suggested as examples.\", 'Each poem should be succinctly described or quoted.', 'Each poem should be easy for teenagers to understand.', \"The poems suggested should reflect Poe's writing style.\", 'Each poem is uniquely suggested with no duplication.', 'The poems are suggested in a clear and organized format.', \"Suitable poem quantity for the writing project to show breadth of style and themes are suggested and would help in showcasing the range of Poe's writing.\"]\n",
            "\u001b[2m[2024-10-18 00:58:55,259 I] \u001b[2;36m[boxn-0:78741] filter_custom:340:\u001b[0m raw verifier output:\n",
            "{\n",
            "  \"Edgar Allan Poe's poems are exclusively suggested as examples.\": \"yes\",\n",
            "  \"Each poem should be succinctly described or quoted.\": \"yes\",\n",
            "  \"Each poem should be easy for teenagers to understand.\": \"yes\",\n",
            "  \"The poems suggested should reflect Poe's writing style.\": \"yes\",\n",
            "  \"Each poem is uniquely suggested with no duplication.\": \"yes\",\n",
            "  \"The poems are suggested in a clear and organized format.\": \"yes\",\n",
            "  \"Suitable poem quantity for the writing project to show breadth of style and themes are suggested and would help in showcasing the range of Poe's writing.\": \"yes\"\n",
            "}\n",
            "\u001b[2m[2024-10-18 00:58:55,260 I] \u001b[2;36m[boxn-0:78741] filter_custom:344:\u001b[0m reward: 1.0\n",
            "False\n",
            "[\n",
            "  {\n",
            "    \"content\": \"I want to learn more about other countries and where they are geographically. Categorise the list below of countries by continent and population. \\n\\nParaguay\\nBarbados\\nAustralia\\nTurkey\\nGreece\\nEstonia\\nGuatamala\\nCosta Rica\\nCanada\\nBrazil\\nThailand\\nIndonesia\\nFiji\",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Here's a list of the countries categorized by continent along with their respective populations:\\n\\n**North America:**\\n1. Canada - Approximately 38 million people\\n2. Guatemala - Approximately 17 million people\\n3. Costa Rica - Approximately 5 million people\\n\\n**South America:**\\n1. Brazil - Approximately 213 million people\\n2. Paraguay - Approximately 7 million people\\n\\n**Europe:**\\n1. Turkey - Approximately 84 million people\\n2. Greece - Approximately 10 million people\\n3. Estonia - Approximately 1.3 million people\\n\\n**Asia:**\\n1. Thailand - Approximately 69 million people\\n2. Indonesia - Approximately 273 million people\\n\\n**Oceania:**\\n1. Australia - Approximately 25 million people\\n2. Fiji - Approximately 900,000 people\\n\\n**Caribbean:**\\n1. Barbados - Approximately 287,000 people\\n\\nPlease note, the populations are rough estimates and can fluctuate, but they give you a general idea of the demographic scale in each country. Also, categorization of Turkey can vary since it spans both Europe and Asia, but it is commonly associated with Europe when categorizing by culture and politics. Similarly, Indonesia could also be considered part of Oceania in some contexts, but it's more often grouped with Asia due to cultural and historical affiliations.\",\n",
            "    \"role\": \"assistant\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Thank you, I am planning on visiting Australia but it is too large a country and I am struggling on which cities to visit. List all large cities and only one exciting thing to see or do there \",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Australia is a vast country with a number of exciting cities to visit, each with its own unique attractions. Here are some of the major cities in Australia and one highlight for each:\\n\\n1. **Sydney**: \\n   - **Sydney Opera House**: This iconic, sail-shaped building is world-renowned for its unique architecture and picturesque location on the Sydney Harbour. It hosts a variety of performances, including opera, ballet, and theater productions.\\n\\n2. **Melbourne**:\\n   - **Royal Melbourne Golf Club**: Golf enthusiasts will enjoy visiting this prestigious club, which has hosted numerous national and international tournaments and is considered one of the best golf courses in the world.\\n\\n3. **Brisbane**:\\n   - **Lone Pine Koala Sanctuary**: This is the world's first and largest koala sanctuary. Visitors can cuddle a koala, hand-feed kangaroos, and encounter a variety of other Australian wildlife.\\n\\n4. **Perth**:\\n   - **Kings Park and Botanic Garden**: One of the largest inner-city parks in the world, Kings Park offers stunning views of the Swan River and Perth city skyline, along with beautiful gardens and natural bushland.\\n\\n5. **Adelaide**:\\n   - **Kangaroo Island**: Just a short ferry ride from the mainland, Kangaroo Island is known for its diverse wildlife and natural beauty. It's a great place to see kangaroos, koalas, sea lions, and penguins in their natural habitats.\\n\\n6. **Canberra**:\\n   - **Australian War Memorial**: This is both a shrine to Australians who have died in war and a world-class museum, offering a moving and educational experience.\\n\\n7. **Gold Coast**:\\n   - **Surfers Paradise Beach**: This vibrant coastal city is famous for its beautiful beaches, high-rise buildings, and bustling nightlife. Surfers Paradise Beach is a great spot for surfing, swimming, and sunbathing.\\n\\n8. **Cairns**:\\n   - **Great Barrier Reef**: While not directly in the city, Cairns is a popular gateway to the Great Barrier Reef, the world's largest coral reef system. You can snorkel or dive to see the incredible marine life and colorful corals.\\n\\n9. **Darwin**:\\n   - **Kakadu National Park**: This vast, biodiverse nature reserve is home to wetlands, rivers, and sandstone escarpments, and it's a great place to learn about Australia's indigenous cultures.\\n\\n10. **Hobart**:\\n    - **Museum of Old and New Art (MONA)**: This innovative museum showcases a mix of ancient artifacts and contemporary art, and its quirky, thought-provoking exhibits make it a must-see.\\n\\nRemember, each city has a multitude of attractions, so this is just a taste of what you can experience. Enjoy your trip!\",\n",
            "    \"role\": \"assistant\"\n",
            "  }\n",
            "]\n",
            "\u001b[2m[2024-10-18 00:59:00,443 I] \u001b[2;36m[boxn-0:78741] filter_custom:335:\u001b[0m rubrics:\n",
            "['All listed cities are in Australia.', 'Each listed city has only one exciting thing to see or do associated with it.', 'Exciting thing to see or do are unique for each city listed.', 'At least ten large Australian cities are listed.', 'Exciting things listed are verifiable and reasonably touristic or cultural attractions.']\n",
            "\u001b[2m[2024-10-18 00:59:00,637 I] \u001b[2;36m[boxn-0:78741] filter_custom:340:\u001b[0m raw verifier output:\n",
            "{\n",
            "  \"All listed cities are in Australia.\": \"yes\",\n",
            "  \"Each listed city has only one exciting thing to see or do associated with it.\": \"yes\",\n",
            "  \"Exciting thing to see or do are unique for each city listed.\": \"yes\",\n",
            "  \"At least ten large Australian cities are listed.\": \"yes\",\n",
            "  \"Exciting things listed are verifiable and reasonably touristic or cultural attractions.\": \"yes\"\n",
            "}\n",
            "\u001b[2m[2024-10-18 00:59:00,638 I] \u001b[2;36m[boxn-0:78741] filter_custom:344:\u001b[0m reward: 1.0\n",
            "False\n",
            "[\n",
            "  {\n",
            "    \"content\": \"I'm trying to figure out where a piece of text came from. Identify the following passage as either a 1) summary, advertisement, text message or legalese. Please respond only with your identification, and an optional one sentence justification:\\n\\nIn war-ravaged Panem, Coriolanus Snow is one of twenty-four Academy students selected to mentor a 10th Annual Hunger Games tribute. Coriolanus hopes for the Plinth Prize scholarship to restore his family's prosperity, which was damaged after his father's death.\\n\\nCoriolanus is assigned District 12's female tribute, Lucy Gray Baird. During the reaping ceremony, she engages Capitol viewers by slipping a snake down the dress of Mayor Lipp's daughter, Mayfair, then singing for the crowd. Coriolanus earns Lucy Gray's trust while accompanying her to the Capitol Zoo, where tributes are publicly caged.\",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Summary. \\n\\nThe passage outlines the main plot points and character introductions from Suzanne Collins' novel \\\"The Hunger Games: The Ballad of Songbirds and Snakes,\\\" without verbatim legal language or commercial intent typical of advertisement or legalese.\",\n",
            "    \"role\": \"assistant\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Please extract all mentions of names out of the provided summary. You can include titles (like Jr, Sr, President etc) in the names - please number each list item, and only include an extracted name following each number.\",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"1. Coriolanus Snow\\n2. Lucy Gray Baird\\n3. Mayor Lipp\\n4. Mayfair Lipp\",\n",
            "    \"role\": \"assistant\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Based on only the summary from the first turn and no external information, write a brief plot (less than four sentences) for an idea on an offshoot story focused specifically on each character. There should be four summaries total.\",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"1. Coriolanus Snow, grappling with the legacy of his father and the decline of his family's fortune, sees an opportunity to rise through the societal ranks of Panem by mentoring a tribute in the Hunger Games. His journey becomes one of redemption and ambition as he navigates the complexities of Capitol politics and his own moral compass, striving to secure the Plinth Prize and restore his family's status.\\n\\n2. Lucy Gray Baird, a charismatic and cunning tribute from District 12, uses her wit and charm to captivate the Capitol audience, challenging the status quo. Her story explores the power of performance and perception as she manipulates her image to survive in a system designed for her to fail, all the while forming a complex bond with her mentor, Coriolanus.\\n\\n3. Mayor Lipp, a figure of authority in the Capitol, faces the challenge of maintaining order and control as the Hunger Games unfold. His story delves into the pressures of leadership and the personal costs of enforcing a brutal regime, as his own daughter, Mayfair, becomes unexpectedly entwined in the Games' spectacle.\\n\\n4. Mayfair Lipp, initially a mere spectator in the Capitol's cruel entertainment, finds herself directly affected by the Games when Lucy Gray's act of defiance at the reaping targets her. Her offshoot story explores themes of privilege, victimhood, and ultimately, the potential for redemption and change as she confronts the harsh realities of the world she's been sheltered from.\",\n",
            "    \"role\": \"assistant\"\n",
            "  }\n",
            "]\n",
            "\u001b[2m[2024-10-18 00:59:05,033 I] \u001b[2;36m[boxn-0:78741] filter_custom:335:\u001b[0m rubrics:\n",
            "['There are four brief plots in total.', 'Each plot is uniquely centered around one of the four characters: Coriolanus Snow, Lucy Gray Baird, Mayor Lipp, and Mayfair Lipp.', \"Each plot's events either precedes or follows the events in the summary or presents an alternate storyline to them.\", 'Each plot is less than four sentences long.', 'Each response is cohesive and clearly follows structural narrative rules.']\n",
            "\u001b[2m[2024-10-18 00:59:05,156 I] \u001b[2;36m[boxn-0:78741] filter_custom:340:\u001b[0m raw verifier output:\n",
            "{\n",
            "  \"There are four brief plots in total.\": \"yes\",\n",
            "  \"Each plot is uniquely centered around one of the four characters: Coriolanus Snow, Lucy Gray Baird, Mayor Lipp, and Mayfair Lipp.\": \"yes\",\n",
            "  \"Each plot's events either precedes or follows the events in the summary or presents an alternate storyline to them.\": \"yes\",\n",
            "  \"Each plot is less than four sentences long.\": \"yes\",\n",
            "  \"Each response is cohesive and clearly follows structural narrative rules.\": \"yes\"\n",
            "}\n",
            "\u001b[2m[2024-10-18 00:59:05,156 I] \u001b[2;36m[boxn-0:78741] filter_custom:344:\u001b[0m reward: 1.0\n",
            "False\n",
            "[\n",
            "  {\n",
            "    \"content\": \"Based on the following article, what has Washington decided to do following the incident? Also, if this starts a regional war, what will this obstruct?\\n\\n\\\"US citizens have been injured following a rocket attack on an American air base in Iraq, sparking serious fears of World War 3. The Al-Asad Airbase was targeted by missiles just days after the US deployed 12 warships and fighter jets to the Middle East following the assassination of Hamas leader Ismail Haniyeh in Tehran. Two Soviet-made Katyusha rockets reportedly \\\"fell inside the base\\\", with one landing in a nearby village. President Biden and Vice President Kamala Harris were briefed on the situation, discussing \\\"developments in the Middle East.\\\"\\n\\nIn a tweet, Biden stated: \\\"We received updates on threats posed by Iran and its proxies, diplomatic efforts to de-escalate regional tensions, and preparations to support Israel should it be attacked again,\\\" adding that they are prepared to \\\"We also discussed the steps we are taking to defend our forces and respond to any attack against our personnel in a manner and place of our choosing.\\\" An official confirmed to CNN that there had indeed been a \\\"We can confirm that there was a suspected rocket attack today against US and Coalition forces at Al-Asad Airbase, Iraq,\\\".\\n\\n\\\"Initial indications are that several US personnel were injured. Base personnel are conducting a post-attack damage assessment.\\\" The missile attack comes amid escalating tensions in the Middle East following the assassination of high-ranking Hamas and Hezbollah members.\\n\\nThere are fears that Iran is preparing to launch an imminent attack on Israel, potentially triggering a full-scale war in the region. Washington has decided to increase its presence in response to Iran's threat to attack Israel after the killing of Hamas leader Ismail Haniyeh. Threats from Iran and its allies in Gaza, Lebanon, and Yemen have prompted the US reaction.\\n\\nSome experts on Israeli foreign policy are concerned that the regional war will obstruct a ceasefire agreement between Israel and Gaza, as well as bolster Israeli Prime Minister Benjamin Netanyahu's power and public image among Israelis. A statement issued by President Joe Biden last week emphasized the US's commitment to supporting its ally Israel against attacks from Iran and its proxies, without mentioning a ceasefire deal in Gaza. \\\"All of a sudden the need for a ceasefire in Gaza has disappeared. This is reckless,\\\" said Mairav Zonsein, a senior Israel analyst with The Crisis Group.\\\"\",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Based on the article, Washington has decided to increase its military presence in the Middle East in response to Iran's threat to attack Israel following the assassination of Hamas leader Ismail Haniyeh. This decision comes after a rocket attack on the Al-Asad Airbase in Iraq which injured U.S. personnel, and amid fears of escalating tensions potentially leading to a broader conflict.\\n\\nIf this situation escalates into a regional war, it could obstruct several key objectives and ongoing processes:\\n\\n1. **Ceasefire Agreement**: A regional war could significantly hinder efforts to achieve a ceasefire between Israel and Gaza. The focus and resources might shift from negotiation and peace talks to military engagement and defense strategies.\\n\\n2. **Diplomatic Efforts**: Ongoing diplomatic efforts to de-escalate tensions in the region would likely be sidelined as countries potentially involved in the conflict prioritize military and strategic responses over dialogue and diplomacy.\\n\\n3. **Regional Stability**: A full-scale war would disrupt stability across the Middle East, affecting not just the countries directly involved but also having broader geopolitical implications, including impacts on global oil prices, security of trade routes, and the potential for the conflict to spread beyond the immediate region.\\n\\n4. **International Relations**: Such a conflict might also strain international relations, as different nations could be drawn in or pressured to take sides, affecting global alliances and cooperative efforts in other areas, like counter-terrorism or economic collaboration.\\n\\n5. **Humanitarian Impact**: A war would have devastating humanitarian consequences, with loss of life, displacement of people, and destruction of infrastructure, overshadowing and likely worsening the already dire humanitarian situation in places like Gaza.\\n\\n6. **Israel's Domestic Politics**: As mentioned, the escalation could bolster Israeli Prime Minister Benjamin Netanyahu's position, impacting internal politics and policies, possibly making the government less inclined towards concessions or peace negotiations.\\n\\nIn summary, an escalation into a regional war would pose severe challenges to peace efforts, stability, humanitarian conditions, and international relations in the Middle East, shifting priorities away from negotiation and towards military engagement.\",\n",
            "    \"role\": \"assistant\"\n",
            "  }\n",
            "]\n",
            "\u001b[2m[2024-10-18 00:59:10,779 I] \u001b[2;36m[boxn-0:78741] filter_custom:335:\u001b[0m rubrics:\n",
            "[\"Washington has decided to increase its military presence in response to Iran's threat to attack Israel after the killing of Hamas leader Ismail Haniyeh.\", 'If the incident spirals into a regional war, it will obstruct a ceasefire agreement between Israel and Gaza.', \"If the incident escalates, it will bolster Israeli Prime Minister Benjamin Netanyahu's power.\"]\n",
            "\u001b[2m[2024-10-18 00:59:10,897 I] \u001b[2;36m[boxn-0:78741] filter_custom:340:\u001b[0m raw verifier output:\n",
            "{\n",
            "  \"Washington has decided to increase its military presence in response to Iran's threat to attack Israel after the killing of Hamas leader Ismail Haniyeh.\": \"yes\",\n",
            "  \"If the incident spirals into a regional war, it will obstruct a ceasefire agreement between Israel and Gaza.\": \"yes\",\n",
            "  \"If the incident escalates, it will bolster Israeli Prime Minister Benjamin Netanyahu's power.\": \"yes\"\n",
            "}\n",
            "\u001b[2m[2024-10-18 00:59:10,898 I] \u001b[2;36m[boxn-0:78741] filter_custom:344:\u001b[0m reward: 1.0\n",
            "False\n",
            "[\n",
            "  {\n",
            "    \"content\": \"Summarize the concepts provided in the  Gettysburg address. Explain it in a way that a fourth grader would be able to understand.\",\n",
            "    \"role\": \"user\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"The Gettysburg Address is a famous speech given by President Abraham Lincoln during the American Civil War. Here's a summary of the main concepts in a way a fourth grader can understand:\\n\\n1. The United States was founded on the idea that all people are equal and have the right to life, liberty, and the pursuit of happiness.\\n\\n2. The Civil War is a test to see if a country based on these ideas can survive.\\n\\n3. The soldiers who died at the Battle of Gettysburg fought bravely to defend these ideas and ensure that the country remains united.\\n\\n4. The living must honor the dead by continuing to work towards a better future for the nation, where freedom and equality are upheld.\\n\\n5. The ultimate goal is to have a government \\\"of the people, by the people, for the people,\\\" meaning that the people have the power to choose their leaders and shape their country.\\n\\nIn simpler terms, President Lincoln is saying that the United States is a special country because it believes everyone should be treated fairly and have the chance to live a happy life. The soldiers at Gettysburg fought to keep the country together and make sure these ideas continue. Now, it's up to the living to keep working towards a better future where everyone is free and equal.\",\n",
            "    \"role\": \"assistant\"\n",
            "  }\n",
            "]\n",
            "\u001b[2m[2024-10-18 00:59:17,941 I] \u001b[2;36m[boxn-0:78741] filter_custom:335:\u001b[0m rubrics:\n",
            "['The summary covers the concept of making America a nation conceived in liberty and dedicated to the proposition that all men are created equal.', 'The summary mentions the importance of `re-dedication` to the cause of freedom and democracy.', \"The summary describes the civil war as a test for the nation's survival.\", 'The explanation avoids using complex terms or concepts not understood by a typical 4th grader.', 'The language used is simple and appropriate for a fourth grader.', \"The explanation uses analogies or metaphors that might resonate with a fourth grader's understanding of the world.\"]\n",
            "\u001b[2m[2024-10-18 00:59:18,081 I] \u001b[2;36m[boxn-0:78741] filter_custom:340:\u001b[0m raw verifier output:\n",
            "{\n",
            "  \"The summary covers the concept of making America a nation conceived in liberty and dedicated to the proposition that all men are created equal.\": \"yes\",\n",
            "  \"The summary mentions the importance of `re-dedication` to the cause of freedom and democracy.\": \"no\",\n",
            "  \"The summary describes the civil war as a test for the nation's survival.\": \"yes\",\n",
            "  \"The explanation avoids using complex terms or concepts not understood by a typical 4th grader.\": \"yes\",\n",
            "  \"The language used is simple and appropriate for a fourth grader.\": \"yes\",\n",
            "  \"The explanation uses analogies or metaphors that might resonate with a fourth grader's understanding of the world.\": \"no\"\n",
            "}\n",
            "\u001b[2m[2024-10-18 00:59:18,082 I] \u001b[2;36m[boxn-0:78741] filter_custom:344:\u001b[0m reward: 0.6666666666666666\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_path = \"/data/datasets/preferences-v2/mixture/grand_reward/train/part-00026-of-00067.parquet\"\n",
        "# pref_data = utils.read_df(glob.glob(data_path))\n",
        "\n",
        "# pref_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "I1zLqQqX-V6s",
        "outputId": "67a7ba11-3d20-4817-9546-8000e8ee55cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'utils' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/datasets/preferences-v2/mixture/grand_reward/train/part-00026-of-00067.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m pref_data \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mread_df(glob\u001b[38;5;241m.\u001b[39mglob(data_path))\n\u001b[1;32m      5\u001b[0m pref_data \n",
            "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics/custom_convos/part-00000-of-00001.parquet\"\n",
        "sft_data = utils.read_df(glob.glob(data_path))\n",
        "\n",
        "reward_good = 0.5\n",
        "bad_rows = []\n",
        "good_rows = []\n",
        "for idx, (_, row) in enumerate(sft_data.iterrows()):\n",
        "  reward = row['metadata']['rubric_verifier']['reward']\n",
        "  if reward and reward < reward_good:\n",
        "    bad_rows.append(row)\n",
        "  else:\n",
        "    good_rows.append(row)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "id": "edjBp25MgwQw",
        "outputId": "a62b7e56-4e37-4950-88b6-ba14912858e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No objects to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics/custom_convos/part-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m sft_data \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m reward_good \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      5\u001b[0m bad_rows \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m~/xai/train/xlm/posttrain/utils.py:416\u001b[0m, in \u001b[0;36mread_df\u001b[0;34m(paths, verbose)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rubric Filter Analysis 10/22"
      ],
      "metadata": {
        "id": "T0UV2oS7ys9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "INPUT_ROOT = \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics\"\n",
        "INPUT_ROOT_SIDE =  \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics_filtered_0_5\"\n",
        "OUTPUT_STATS_FILE = os.path.join(INPUT_ROOT, \"stats.parquet\")\n",
        "\n",
        "def process_parquet_file(file_path):\n",
        "    \"\"\"Process a single parquet file to extract category, reward, rubrics, and conversation length data.\"\"\"\n",
        "    table = pq.read_table(file_path)\n",
        "    df = table.to_pandas()\n",
        "\n",
        "    # Ensure relevant columns exist\n",
        "    if 'metadata' not in df:\n",
        "        return None\n",
        "    if 'domain' not in df:\n",
        "        df['domain'] = 'UNKNOWN'\n",
        "\n",
        "    # Extract relevant data\n",
        "    df['reward'] = df['metadata'].apply(lambda x: x.get('rubric_verifier', {}).get('reward', None))\n",
        "    df['rubrics'] = df['metadata'].apply(lambda x: len(x.get('rubric_verifier', {}).get('rubrics', [])))\n",
        "    df['response_length'] = df['conversation'].apply(lambda x: len(x[-1]['content']) if len(x) > 0 else 0)\n",
        "\n",
        "    # Filter out rows with no reward\n",
        "    df = df.dropna(subset=['reward'])\n",
        "\n",
        "    return df[['domain', 'reward', 'rubrics', 'response_length']]\n",
        "\n",
        "def bucket_data(df):\n",
        "    \"\"\"Assign buckets based on 10 percentile buckets for reward, rubrics, and conversation length.\"\"\"\n",
        "    # Bucketing for conversation length\n",
        "    df['length_bucket'] = pd.qcut(df['response_length'], q=10, labels=False, duplicates='drop')\n",
        "\n",
        "    # Bucketing for rubrics\n",
        "    df['rubrics_bucket'] = pd.qcut(df['rubrics'], q=10, labels=False, duplicates='drop')\n",
        "\n",
        "    # Bucketing for rewards\n",
        "    df['reward_bucket'] = pd.qcut(df['reward'], q=10, labels=False, duplicates='drop')\n",
        "\n",
        "    return df\n",
        "\n",
        "def get_bucket_boundaries(df):\n",
        "    \"\"\"Calculate the boundaries for each of the 10 percentile buckets.\"\"\"\n",
        "    boundaries = {\n",
        "        'reward_boundaries': np.percentile(df['reward'].dropna(), [i * 10 for i in range(11)]),\n",
        "        'rubrics_boundaries': np.percentile(df['rubrics'].dropna(), [i * 10 for i in range(11)]),\n",
        "        'length_boundaries': np.percentile(df['response_length'].dropna(), [i * 10 for i in range(11)]),\n",
        "    }\n",
        "\n",
        "    # Drop duplicates from the boundaries\n",
        "    boundaries['reward_boundaries'] = np.unique(boundaries['reward_boundaries'])\n",
        "    boundaries['rubrics_boundaries'] = np.unique(boundaries['rubrics_boundaries'])\n",
        "    boundaries['length_boundaries'] = np.unique(boundaries['length_boundaries'])\n",
        "\n",
        "    return boundaries\n",
        "\n",
        "def process_directory(input_root):\n",
        "    \"\"\"Process all parquet files in the input directory and collect data.\"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    for root, dirs, files in os.walk(input_root):\n",
        "        for file in tqdm(files, desc=\"Processing files\"):\n",
        "            if file.endswith(\".parquet\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "                df = process_parquet_file(file_path)\n",
        "                if df is not None and not df.empty:\n",
        "                    all_data.append(df)\n",
        "\n",
        "    if all_data:\n",
        "        combined_df = pd.concat(all_data)\n",
        "        return combined_df\n",
        "    return None\n",
        "\n",
        "def calculate_average_and_count(df, bucket_column, value_column):\n",
        "    \"\"\"Calculate average and count for specified bucket column.\"\"\"\n",
        "    avg = df.groupby(bucket_column)[value_column].mean()\n",
        "    count = df.groupby(bucket_column)[value_column].count()\n",
        "\n",
        "    return avg, count\n",
        "\n",
        "def plot_averages_and_counts(raw_df, boundaries):\n",
        "    \"\"\"Plot average rubrics and average rewards for each bucket in a 2x2 grid.\"\"\"\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    fig.suptitle('Average Rubrics and Average Reward per Bucket')\n",
        "\n",
        "    # Average Rubrics per Reward Bucket\n",
        "    avg_rubrics, count_rubrics = calculate_average_and_count(raw_df, 'reward_bucket', 'rubrics')\n",
        "    bucket_labels = [f\"[{boundaries['reward_boundaries'][i]:.2f}, {boundaries['reward_boundaries'][i + 1]:.2f})\" for i in range(len(boundaries['reward_boundaries']) - 1)]\n",
        "\n",
        "    bars = axs[0, 0].bar(bucket_labels, avg_rubrics, color='skyblue', edgecolor='black')\n",
        "    axs[0, 0].set_xlabel('Reward Bucket Boundaries')\n",
        "    axs[0, 0].set_ylabel('Average Rubrics')\n",
        "    axs[0, 0].set_title('Average Rubrics per Reward Bucket')\n",
        "    axs[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Annotate counts on bars\n",
        "    # for bar, count in zip(bars, count_rubrics):\n",
        "    #     axs[0, 0].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count),\n",
        "    #                    ha='center', va='bottom')\n",
        "\n",
        "    # Average Rubrics per Conversation Length Bucket\n",
        "    avg_rubrics_length, count_rubrics_length = calculate_average_and_count(raw_df, 'length_bucket', 'rubrics')\n",
        "    bucket_labels_length = [f\"[{boundaries['length_boundaries'][i]:.2f}, {boundaries['length_boundaries'][i + 1]:.2f})\" for i in range(len(boundaries['length_boundaries']) - 1)]\n",
        "\n",
        "    bars_length = axs[0, 1].bar(bucket_labels_length, avg_rubrics_length, color='skyblue', edgecolor='black')\n",
        "    axs[0, 1].set_xlabel('Response Length Bucket Boundaries')\n",
        "    axs[0, 1].set_ylabel('Average Rubrics')\n",
        "    axs[0, 1].set_title('Average Rubrics per Length Bucket')\n",
        "    axs[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Annotate counts on bars\n",
        "    # for bar, count in zip(bars_length, count_rubrics_length):\n",
        "    #     axs[0, 1].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count),\n",
        "    #                    ha='center', va='bottom')\n",
        "\n",
        "    # Average Reward per Rubrics Bucket\n",
        "    avg_reward, count_reward = calculate_average_and_count(raw_df, 'rubrics_bucket', 'reward')\n",
        "    bucket_labels_rubrics = [f\"[{boundaries['rubrics_boundaries'][i]:.2f}, {boundaries['rubrics_boundaries'][i + 1]:.2f})\" for i in range(len(boundaries['rubrics_boundaries']) - 1)]\n",
        "\n",
        "    bars_rubrics = axs[1, 0].bar(bucket_labels_rubrics, avg_reward, color='skyblue', edgecolor='black')\n",
        "    axs[1, 0].set_xlabel('Rubrics Bucket Boundaries')\n",
        "    axs[1, 0].set_ylabel('Average Reward')\n",
        "    axs[1, 0].set_title('Average Reward per Rubrics Bucket')\n",
        "    axs[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Annotate counts on bars\n",
        "    # for bar, count in zip(bars_rubrics, count_reward):\n",
        "    #     axs[1, 0].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count),\n",
        "    #                    ha='center', va='bottom')\n",
        "\n",
        "    # Average Reward per Conversation Length Bucket\n",
        "    avg_reward_length, count_reward_length = calculate_average_and_count(raw_df, 'length_bucket', 'reward')\n",
        "\n",
        "    bars_length_reward = axs[1, 1].bar(bucket_labels_length, avg_reward_length, color='skyblue', edgecolor='black')\n",
        "    axs[1, 1].set_xlabel('Response Length Bucket Boundaries')\n",
        "    axs[1, 1].set_ylabel('Average Reward')\n",
        "    axs[1, 1].set_title('Average Reward per Length Bucket')\n",
        "    axs[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Annotate counts on bars\n",
        "    # for bar, count in zip(bars_length_reward, count_reward_length):\n",
        "    #     axs[1, 1].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count),\n",
        "    #                    ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Process the directory and get the raw DataFrame\n",
        "    raw_df = process_directory(INPUT_ROOT)\n",
        "\n",
        "    if raw_df is not None:\n",
        "        # Bucket the data\n",
        "        raw_df = bucket_data(raw_df)\n",
        "\n",
        "        # Save the raw DataFrame to Parquet for further analysis\n",
        "        raw_df.to_parquet(OUTPUT_STATS_FILE, index=False)\n",
        "        print(f\"Raw DataFrame saved to {OUTPUT_STATS_FILE}\")\n",
        "\n",
        "        # Get bucket boundaries\n",
        "        boundaries = get_bucket_boundaries(raw_df)\n",
        "        print(\"Bucket Boundaries:\", boundaries)\n",
        "\n",
        "        # Generate plots\n",
        "        plot_averages_and_counts(raw_df, boundaries)\n",
        "\n",
        "    else:\n",
        "        print(\"No valid data found.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "_ihFjOEfn1Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_ROOT_SIDE =  \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics_filtered_0_8\"\n",
        "\n",
        "def bucket_data_with_boundaries(df, boundaries):\n",
        "    \"\"\"Assign buckets based on pre-calculated boundaries for reward, rubrics, and conversation length.\"\"\"\n",
        "    # Bucketing for conversation length\n",
        "    df['length_bucket'] = pd.cut(df['response_length'], bins=boundaries['length_boundaries'], labels=False, include_lowest=True)\n",
        "\n",
        "    # Bucketing for rubrics\n",
        "    df['rubrics_bucket'] = pd.cut(df['rubrics'], bins=boundaries['rubrics_boundaries'], labels=False, include_lowest=True)\n",
        "\n",
        "    # Bucketing for rewards\n",
        "    df['reward_bucket'] = pd.cut(df['reward'], bins=boundaries['reward_boundaries'], labels=False, include_lowest=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def plot_averages_and_counts_side_by_side(df1, df2, boundaries):\n",
        "    \"\"\"Plot average rubrics and rewards for both datasets in a side-by-side comparison, labeled as 'Partial' and 'Partial+Filter'.\"\"\"\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    fig.suptitle('Comparison of Average Rubrics and Average Reward per Bucket (Partial vs. Partial+Filter)')\n",
        "\n",
        "    # Labels for buckets\n",
        "    reward_labels = [f\"[{boundaries['reward_boundaries'][i]:.2f}, {boundaries['reward_boundaries'][i + 1]:.2f})\" for i in range(len(boundaries['reward_boundaries']) - 1)]\n",
        "    length_labels = [f\"[{boundaries['length_boundaries'][i]:.2f}, {boundaries['length_boundaries'][i + 1]:.2f})\" for i in range(len(boundaries['length_boundaries']) - 1)]\n",
        "    rubrics_labels = [f\"[{boundaries['rubrics_boundaries'][i]:.2f}, {boundaries['rubrics_boundaries'][i + 1]:.2f})\" for i in range(len(boundaries['rubrics_boundaries']) - 1)]\n",
        "\n",
        "    bar_width = 0.4  # Width of each bar\n",
        "    x = np.arange(len(reward_labels))  # The label locations\n",
        "\n",
        "    # Average Rubrics per Reward Bucket for both datasets\n",
        "    avg_rubrics1, _ = calculate_average_and_count(df1, 'reward_bucket', 'rubrics')\n",
        "    avg_rubrics2, _ = calculate_average_and_count(df2, 'reward_bucket', 'rubrics')\n",
        "\n",
        "    axs[0, 0].bar(x - bar_width / 2, avg_rubrics1, width=bar_width, label='Partial', color='skyblue', edgecolor='black')\n",
        "    axs[0, 0].bar(x + bar_width / 2, avg_rubrics2, width=bar_width, label='Partial+Filter', color='orange', edgecolor='black', alpha=0.7)\n",
        "    axs[0, 0].set_xticks(x)\n",
        "    axs[0, 0].set_xticklabels(reward_labels, rotation=45)\n",
        "    axs[0, 0].set_xlabel('Reward Bucket Boundaries')\n",
        "    axs[0, 0].set_ylabel('Average Rubrics')\n",
        "    axs[0, 0].set_title('Average Rubrics per Reward Bucket')\n",
        "    axs[0, 0].legend()\n",
        "\n",
        "    # Average Rubrics per Length Bucket for both datasets\n",
        "    avg_rubrics_length1, _ = calculate_average_and_count(df1, 'length_bucket', 'rubrics')\n",
        "    avg_rubrics_length2, _ = calculate_average_and_count(df2, 'length_bucket', 'rubrics')\n",
        "\n",
        "    x = np.arange(len(length_labels))  # Update x-axis positions\n",
        "    axs[0, 1].bar(x - bar_width / 2, avg_rubrics_length1, width=bar_width, label='Partial', color='skyblue', edgecolor='black')\n",
        "    axs[0, 1].bar(x + bar_width / 2, avg_rubrics_length2, width=bar_width, label='Partial+Filter', color='orange', edgecolor='black', alpha=0.7)\n",
        "    axs[0, 1].set_xticks(x)\n",
        "    axs[0, 1].set_xticklabels(length_labels, rotation=45)\n",
        "    axs[0, 1].set_xlabel('Response Length Bucket Boundaries')\n",
        "    axs[0, 1].set_ylabel('Average Rubrics')\n",
        "    axs[0, 1].set_title('Average Rubrics per Length Bucket')\n",
        "    axs[0, 1].legend()\n",
        "\n",
        "    # Average Reward per Rubrics Bucket for both datasets\n",
        "    avg_reward1, _ = calculate_average_and_count(df1, 'rubrics_bucket', 'reward')\n",
        "    avg_reward2, _ = calculate_average_and_count(df2, 'rubrics_bucket', 'reward')\n",
        "\n",
        "    x = np.arange(len(rubrics_labels))  # Update x-axis positions\n",
        "    axs[1, 0].bar(x - bar_width / 2, avg_reward1, width=bar_width, label='Partial', color='skyblue', edgecolor='black')\n",
        "    axs[1, 0].bar(x + bar_width / 2, avg_reward2, width=bar_width, label='Partial+Filter', color='orange', edgecolor='black', alpha=0.7)\n",
        "    axs[1, 0].set_xticks(x)\n",
        "    axs[1, 0].set_xticklabels(rubrics_labels, rotation=45)\n",
        "    axs[1, 0].set_xlabel('Rubrics Bucket Boundaries')\n",
        "    axs[1, 0].set_ylabel('Average Reward')\n",
        "    axs[1, 0].set_title('Average Reward per Rubrics Bucket')\n",
        "    axs[1, 0].legend()\n",
        "\n",
        "    # Average Reward per Length Bucket for both datasets\n",
        "    avg_reward_length1, _ = calculate_average_and_count(df1, 'length_bucket', 'reward')\n",
        "    avg_reward_length2, _ = calculate_average_and_count(df2, 'length_bucket', 'reward')\n",
        "\n",
        "    x = np.arange(len(length_labels))  # Reuse length labels for x-axis\n",
        "    axs[1, 1].bar(x - bar_width / 2, avg_reward_length1, width=bar_width, label='Partial', color='skyblue', edgecolor='black')\n",
        "    axs[1, 1].bar(x + bar_width / 2, avg_reward_length2, width=bar_width, label='Partial+Filter', color='orange', edgecolor='black', alpha=0.7)\n",
        "    axs[1, 1].set_xticks(x)\n",
        "    axs[1, 1].set_xticklabels(length_labels, rotation=45)\n",
        "    axs[1, 1].set_xlabel('Response Length Bucket Boundaries')\n",
        "    axs[1, 1].set_ylabel('Average Reward')\n",
        "    axs[1, 1].set_title('Average Reward per Length Bucket')\n",
        "    axs[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Process the main directory\n",
        "    raw_df1 = process_directory(INPUT_ROOT)\n",
        "\n",
        "    # Process the secondary directory\n",
        "    raw_df2 = process_directory(INPUT_ROOT_SIDE)\n",
        "\n",
        "    if raw_df1 is not None and raw_df2 is not None:\n",
        "        # Get bucket boundaries from the main dataset\n",
        "        boundaries = get_bucket_boundaries(raw_df1)\n",
        "\n",
        "        # Bucket both datasets using the same boundaries\n",
        "        raw_df1 = bucket_data_with_boundaries(raw_df1, boundaries)\n",
        "        raw_df2 = bucket_data_with_boundaries(raw_df2, boundaries)\n",
        "\n",
        "        # Save the raw DataFrame to Parquet for further analysis\n",
        "        raw_df1.to_parquet(OUTPUT_STATS_FILE, index=False)\n",
        "        print(f\"Raw DataFrame saved to {OUTPUT_STATS_FILE}\")\n",
        "\n",
        "        # Plot the results side by side for both datasets\n",
        "        plot_averages_and_counts_side_by_side(raw_df1, raw_df2, boundaries)\n",
        "\n",
        "    else:\n",
        "        print(\"No valid data found in one or both directories.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ZqvPzzCTj_KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pyarrow.parquet as pq\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "\n",
        "input_folder = \"/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics\"\n",
        "\n",
        "total_rows = 0\n",
        "total_files = 0\n",
        "total_rows_with_rubrics = 0\n",
        "total_rows_with_rubris_null = 0\n",
        "all_rows_rewards = {}\n",
        "empty_wrong_files = []\n",
        "\n",
        "# Traverse the directory structure\n",
        "for dir in os.listdir(input_folder):\n",
        "    input_path = os.path.join(input_folder, dir)\n",
        "    total_files += 1\n",
        "    try:\n",
        "      if len(os.listdir(input_path)) == 0:\n",
        "        print(\"!!!!EMPTY/WRONG\", input_path)\n",
        "        empty_wrong_files += [dir]\n",
        "        continue\n",
        "      print(input_path)\n",
        "      data = utils.read_df(glob.glob(input_path + \"/*.parquet\"))\n",
        "      all_rows_rewards[dir]  = []\n",
        "      total_rows += data.shape[0]\n",
        "      for _, d in data[\"rubric_metadata\"].items():\n",
        "          r = None\n",
        "          if d:\n",
        "            r = d.get(\"rubric_verifier\", {}).get(\"reward\", None)\n",
        "            # print(f\"{d=} {r=} {r is None}\")\n",
        "            if r is not None:\n",
        "              all_rows_rewards[dir] += [r]\n",
        "          total_rows_with_rubrics += r is not None\n",
        "          total_rows_with_rubris_null += r is None\n",
        "      print(f\"\\n>>>{total_files=} {total_rows=} {total_rows_with_rubrics=} {total_rows_with_rubris_null=}\")\n",
        "    except Exception as e :\n",
        "      print(\"Exception: \", e)\n",
        "      empty_wrong_files += [dir]\n",
        "\n",
        "total_rows_with_rubrics_more_than_05= sum(v> 0.5 for v in rewards_all)\n",
        "print(f\"\\nFINAL {total_files=} {total_rows=} {total_rows_with_rubrics=} {total_rows_with_rubris_null=} {total_rows_with_rubrics_more_than_05=} \")\n",
        "print(empty_wrong_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpVz217hah7J",
        "outputId": "82c0b35c-39f7-45aa-da84-4985f11ff12c"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/meta-knowledge_train_sft\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 190.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=1 total_rows=58 total_rows_with_rubrics=58 total_rows_with_rubris_null=0\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/custom_convos-v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 147.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=2 total_rows=163 total_rows_with_rubrics=163 total_rows_with_rubris_null=0\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/af_top2_reward_math\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=3 total_rows=13880 total_rows_with_rubrics=13519 total_rows_with_rubris_null=361\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/mapcf_claude_40k\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=4 total_rows=19350 total_rows_with_rubrics=13613 total_rows_with_rubris_null=5737\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/arc_title_snippet_sum_lex_pass_rand\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 53.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=5 total_rows=19484 total_rows_with_rubrics=13742 total_rows_with_rubris_null=5742\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/q0220_top10_arc_dates_snippet_title_fulltext_reversedateorder_rand_pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 79.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=6 total_rows=19517 total_rows_with_rubrics=13762 total_rows_with_rubris_null=5755\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/q0209_top10_arc_dates_snippet_title_fulltext_reversedateorder_rand_pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 56.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=7 total_rows=19577 total_rows_with_rubrics=13795 total_rows_with_rubris_null=5782\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/lepton_chunk_embedder_pass_rand\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 60.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=8 total_rows=19788 total_rows_with_rubrics=13892 total_rows_with_rubris_null=5896\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/multiturn_25k_d_f_turn_6_20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=9 total_rows=24234 total_rows_with_rubrics=18201 total_rows_with_rubris_null=6033\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/ultrafeed_wizardlm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 51.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=10 total_rows=25620 total_rows_with_rubrics=19558 total_rows_with_rubris_null=6062\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/ja_0404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=11 total_rows=30554 total_rows_with_rubrics=24480 total_rows_with_rubris_null=6074\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/leetcode_devins\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 41.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=12 total_rows=32854 total_rows_with_rubrics=26454 total_rows_with_rubris_null=6400\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/openchat_xai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 42.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=13 total_rows=33553 total_rows_with_rubrics=27141 total_rows_with_rubris_null=6412\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/lepton_chunk_rejector_pass_rand\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 75.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=14 total_rows=33766 total_rows_with_rubrics=27260 total_rows_with_rubris_null=6506\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/leetcode_direct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=15 total_rows=37259 total_rows_with_rubrics=27260 total_rows_with_rubris_null=9999\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/ultrafeed_sharegpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=16 total_rows=40120 total_rows_with_rubrics=27260 total_rows_with_rubris_null=12860\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/q0220_bing2_top10_arc_dates_snippet_title_fulltext_reversedateorder_rand_pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 83.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=17 total_rows=40154 total_rows_with_rubrics=27260 total_rows_with_rubris_null=12894\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/lmsys_data_share_vibes_0809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 332.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=18 total_rows=40170 total_rows_with_rubrics=27260 total_rows_with_rubris_null=12910\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/4o_outputs_37k_filtered_d_f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=19 total_rows=69493 total_rows_with_rubrics=56398 total_rows_with_rubris_null=13095\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/chatgpt_exports_all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 49.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=20 total_rows=70177 total_rows_with_rubrics=56708 total_rows_with_rubris_null=13469\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/apps_cld_refresh\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=21 total_rows=78118 total_rows_with_rubrics=64452 total_rows_with_rubris_null=13666\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/xai_instruct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=22 total_rows=96184 total_rows_with_rubrics=82368 total_rows_with_rubris_null=13816\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/bigcodebench_direct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=23 total_rows=97293 total_rows_with_rubrics=83426 total_rows_with_rubris_null=13867\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/stacex_vendor_lmsyslike_top20k_proc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=24 total_rows=102763 total_rows_with_rubrics=83520 total_rows_with_rubris_null=19243\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/lepton_sum_gpt4_pass_rand\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 75.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=25 total_rows=102942 total_rows_with_rubrics=83694 total_rows_with_rubris_null=19248\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/claude_on_removed_prompts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=26 total_rows=114431 total_rows_with_rubrics=94971 total_rows_with_rubris_null=19460\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/wildchat_ja\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 129.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=27 total_rows=114597 total_rows_with_rubrics=95133 total_rows_with_rubris_null=19464\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/qwen_filtered_synthetic_math1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  7.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=28 total_rows=184597 total_rows_with_rubrics=163669 total_rows_with_rubris_null=20928\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/capybara\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=29 total_rows=191235 total_rows_with_rubrics=170293 total_rows_with_rubris_null=20942\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/strawberry-v3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 180.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=30 total_rows=191439 total_rows_with_rubrics=170497 total_rows_with_rubris_null=20942\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/dry_run_mbpp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=31 total_rows=196909 total_rows_with_rubrics=170591 total_rows_with_rubris_null=26318\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/qwen_filtered_gsm8k1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=32 total_rows=202471 total_rows_with_rubrics=176153 total_rows_with_rubris_null=26318\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/qwen_filtered_olympiads1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  6.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=33 total_rows=232471 total_rows_with_rubrics=205237 total_rows_with_rubris_null=27234\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/q0209_top10_lepton_dates_snippet_title_fulltext_rand_pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 118.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=34 total_rows=232498 total_rows_with_rubrics=205253 total_rows_with_rubris_null=27245\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/wildchat_imperative\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 53.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=35 total_rows=232873 total_rows_with_rubrics=205626 total_rows_with_rubris_null=27247\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/sft_proc_k4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  5.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=36 total_rows=260061 total_rows_with_rubrics=232295 total_rows_with_rubris_null=27766\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/ver-v1-7_distilled_no_refusal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=37 total_rows=261274 total_rows_with_rubrics=232684 total_rows_with_rubris_null=28590\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/qwen_filtered_math1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=38 total_rows=268140 total_rows_with_rubrics=239517 total_rows_with_rubris_null=28623\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/arc_title_snippet_sum_gpt4_pass_rand\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=39 total_rows=268361 total_rows_with_rubrics=239725 total_rows_with_rubris_null=28636\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/identity_multilingual_singleturn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=40 total_rows=274395 total_rows_with_rubrics=245658 total_rows_with_rubris_null=28737\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/identity_grok_multiturn-v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 74.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=41 total_rows=274899 total_rows_with_rubrics=246161 total_rows_with_rubris_null=28738\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/4o_output_cba_24k_d_f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=42 total_rows=296259 total_rows_with_rubrics=267254 total_rows_with_rubris_null=29005\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/sft_mixture_misgender_with_rag_subsampled\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 198.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=43 total_rows=296379 total_rows_with_rubrics=267370 total_rows_with_rubris_null=29009\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/complex_query_single_doc_single_turn_1013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=44 total_rows=301849 total_rows_with_rubrics=267464 total_rows_with_rubris_null=34385\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/lepton_sum_gpt4_old_pass_rand\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 72.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=45 total_rows=302019 total_rows_with_rubrics=267565 total_rows_with_rubris_null=34454\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/q0220_para_title_snippet_sum_gpt4_pass_rand\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 186.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=46 total_rows=302056 total_rows_with_rubrics=267585 total_rows_with_rubris_null=34471\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/cc_selfrepair\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=47 total_rows=306743 total_rows_with_rubrics=272219 total_rows_with_rubris_null=34524\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/cnbc_v2_ge4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 139.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=48 total_rows=306901 total_rows_with_rubrics=272377 total_rows_with_rubris_null=34524\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/arc_title_snippet_sum_tex_pass_rand\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 64.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=49 total_rows=306997 total_rows_with_rubrics=272472 total_rows_with_rubris_null=34525\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/q0209_lepton_fulltext_rand_pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 118.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=50 total_rows=307020 total_rows_with_rubrics=272480 total_rows_with_rubris_null=34540\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/scale_prod_1to7_amazing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=51 total_rows=307816 total_rows_with_rubrics=273242 total_rows_with_rubris_null=34574\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/col_r_on_competitive_coding_proc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=52 total_rows=309132 total_rows_with_rubrics=274555 total_rows_with_rubris_null=34577\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/synthetic_humaneval_cot\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=53 total_rows=313573 total_rows_with_rubrics=278721 total_rows_with_rubris_null=34852\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/format_sft_lmsys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=54 total_rows=319043 total_rows_with_rubrics=278815 total_rows_with_rubris_null=40228\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/lmsys_xai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 46.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=55 total_rows=320464 total_rows_with_rubrics=280230 total_rows_with_rubris_null=40234\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/simple_query_single_doc_multi_turn_0905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=56 total_rows=325934 total_rows_with_rubrics=280324 total_rows_with_rubris_null=45610\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/vibe_decimal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 101.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=57 total_rows=326600 total_rows_with_rubrics=280990 total_rows_with_rubris_null=45610\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/mbpp_star\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=58 total_rows=329955 total_rows_with_rubrics=284343 total_rows_with_rubris_null=45612\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/vibes_kenya_proc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 311.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=59 total_rows=329965 total_rows_with_rubrics=284353 total_rows_with_rubris_null=45612\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/xai_instruct_0426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=60 total_rows=339965 total_rows_with_rubrics=294098 total_rows_with_rubris_null=45867\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/ko_ja_de_4o_filtered_flang\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=61 total_rows=371315 total_rows_with_rubrics=325172 total_rows_with_rubris_null=46143\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/identity_grok-v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 74.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=62 total_rows=372205 total_rows_with_rubrics=325172 total_rows_with_rubris_null=47033\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/q0220_para_title_snippet_sum_tex_pass_rand\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 108.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=63 total_rows=372231 total_rows_with_rubrics=325172 total_rows_with_rubris_null=47059\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/q0209_top10_lepton_fulltext_rand_pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 100.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=64 total_rows=372263 total_rows_with_rubrics=325172 total_rows_with_rubris_null=47091\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/grok_lmsys_wildchat_longalign_241021_gt8k\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=65 total_rows=377144 total_rows_with_rubrics=325172 total_rows_with_rubris_null=51972\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/q0209_top10_lepton_dates_fulltext_rand_pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 86.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=66 total_rows=377182 total_rows_with_rubrics=325172 total_rows_with_rubris_null=52010\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/wildchat_requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 50.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=67 total_rows=377556 total_rows_with_rubrics=325527 total_rows_with_rubris_null=52029\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/humanity_vibes_0809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 211.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=68 total_rows=377694 total_rows_with_rubrics=325527 total_rows_with_rubris_null=52167\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/humanity_vibes_proc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 42.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=69 total_rows=379967 total_rows_with_rubrics=325527 total_rows_with_rubris_null=54440\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/selfexec_p25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=70 total_rows=385437 total_rows_with_rubrics=325621 total_rows_with_rubris_null=59816\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/wildchat_multiturn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=71 total_rows=399644 total_rows_with_rubrics=330837 total_rows_with_rubris_null=68807\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/alpaca_exclude_rewrite\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=72 total_rows=443290 total_rows_with_rubrics=374306 total_rows_with_rubris_null=68984\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/claude3_5_codegen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=73 total_rows=448760 total_rows_with_rubrics=374400 total_rows_with_rubris_null=74360\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/multi_turn_gpt_50-100_filter_filterrefusal_f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=74 total_rows=452547 total_rows_with_rubrics=378104 total_rows_with_rubris_null=74443\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/identity_multilingual_multiturn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=75 total_rows=457763 total_rows_with_rubrics=383263 total_rows_with_rubris_null=74500\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/multi_turn_gpt_0-50_filter_filterrefusal_f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=76 total_rows=461599 total_rows_with_rubrics=387028 total_rows_with_rubris_null=74571\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/ambiguous_query_train_sft\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 204.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=77 total_rows=461633 total_rows_with_rubrics=387061 total_rows_with_rubris_null=74572\n",
            "/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/qwen_filtered_aops_forum1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>total_files=78 total_rows=467103 total_rows_with_rubrics=392336 total_rows_with_rubris_null=74767\n",
            "\n",
            "FINAL total_files=78 total_rows=467103 total_rows_with_rubrics=392336 total_rows_with_rubris_null=74767 total_rows_with_rubrics_more_than_05=200267 \n",
            "[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_all = sum([v for v in all_rows_rewards.values() if v is not None], [])\n",
        "print(len(rewards_all))\n",
        "print(sum(v> 0.5 for v in rewards_all if v is not None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dYMbVce2j1E",
        "outputId": "032322c2-3bac-4817-df0f-5a132fbb1e8a"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "392336\n",
            "200267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "ref_folder = \"/data/datasets/sft-data/staging/common_filter_102924_fix_cutoff_w_rubrics_repetition_redone/\"\n",
        "ref_folder = \"/data/yuchen/bk_data\"\n",
        "output_folder = \"/data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics\"\n",
        "\n",
        "for m in empty_files:\n",
        "  path_ref = ref_folder + \"/\" + m\n",
        "  path_output = output_folder +\"/\" + m\n",
        "  print(\"\\n\", path_ref, path_output)\n",
        "  if len(os.listdir(path_ref)) > 0:\n",
        "    # if os.path.exists(path_output):\n",
        "    #     print(\"backup the folder first\", path_output)\n",
        "    #     shutil.move(path_output, \"/data/yuchen/bk_data/\")\n",
        "    df = utils.read_df(glob.glob(input_path + \"/*.parquet\"))\n",
        "    # df = df.drop_duplicates([id])\n",
        "\n",
        "    # if \"rubric_metadata\" in df.columns:\n",
        "    #   print(\"\\n already exists in columsn \",df.columns.tolist())\n",
        "\n",
        "    #   shutil.copytree(path_ref, path_output)\n",
        "    # else:\n",
        "    df['rubric_metadata'] = df['metadata']\n",
        "    df = df.drop(columns=['metadata'])\n",
        "    print(df.columns.tolist(), path_output)\n",
        "    utils.df_to_parquet(df, path_output)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4pK6uThFoHu",
        "outputId": "48b62bb0-14a2-4e0c-b862-d2dac7e5af82"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mapcf_claude_40k', 'stacex_vendor_lmsyslike_top20k_proc', 'dry_run_mbpp', 'complex_query_single_doc_single_turn_1013', 'format_sft_lmsys', 'simple_query_single_doc_multi_turn_0905', 'selfexec_p25', 'claude3_5_codegen']\n",
            "\n",
            " /data/yuchen/bk_data/mapcf_claude_40k /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/mapcf_claude_40k\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['prompt', 'gt_solution', 'pred_solution', 'correct', 'id', 'conversation', 'conversation_str', 'repetitive/reasoning', 'repetitive/label', 'domain', 'rubric_metadata'] /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/mapcf_claude_40k\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing df as parquets: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " /data/yuchen/bk_data/stacex_vendor_lmsyslike_top20k_proc /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/stacex_vendor_lmsyslike_top20k_proc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['prompt', 'gt_solution', 'pred_solution', 'correct', 'id', 'conversation', 'conversation_str', 'repetitive/reasoning', 'repetitive/label', 'domain', 'rubric_metadata'] /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/stacex_vendor_lmsyslike_top20k_proc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing df as parquets: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " /data/yuchen/bk_data/dry_run_mbpp /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/dry_run_mbpp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['prompt', 'gt_solution', 'pred_solution', 'correct', 'id', 'conversation', 'conversation_str', 'repetitive/reasoning', 'repetitive/label', 'domain', 'rubric_metadata'] /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/dry_run_mbpp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing df as parquets: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " /data/yuchen/bk_data/complex_query_single_doc_single_turn_1013 /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/complex_query_single_doc_single_turn_1013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['prompt', 'gt_solution', 'pred_solution', 'correct', 'id', 'conversation', 'conversation_str', 'repetitive/reasoning', 'repetitive/label', 'domain', 'rubric_metadata'] /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/complex_query_single_doc_single_turn_1013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing df as parquets: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " /data/yuchen/bk_data/format_sft_lmsys /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/format_sft_lmsys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['prompt', 'gt_solution', 'pred_solution', 'correct', 'id', 'conversation', 'conversation_str', 'repetitive/reasoning', 'repetitive/label', 'domain', 'rubric_metadata'] /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/format_sft_lmsys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing df as parquets: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " /data/yuchen/bk_data/simple_query_single_doc_multi_turn_0905 /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/simple_query_single_doc_multi_turn_0905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['prompt', 'gt_solution', 'pred_solution', 'correct', 'id', 'conversation', 'conversation_str', 'repetitive/reasoning', 'repetitive/label', 'domain', 'rubric_metadata'] /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/simple_query_single_doc_multi_turn_0905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing df as parquets: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " /data/yuchen/bk_data/selfexec_p25 /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/selfexec_p25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['prompt', 'gt_solution', 'pred_solution', 'correct', 'id', 'conversation', 'conversation_str', 'repetitive/reasoning', 'repetitive/label', 'domain', 'rubric_metadata'] /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/selfexec_p25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing df as parquets: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " /data/yuchen/bk_data/claude3_5_codegen /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/claude3_5_codegen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['prompt', 'gt_solution', 'pred_solution', 'correct', 'id', 'conversation', 'conversation_str', 'repetitive/reasoning', 'repetitive/label', 'domain', 'rubric_metadata'] /data/datasets/sft-data/staging/common_filter_110124_tmp_rubrics/claude3_5_codegen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing df as parquets: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "eval_results_1 = json.load(open(\"/data/evals/yuchen/gifeval/raw/v5l-cutoff-partial-sglang.yuchen.svc.max.x.ai/2024-10-23-060441/metrics.json\"))\n",
        "# eval_results_2 = json.load(open(\"/data/evals/yuchen/gifeval/raw/v5l-cutoff-partial-filter05-sglang.yuchen.svc.max.x.ai/2024-10-23-061006/metrics.json\"))\n",
        "eval_results_2 = json.load(open(\"/data/evals/yuchen/gifeval/raw/v5l-cutoff-partial-filter08-sglang.yuchen.svc.max.x.ai/2024-10-23-085555/metrics.json\"))\n",
        "\n",
        "eval_results_by_cat = lambda eval_results, by_cat: reduce(\n",
        "  lambda x, y: {**x, **y},\n",
        "  [\n",
        "   {'gifeval' + \":\" + m + \"_\" + k.replace(\" (exclude tie)\", \"\"): float(v[:v.find(\"(\")]) for k, v in kv.items() if 'win_rate_by_' in k }\n",
        "   for m, kv in eval_results[by_cat].items()\n",
        "  ]\n",
        ")\n",
        "by = 'by_source'\n",
        "by = 'by_category'\n",
        "eval_results_1_by_cat = eval_results_by_cat(eval_results_1, by)\n",
        "eval_results_2_by_cat = eval_results_by_cat(eval_results_2, by)\n",
        "\n",
        "data = [ {'metric': k, 'partial': eval_results_1_by_cat[k] ,  'partial_filter08': eval_results_2_by_cat[k]}  for k  in eval_results_1_by_cat]\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "kvgPrGzviacR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2185bd-6517-482f-e7cf-1a496f5a9b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      metric  partial  partial_filter08\n",
              "0           gifeval:Advice_win_rate_by_label    55.31             50.71\n",
              "1           gifeval:Advice_win_rate_by_score    46.22             39.11\n",
              "2    gifeval:Brainstorming_win_rate_by_label    47.69             44.73\n",
              "3    gifeval:Brainstorming_win_rate_by_score    44.00             44.00\n",
              "4   gifeval:Classification_win_rate_by_label    45.39             35.86\n",
              "5   gifeval:Classification_win_rate_by_score    43.64             38.91\n",
              "6        gifeval:Closed QA_win_rate_by_label    37.88             29.59\n",
              "7        gifeval:Closed QA_win_rate_by_score    48.44             40.44\n",
              "8   gifeval:Conversational_win_rate_by_label    27.99             28.14\n",
              "9   gifeval:Conversational_win_rate_by_score    37.33             41.33\n",
              "10      gifeval:Extraction_win_rate_by_label    39.80             33.78\n",
              "11      gifeval:Extraction_win_rate_by_score    43.27             36.00\n",
              "12    gifeval:Modification_win_rate_by_label    30.94             30.91\n",
              "13    gifeval:Modification_win_rate_by_score    30.00             30.00\n",
              "14         gifeval:Open QA_win_rate_by_label    28.79             31.38\n",
              "15         gifeval:Open QA_win_rate_by_score    38.55             36.00\n",
              "16           gifeval:Other_win_rate_by_label    31.43             27.99\n",
              "17           gifeval:Other_win_rate_by_score    31.11             30.67\n",
              "18         gifeval:Persona_win_rate_by_label    40.72             38.67\n",
              "19         gifeval:Persona_win_rate_by_score    44.00             41.33\n",
              "20       gifeval:Retrieval_win_rate_by_label    35.86             30.29\n",
              "21       gifeval:Retrieval_win_rate_by_score    36.00             30.00\n",
              "22       gifeval:Rewriting_win_rate_by_label    41.82             35.78\n",
              "23       gifeval:Rewriting_win_rate_by_score    51.56             41.78\n",
              "24   gifeval:Summarization_win_rate_by_label    32.49             21.74\n",
              "25   gifeval:Summarization_win_rate_by_score    35.11             29.78\n",
              "26         gifeval:Writing_win_rate_by_label    21.47             20.85\n",
              "27         gifeval:Writing_win_rate_by_score    27.11             32.00"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>metric</th>\n",
              "      <th>partial</th>\n",
              "      <th>partial_filter08</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gifeval:Advice_win_rate_by_label</td>\n",
              "      <td>55.31</td>\n",
              "      <td>50.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gifeval:Advice_win_rate_by_score</td>\n",
              "      <td>46.22</td>\n",
              "      <td>39.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gifeval:Brainstorming_win_rate_by_label</td>\n",
              "      <td>47.69</td>\n",
              "      <td>44.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gifeval:Brainstorming_win_rate_by_score</td>\n",
              "      <td>44.00</td>\n",
              "      <td>44.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gifeval:Classification_win_rate_by_label</td>\n",
              "      <td>45.39</td>\n",
              "      <td>35.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gifeval:Classification_win_rate_by_score</td>\n",
              "      <td>43.64</td>\n",
              "      <td>38.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>gifeval:Closed QA_win_rate_by_label</td>\n",
              "      <td>37.88</td>\n",
              "      <td>29.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>gifeval:Closed QA_win_rate_by_score</td>\n",
              "      <td>48.44</td>\n",
              "      <td>40.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>gifeval:Conversational_win_rate_by_label</td>\n",
              "      <td>27.99</td>\n",
              "      <td>28.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>gifeval:Conversational_win_rate_by_score</td>\n",
              "      <td>37.33</td>\n",
              "      <td>41.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>gifeval:Extraction_win_rate_by_label</td>\n",
              "      <td>39.80</td>\n",
              "      <td>33.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>gifeval:Extraction_win_rate_by_score</td>\n",
              "      <td>43.27</td>\n",
              "      <td>36.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>gifeval:Modification_win_rate_by_label</td>\n",
              "      <td>30.94</td>\n",
              "      <td>30.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>gifeval:Modification_win_rate_by_score</td>\n",
              "      <td>30.00</td>\n",
              "      <td>30.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>gifeval:Open QA_win_rate_by_label</td>\n",
              "      <td>28.79</td>\n",
              "      <td>31.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>gifeval:Open QA_win_rate_by_score</td>\n",
              "      <td>38.55</td>\n",
              "      <td>36.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>gifeval:Other_win_rate_by_label</td>\n",
              "      <td>31.43</td>\n",
              "      <td>27.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>gifeval:Other_win_rate_by_score</td>\n",
              "      <td>31.11</td>\n",
              "      <td>30.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>gifeval:Persona_win_rate_by_label</td>\n",
              "      <td>40.72</td>\n",
              "      <td>38.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>gifeval:Persona_win_rate_by_score</td>\n",
              "      <td>44.00</td>\n",
              "      <td>41.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>gifeval:Retrieval_win_rate_by_label</td>\n",
              "      <td>35.86</td>\n",
              "      <td>30.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>gifeval:Retrieval_win_rate_by_score</td>\n",
              "      <td>36.00</td>\n",
              "      <td>30.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>gifeval:Rewriting_win_rate_by_label</td>\n",
              "      <td>41.82</td>\n",
              "      <td>35.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>gifeval:Rewriting_win_rate_by_score</td>\n",
              "      <td>51.56</td>\n",
              "      <td>41.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>gifeval:Summarization_win_rate_by_label</td>\n",
              "      <td>32.49</td>\n",
              "      <td>21.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>gifeval:Summarization_win_rate_by_score</td>\n",
              "      <td>35.11</td>\n",
              "      <td>29.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>gifeval:Writing_win_rate_by_label</td>\n",
              "      <td>21.47</td>\n",
              "      <td>20.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>gifeval:Writing_win_rate_by_score</td>\n",
              "      <td>27.11</td>\n",
              "      <td>32.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Surge preference data 10/25\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9nFpVNt_NnvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_grained_output = json.load(open(\"/data/datasets/preferences-v2/surge/raw/Fine-Grained Outcomes #2 - 2024-10-08 - training.json\"))"
      ],
      "metadata": {
        "id": "8ANZxwwtNqMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tEumPYbtS_ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append(os.path.expanduser('~/xai/datasets/spark/preference/'))\n",
        "from scratch.surge import Surge\n",
        "\n",
        "file_name = 'general_2'\n",
        "file_path = 'RLHF - General - Batch 2.json'\n",
        "\n",
        "sdata = SurgeRenew(\n",
        "    name=file_name.replace(\"surge_\", \"surge_finegrain_\"),\n",
        "    path_glob=f\"/data/datasets/preferences-v2/surge/{file_path}\",\n",
        "    is_confident=False,\n",
        "    is_hard=False,\n",
        "    is_aspect=True,\n",
        ").raw(regenerate_if_found=True).filtered()\n",
        "\n",
        "\n",
        "output_path = RAW_PATH / sdata.name\n",
        "data = pd.read_parquet(output_path / \"part-00000-of-00001.parquet\")\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCH6TeNhTV0i",
        "outputId": "1bfa7955-5fbe-4524-a1bd-cdef652b8be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SurgeRenew' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[198], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral_2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRLHF - General - Batch 2.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m sdata \u001b[38;5;241m=\u001b[39m \u001b[43mSurgeRenew\u001b[49m(\n\u001b[1;32m      9\u001b[0m     name\u001b[38;5;241m=\u001b[39mfile_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurge_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurge_finegrain_\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m     path_glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/datasets/preferences-v2/surge/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     is_confident\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     is_hard\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     is_aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m )\u001b[38;5;241m.\u001b[39mraw(regenerate_if_found\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfiltered()\n\u001b[1;32m     17\u001b[0m output_path \u001b[38;5;241m=\u001b[39m RAW_PATH \u001b[38;5;241m/\u001b[39m sdata\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(output_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SurgeRenew' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['quality_rating_B']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qap9WmgwWbfI",
        "outputId": "f85d7ad0-0672-4cf5-f515-40ab85bdb399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        4.0\n",
              "1     -100.0\n",
              "2        2.0\n",
              "3     -100.0\n",
              "4     -100.0\n",
              "       ...  \n",
              "258      4.0\n",
              "259      2.0\n",
              "260   -100.0\n",
              "261   -100.0\n",
              "262      5.0\n",
              "Name: quality_rating_B, Length: 263, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @serde.serde\n",
        "import copy\n",
        "import sys\n",
        "sys.path.append(os.path.expanduser('~/xai/datasets/spark/preference/'))\n",
        "\n",
        "from scratch.surge import *\n",
        "\n",
        "@serde.serde\n",
        "class Surge(common.PreferenceDatasetPipeline):\n",
        "    name: str\n",
        "    path_glob: str\n",
        "    is_hard: bool\n",
        "    is_confident: bool\n",
        "    is_aspect: bool\n",
        "    rubrics_type: int = 1\n",
        "    train_rubrics: bool = True\n",
        "    sft_data_generator = None\n",
        "\n",
        "    def create_preference_records(\n",
        "        self,\n",
        "        record: dict,\n",
        "        is_hard: bool = False,  # Produce hard preferences.\n",
        "        is_confident: bool = False,  # Include only confident confident preferences.\n",
        "        is_aspect: bool = False,  # Include per-aspect preferences.\n",
        "    ):\n",
        "        if self.sft_data_generator is None:\n",
        "            self.sft_data_generator = ContextualRewardGenerator(\n",
        "                # rubrics_type=self.rubrics_type, train_rubrics=self.train_rubrics\n",
        "            )\n",
        "            self.sft_data_generator.initialize()\n",
        "        task_id = record[\"task_id\"]\n",
        "        try:\n",
        "            conv = json.loads(record[\"Conversation\"])\n",
        "        except:\n",
        "            conv = json5.loads(record[\"conversation\"])\n",
        "\n",
        "        if \"system_prompt_input\" in record and record[\"system_prompt_input\"] != \"\":\n",
        "            history = [dict(role=\"system\", content=record[\"system_prompt_input\"])]\n",
        "        elif \"1. system_prompt_input\" in record and record[\"1. system_prompt_input\"] != \"\":\n",
        "            history = [dict(role=\"system\", content=record[\"1. system_prompt_input\"])]\n",
        "        elif \"model_1_system_prompt\" in conv and conv[\"model_1_system_prompt\"] != \"\":\n",
        "            # assume model_1_system_prompt = model_2_system_prompt\n",
        "            assert (\n",
        "                conv[\"model_1_system_prompt\"] == conv[\"model_2_system_prompt\"]\n",
        "            ), \"model_1_system_prompt != model_2_system_prompt\"\n",
        "            history = [dict(role=\"system\", content=conv[\"model_1_system_prompt\"])]\n",
        "        else:\n",
        "            history = []\n",
        "        local_preference_records = []\n",
        "        local_counter = collections.Counter()\n",
        "        somekeys = {}\n",
        "        bad_conversations = []\n",
        "\n",
        "        for turn_id, message in enumerate(conv[\"messages\"]):\n",
        "            message = SurgeMessage(**message)\n",
        "\n",
        "            if turn_id % 2 == 0:\n",
        "                assert message.turn == \"human\"\n",
        "\n",
        "                user_content = message.choices[0].message\n",
        "                assert EOS_TOKEN not in user_content\n",
        "\n",
        "                user_message = dict(role=\"user\", content=user_content)\n",
        "                history.append(user_message)\n",
        "            else:\n",
        "                assert message.turn == \"models\"\n",
        "                round_id = turn_id // 2 + 1\n",
        "\n",
        "                if not any(choice.message for choice in message.choices):\n",
        "                    local_counter[\"filter:both_empty_response\"] += 1\n",
        "                    break\n",
        "\n",
        "                if any(not choice.message for choice in message.choices):\n",
        "                    local_counter[\"filter:one_empty_response\"] += 1\n",
        "                    break\n",
        "\n",
        "                if sum(choice.selected is True for choice in message.choices) != 1:\n",
        "                    local_counter[\"filter:no_preferred_response\"] += 1\n",
        "                    break\n",
        "\n",
        "                if \"Preference\" in record:\n",
        "                    if record[\"Preference\"]:\n",
        "                        text_preference = record[\"Preference\"][\"text\"]\n",
        "                    else:\n",
        "                        local_counter[\"filter:no_preference\"] += 1\n",
        "                        break\n",
        "                else:\n",
        "                    if record[f\"{round_id}. Preference\"]:\n",
        "                        text_preference = record[f\"{round_id}. Preference\"][\"text\"]\n",
        "                    else:\n",
        "                        local_counter[\"filter:no_preference\"] += 1\n",
        "                        break\n",
        "                if \"Explanation\" in record and record[\"Explanation\"]:\n",
        "                    text_explanation = record[\"Explanation\"]\n",
        "                elif f\"{round_id}. Explanation\" in record and record[f\"{round_id}. Explanation\"]:\n",
        "                    text_explanation = record[f\"{round_id}. Explanation\"]\n",
        "                else:\n",
        "                    local_counter[\"filter:no_explanation\"] += 1\n",
        "                    break\n",
        "\n",
        "\n",
        "                preference = get_numerical_preference(text_preference)\n",
        "                domain = \"unknown\"\n",
        "                if (\n",
        "                    f\"{round_id}. Prompt category\" in record\n",
        "                    and record[f\"{round_id}. Prompt category\"]\n",
        "                    and isinstance(record[f\"{round_id}. Prompt category\"], str)\n",
        "                ):\n",
        "                    domain = record[f\"{round_id}. Prompt category\"]\n",
        "                elif \"math\" in self.name:\n",
        "                    domain = \"math\"\n",
        "                elif \"coding\" in self.name:\n",
        "                    domain = \"coding\"\n",
        "                text_explanation = text_explanation.replace(\"response 1\", \"response a\").replace(\n",
        "                    \"response 2\", \"response b\"\n",
        "                )\n",
        "                preference_record = dict(\n",
        "                    id=f\"{task_id}:{round_id}\", domain=domain, critic=text_explanation,\n",
        "                    conversation=list(), conv_loss_mask=list(),\n",
        "                )\n",
        "                edit_tuple = dict(content=None, edit_id=None)\n",
        "                for i, choice in enumerate(message.choices):\n",
        "                    content: str = choice.message.split(EOS_TOKEN)[0]\n",
        "                    if choice.original_message is not None:\n",
        "                        local_counter[\"user_edit\"] += 1\n",
        "                        content = choice.original_message.split(EOS_TOKEN)[0]\n",
        "                        edit_content = choice.message.split(EOS_TOKEN)[0]\n",
        "                        if common.get_rouge_score(content, edit_content) < 0.95:\n",
        "                            edit_tuple = dict(content=edit_content, edit_id=i)\n",
        "                        else:\n",
        "                            local_counter[\"rewrite_too_similar\"] += 1\n",
        "                    assistant_message = dict(role=\"assistant\", content=content)\n",
        "                    preference_record[f\"conversation{i}\"] = history + [assistant_message]\n",
        "                    # preference_record[f\"response{i}\"] = content\n",
        "\n",
        "                    if choice.selected:\n",
        "                        preferred_assistant_message = assistant_message\n",
        "\n",
        "                # Don't include non-confident preferences.\n",
        "                if is_confident and not preference_is_confident(preference):\n",
        "                    local_counter[\"filter:preference_not_confident\"] += 1\n",
        "                    continue\n",
        "\n",
        "                # Check preference.\n",
        "                if is_hard:\n",
        "                    preference = common.round_preference_exclude_tie(preference)\n",
        "\n",
        "                preference_record[\"preference_overall\"] = preference\n",
        "                assert len(preference_record[\"conversation0\"]) == len(\n",
        "                    preference_record[\"conversation1\"]\n",
        "                )\n",
        "                if \"model_1_id\" in conv and \"model_2_id\" in conv:\n",
        "                    preference_record[\"model_0_id\"] = conv[\"model_1_id\"]\n",
        "                    preference_record[\"model_1_id\"] = conv[\"model_2_id\"]\n",
        "                if is_aspect:\n",
        "                    prefix_A = \"%d. Response A - \" % round_id\n",
        "                    prefix_B = \"%d. Response B - \" % round_id\n",
        "                    for aspect_key in SurgeAspectMap:\n",
        "                        if prefix_A + aspect_key in record:\n",
        "                            rating_map = SurgeAspectMap[aspect_key].ratings\n",
        "                            rating_range = SurgeAspectMap[aspect_key].range\n",
        "                            key_A = record[prefix_A + aspect_key]\n",
        "                            key_B = record[prefix_B + aspect_key]\n",
        "                            score_A = common.fuzzy_query_map(rating_map, key_A)\n",
        "                            score_B = common.fuzzy_query_map(rating_map, key_B)\n",
        "\n",
        "                            if score_A and score_B:\n",
        "                                pref = (score_B - score_A) / rating_range / 2 + 0.5\n",
        "\n",
        "                                if is_hard:\n",
        "                                    pref = common.round_preference_exclude_tie(pref > 0.5)\n",
        "                                preference_record[\n",
        "                                    \"preference_\" + SurgeAspectMap[aspect_key].name\n",
        "                                ] = pref\n",
        "                                preference_record[aspect_key+'_rating_A'] = int(score_A + 3)\n",
        "                                preference_record[aspect_key+'_rating_B'] = int(score_B + 3)\n",
        "                            else:\n",
        "                                local_counter[\"edit:no_annotation_for_\" + aspect_key] += 1\n",
        "\n",
        "                preference_record[\"preference_overall\"] = preference\n",
        "                critic = preference_record[\"critic\"]\n",
        "\n",
        "                # TODO(@yuchen): CONSTRUCT AN SFT TRAINING MESSAGE\n",
        "                # conversation for SFT history + assistant_message1 + user_rubrics + assistant_r1\n",
        "                conversation = [x.update({'role': x['role'].replace('user', 'HUMAN').replace('assistant', 'ASSIS')}) or x for x in history]\n",
        "                conv_loss_mask = list([bool(False) for _ in history])\n",
        "\n",
        "                conversation, conv_loss_mask, rubrics = self.sft_data_generator.create_contextual_messages(example=preference_record)\n",
        "\n",
        "                preference_record['conversation'] = list(conversation)\n",
        "                preference_record[\"conv_loss_mask\"] = list(conv_loss_mask)\n",
        "                all_roles = [int(m[\"role\"] in 'HUMAN') for m in conversation]\n",
        "                any_same = any([r1 == r0 for r1, r0 in zip(all_roles[1:], all_roles[:-1])])\n",
        "                if any_same:\n",
        "                  # print([x['role'] for x in conversation])\n",
        "                  # print(json.dumps(history, indent=2))\n",
        "                  bad_conversations.append(history)\n",
        "                  continue\n",
        "\n",
        "                # END OF CONSTRUCT AN SFT TRAINING MESSAGE\n",
        "\n",
        "                local_preference_records.append(preference_record)\n",
        "                local_counter[\"preference\"] += 1\n",
        "                if edit_tuple[\"content\"] is not None:\n",
        "                    # Include <edit, orig> pair\n",
        "                    edit_label = chr((ord(\"A\") + edit_tuple[\"edit_id\"]))\n",
        "                    edit_preference_record = dict(\n",
        "                        id=f\"{task_id}:{round_id}:rewrite\",\n",
        "                        conversation0=history\n",
        "                        + [dict(role=\"assistant\", content=edit_tuple[\"content\"])],\n",
        "                        conversation1=preference_record[f\"conversation{i}\"],\n",
        "                        preference_overall=0,\n",
        "                        domain=domain,\n",
        "                        critic=preference_record['critic'],\n",
        "                        conversation=preference_record[\"conversation\"],\n",
        "                        conv_loss_mask=preference_record['conv_loss_mask'],\n",
        "                    )\n",
        "                    if f\"model_{i}_id\" in preference_record:\n",
        "                        edit_preference_record[\"model_0_id\"] = \"edit\"\n",
        "                        edit_preference_record[\"model_1_id\"] = preference_record[f\"model_{i}_id\"]\n",
        "                    if is_aspect:\n",
        "                        prefix_ref = \"%d. Response %s - \" % (round_id, edit_label)\n",
        "                        for aspect_key in SurgeAspectMap:\n",
        "                            if prefix_ref + aspect_key in record:\n",
        "                                rating_map = SurgeAspectMap[aspect_key].ratings\n",
        "                                rating_range = SurgeAspectMap[aspect_key].range\n",
        "                                max_rating = max(rating_map.values())\n",
        "                                key_ref = record[prefix_ref + aspect_key]\n",
        "                                score_ref = common.fuzzy_query_map(rating_map, key_ref)\n",
        "                                if score_ref:\n",
        "                                    edit_preference_record[\n",
        "                                        \"preference_\" + SurgeAspectMap[aspect_key].name\n",
        "                                    ] = (score_ref - max_rating) / rating_range / 2 + 0.5\n",
        "                    local_preference_records.append(edit_preference_record)\n",
        "                    local_counter[\"rewrite_preference\"] += 1\n",
        "                # Update history.\n",
        "                history.append(preferred_assistant_message)\n",
        "        # if len(bad_conversations) > 1:\n",
        "        #   print(\"all \", len(history), \"bad\", len(bad_conversations))\n",
        "        #   print(json.dumps(bad_conversations, indent=2))\n",
        "        return local_preference_records, local_counter\n",
        "\n",
        "    def raw(self, regenerate_if_found=False):\n",
        "        super().raw()\n",
        "\n",
        "        preference_records = []\n",
        "        counter = collections.Counter()\n",
        "        if \"*\" in self.path_glob:\n",
        "            paths = glob.glob(self.path_glob)\n",
        "        else:\n",
        "            paths = [self.path_glob]\n",
        "        print(\"paths\", paths)\n",
        "        df = utils.read_df(paths)\n",
        "        for record in utils.df_to_records(df):\n",
        "            try:\n",
        "                local_preference_records, local_counter = self.create_preference_records(\n",
        "                    record,\n",
        "                    is_hard=self.is_hard,\n",
        "                    is_confident=self.is_confident,\n",
        "                    is_aspect=self.is_aspect,\n",
        "                )\n",
        "                preference_records.extend(local_preference_records)\n",
        "                counter.update(local_counter)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "        print(\"Data stats\")\n",
        "        print(\"-\" * 80)\n",
        "        for key, cnt in sorted(counter.items()):\n",
        "            print(f\"{key:>50s} {cnt:>12,}\")\n",
        "\n",
        "        self.name = self.name.replace(\"surge_\", \"renew\" + (\"-critic\" if self.rubrics_type == 1 else \"\")  + (\"-synthetic\" if self.rubrics_type == 2 else \"\")  + \"/surge_\" )\n",
        "        output_path = RAW_PATH / self.name\n",
        "        df = utils.df_from_records(preference_records)\n",
        "        # print(\"type\", df['conversation'].apply(type).unique())\n",
        "        # print(\"type\", df['conv_loss_mask'].apply(type).unique())\n",
        "        # float_indices = df[df['conversation'].apply(lambda x: isinstance(x, float))].index\n",
        "        # print(df.loc[float_indices])\n",
        "        print(\"Output\" , output_path, df.shape)\n",
        "        if df.shape[0] > 0:\n",
        "          common.delete_and_save(output_path, df, regenerate_if_found=True)\n",
        "        return self"
      ],
      "metadata": {
        "id": "pmkJtbj4YQ5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append(os.path.expanduser('~/xai/datasets/spark/preference/'))\n",
        "# from scratch.surge import Surge\n",
        "\n",
        "file_name = 'surge-general_1'\n",
        "file_path = 'RLHF - General - Batch 1.json'  # RLHF - General - Batch 1.json\n",
        "# file_path = '[x-8] RLHF - System Prompts - Batch 4.json' # paths ['/data/datasets/preferences-v2/surge/surge_9/[x-3] RLHF - General - Batch 9.json']\n",
        "\n",
        "\n",
        "surge = Surge(\n",
        "    name=file_name,\n",
        "    path_glob=f\"/data/datasets/preferences-v2/surge/{file_path}\",\n",
        "    is_confident=False,\n",
        "    is_hard=False,\n",
        "    is_aspect=True,\n",
        ")\n",
        "sdata = surge.raw(regenerate_if_found=True)\n",
        "sdata = sdata.filtered()\n",
        "\n",
        "\n",
        "# output_path = RAW_PATH / sdata.name\n",
        "# data = pd.read_parquet(output_path / \"part-00000-of-00001.parquet\")\n",
        "# data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIEB1s0GbyPE",
        "outputId": "78e23719-320d-47a0-fe8c-3a9ce31297a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-10-26 01:18:28,382 I] \u001b[2;36m[colabbox-0:401596] common:31:\u001b[0m Preparing raw for self.name='surge-general_1'\n",
            "paths ['/data/datasets/preferences-v2/surge/RLHF - General - Batch 1.json']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 46.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-10-26 01:18:28,609 E] \u001b[2;36m[colabbox-0:401596] sampling_client:993:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/json/decoder.py:353: RuntimeWarning: coroutine 'Surge.raw' was never awaited\n",
            "  obj, end = self.scan_once(s, idx)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "cannot unpack non-iterable coroutine object\n",
            "--------------------------------------------------------------------------------\n",
            "Data stats\n",
            "--------------------------------------------------------------------------------\n",
            "                         filter:one_empty_response            1\n",
            "Output /data/datasets/preferences-v2/raw/surge-general_1 (0, 0)\n",
            "\u001b[2m[2024-10-26 01:18:30,178 I] \u001b[2;36m[colabbox-0:401596] common:42:\u001b[0m Preparing filtered for self.name='surge-general_1'\n",
            "\u001b[2m[2024-10-26 01:18:30,179 W] \u001b[2;36m[colabbox-0:401596] filter_utils:326:\u001b[0m self.output_path='/data/datasets/preferences-v2/filtered/surge-general_1' exists. Running filtering will remove and regenerate it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_401596/260927136.py:188: RuntimeWarning: coroutine 'ContextualRewardGenerator.create_contextual_messages' was never awaited\n",
            "  conversation, conv_loss_mask, rubrics = self.sft_data_generator.create_contextual_messages(example=preference_record)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "[Subsets]:   0%|                                                                      | 0/4 [00:00<?, ?it/s]\n",
            "[2/4]:   0%|                                                                         | 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "[0/4]:   0%|                                                                         | 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "[3/4]:   0%|                                                                         | 0/65 [00:00<?, ?it/s]\u001b[A\n",
            "[1/4]:   0%|                                                                         | 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "[0/4]:  68%|██████████████████████████████████████████▉                    | 45/66 [00:00<00:00, 444.26it/s]\u001b[A\n",
            "[3/4]:  55%|██████████████████████████████████▉                            | 36/65 [00:00<00:00, 351.27it/s]\u001b[A\n",
            "[1/4]:  62%|███████████████████████████████████████▏                       | 41/66 [00:00<00:00, 407.96it/s]\u001b[A\n",
            "[2/4]:  62%|███████████████████████████████████████▏                       | 41/66 [00:00<00:00, 367.31it/s]\u001b[A\n",
            "                                                                                                            \u001b[A\n",
            "                                                                                                            \u001b[A\n",
            "                                                                                                            \u001b[A\n",
            "[Subsets]: 100%|██████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 15.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-10-26 01:18:30,888 I] \u001b[2;36m[colabbox-0:401596] filter_utils:360:\u001b[0m Saved data to /data/datasets/preferences-v2/filtered/surge-general_1\n",
            "name                                                                                                   count\n",
            "---------------------------------------------------------------------------------------------------  -------\n",
            "#byte                                                                                                384,080\n",
            "#conv                                                                                                    241\n",
            "#data                                                                                                    241\n",
            "#deletes                                                                                                  22\n",
            "#original                                                                                                263\n",
            "#turn                                                                                                    308\n",
            "#updates                                                                                                   0\n",
            "[edit] assistant | re.sub((^Of course! |^Certainly! |^Sure! |^Absolutely! ), , ...)                       24\n",
            "[remove_all] assistant | re.search((claude|anthropic), ...)                                                2\n",
            "[remove_all] assistant | re.search((crucial|important|essential) to (note|remember), ...)                  1\n",
            "[remove_all] assistant | re.search(\\b(2018|2019|2020|2021)\\b, ...)                                         2\n",
            "[remove_all] assistant | re.search(\\b(COVID-19|COVID19|pandemic|coronavirus)\\b, ...)                       3\n",
            "[remove_all] assistant | re.search(\\bI(\\b|\\b[^\\n,:\\.!?]{1,20}\\b)(do not|don\\'t|cannot|can\\'t), ...)        3\n",
            "[remove_all] assistant | re.search(\\bas of\\b, ...)                                                         1\n",
            "[remove_all] assistant | re.search(joke, ...)                                                              3\n",
            "[remove_all] user | re.search(\\b(2018|2019|2020|2021)\\b, ...)                                              7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generage_surge_1t5():\n",
        "    # TODO(lxuechen): This part of the code is messed up and should be rewritten.\n",
        "    datasets = [\n",
        "        # general\n",
        "        Surge(\n",
        "            name=\"surge_general_1_confident\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - General - Batch 1.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_general_2_confident\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - General - Batch 2.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_general_3_confident\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - General - Batch 3.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_general_4_confident\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - General - Batch 4.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        # math\n",
        "        Surge(\n",
        "            name=\"surge_math_2_confident\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Math - Batch 2.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_math_3_confident\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Math - Batch 3.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_math_4_confident\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Math - Batch 4.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        # coding\n",
        "        Surge(\n",
        "            name=\"surge_coding_2_confident\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Coding - Batch 2.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_coding_3_confident\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Coding - Batch 3.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_coding_4_confident\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Coding - Batch 4.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        # general\n",
        "        Surge(\n",
        "            name=\"surge_general_1\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - General - Batch 1.json\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_general_2\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - General - Batch 2.json\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_general_3\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - General - Batch 3.json\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_general_4\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - General - Batch 4.json\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        # math\n",
        "        Surge(\n",
        "            name=\"surge_math_2\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Math - Batch 2.json\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_math_3\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Math - Batch 3.json\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_math_4\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Math - Batch 4.json\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        # coding\n",
        "        Surge(\n",
        "            name=\"surge_coding_2\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Coding - Batch 2.json\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_coding_3\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Coding - Batch 3.json\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_coding_4\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Coding - Batch 4.json\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    datasets += [\n",
        "        Surge(\n",
        "            name=\"surge_general_5\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - General - Batch 5.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_math_5\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Math - Batch 5.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_coding_5\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Coding - Batch 5.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_general_6\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - General - Batch 6.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_math_6\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Math - Batch 6.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "        Surge(\n",
        "            name=\"surge_coding_6\",\n",
        "            path_glob=\"/data/datasets/preferences-v2/surge/RLHF - Coding - Batch 6.json\",\n",
        "            is_confident=True,\n",
        "            is_hard=False,\n",
        "            is_aspect=False,\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    for dataset in datasets:\n",
        "        dataset.filtered()\n",
        "\n",
        "\n",
        "def generate_surge_1t6():\n",
        "    file_names = {\n",
        "        \"surge_general_6\": \"RLHF - General - Batch 6.json\",\n",
        "        \"surge_general_5\": \"RLHF - General - Batch 5.json\",\n",
        "        \"surge_general_4\": \"RLHF - General - Batch 4.json\",\n",
        "        \"surge_general_3\": \"RLHF - General - Batch 3.json\",\n",
        "        \"surge_general_2\": \"RLHF - General - Batch 2.json\",\n",
        "        \"surge_general_1\": \"RLHF - General - Batch 1.json\",\n",
        "        \"surge_coding_6\": \"RLHF - Coding - Batch 6.json\",\n",
        "        \"surge_coding_5\": \"RLHF - Coding - Batch 5.json\",\n",
        "        \"surge_coding_4\": \"RLHF - Coding - Batch 4.json\",\n",
        "        \"surge_coding_3\": \"RLHF - Coding - Batch 3.json\",\n",
        "        \"surge_coding_2\": \"RLHF - Coding - Batch 2.json\",\n",
        "        \"surge_math_6\": \"RLHF - Math - Batch 6.json\",\n",
        "        \"surge_math_5\": \"RLHF - Math - Batch 5.json\",\n",
        "        \"surge_math_4\": \"RLHF - Math - Batch 4.json\",\n",
        "        \"surge_math_3\": \"RLHF - Math - Batch 3.json\",\n",
        "        \"surge_math_2\": \"RLHF - Math - Batch 2.json\",\n",
        "    }\n",
        "    for file_name, file_path in file_names.items():\n",
        "        # Fine-grain (non-hard, aspect) datasets\n",
        "        Surge(\n",
        "            name=file_name.replace(\"surge_\", \"surge_finegrain_\"),\n",
        "            path_glob=f\"/data/datasets/preferences-v2/surge/{file_path}\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=True,\n",
        "        ).raw(regenerate_if_found=True).filtered()\n",
        "\n",
        "\n",
        "def generate_surge_7():\n",
        "    file_names = {\n",
        "        \"surge_general_7\": \"[x-3] RLHF - General - Batch 7.json\",\n",
        "        \"surge_coding_7\": \"[x-4] RLHF - Coding - Batch 7.json\",\n",
        "        \"surge_math_7\": \"[x-5] RLHF - Math - Batch 7.json\",\n",
        "        \"surge_system_prompt_1\": \"[x-8] RLHF - System Prompts - Batch 1.json\",\n",
        "        \"surge_hyperspecific_1\": \"[x-10] RLHF - Hyperspecific - Batch 1.json\",\n",
        "        \"surge_api_1\": \"[x-9] RLHF - Enterprise API - Batch 1.json\",\n",
        "    }\n",
        "\n",
        "    for file_name, file_path in file_names.items():\n",
        "        # Hard and non-aspect datasets\n",
        "        Surge(\n",
        "            name=file_name,\n",
        "            path_glob=f\"/data/datasets/preferences-v2/surge/new_surge/{file_path}\",\n",
        "            is_confident=False,\n",
        "            is_hard=True,\n",
        "            is_aspect=False,\n",
        "        ).raw().filtered()\n",
        "        # Fine-grain (non-hard, aspect) datasets\n",
        "        Surge(\n",
        "            name=file_name.replace(\"surge_\", \"surge_finegrain_\"),\n",
        "            path_glob=f\"/data/datasets/preferences-v2/surge/new_surge/{file_path}\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=True,\n",
        "        ).raw(regenerate_if_found=True).filtered()\n",
        "\n",
        "\n",
        "def generate_surge_8():\n",
        "    file_names = {\n",
        "        \"surge_general_8\": \"[x-3] RLHF - General - Batch 8.json\",\n",
        "        \"surge_coding_8\": \"[x-4] RLHF - Coding - Batch 8.json\",\n",
        "        \"surge_math_8\": \"[x-5] RLHF - Math - Batch 8.json\",\n",
        "        \"surge_system_prompt_2\": \"[x-8] RLHF - System Prompts - Batch 2.json\",\n",
        "        \"surge_hyperspecific_2\": \"[x-10] RLHF - Hyperspecific - Batch 2.json\",\n",
        "        \"surge_api_2\": \"[x-9] RLHF - Enterprise API - Batch 2.json\",\n",
        "    }\n",
        "    for file_name, file_path in file_names.items():\n",
        "        # Hard and non-aspect datasets\n",
        "        Surge(\n",
        "            name=file_name,\n",
        "            path_glob=f\"/data/datasets/preferences-v2/surge/{file_path}\",\n",
        "            is_confident=False,\n",
        "            is_hard=True,\n",
        "            is_aspect=False,\n",
        "        ).raw().filtered()\n",
        "        # Fine-grain (non-hard, aspect) datasets\n",
        "        Surge(\n",
        "            name=file_name.replace(\"surge_\", \"surge_finegrain_\"),\n",
        "            path_glob=f\"/data/datasets/preferences-v2/surge/{file_path}\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=True,\n",
        "        ).raw(regenerate_if_found=True).filtered()\n",
        "\n",
        "\n",
        "def generate_surge_9_10():\n",
        "    file_names = {\n",
        "        \"surge_general_9\": \"surge_9/[x-3] RLHF - General - Batch 9.json\",\n",
        "        \"surge_coding_9\": \"surge_9/[x-4] RLHF - Coding - Batch 9.json\",\n",
        "        \"surge_math_9\": \"surge_9/[x-5] RLHF - Math - Batch 9.json\",\n",
        "        \"surge_system_prompt_3\": \"surge_9/[x-8] RLHF - System Prompts - Batch 3.json\",\n",
        "        \"surge_hyperspecific_3\": \"surge_9/[x-10] RLHF - Hyperspecific - Batch 3.json\",\n",
        "        \"surge_api_3\": \"surge_9/[x-9] RLHF - Enterprise API - Batch 3.json\",\n",
        "        \"surge_general_10\": \"surge_10/[x-3] RLHF - General - Batch 10.json\",\n",
        "        \"surge_coding_10\": \"surge_10/[x-4] RLHF - Coding - Batch 10.json\",\n",
        "        \"surge_math_10\": \"surge_10/[x-5] RLHF - Math - Batch 10.json\",\n",
        "        \"surge_system_prompt_4\": \"surge_10/[x-8] RLHF - System Prompts - Batch 4.json\",\n",
        "        \"surge_hyperspecific_4\": \"surge_10/[x-10] RLHF - Hyperspecific - Batch 4.json\",\n",
        "        \"surge_api_4\": \"surge_10/[x-9] RLHF - Enterprise API - Batch 4.json\",\n",
        "    }\n",
        "    for file_name, file_path in file_names.items():\n",
        "        # Hard and non-aspect datasets\n",
        "        Surge(\n",
        "            name=file_name,\n",
        "            path_glob=f\"/data/datasets/preferences-v2/surge/{file_path}\",\n",
        "            is_confident=False,\n",
        "            is_hard=True,\n",
        "            is_aspect=False,\n",
        "        ).raw().filtered()\n",
        "        # Fine-grain (non-hard, aspect) datasets\n",
        "        Surge(\n",
        "            name=file_name.replace(\"surge_\", \"surge_finegrain_\"),\n",
        "            path_glob=f\"/data/datasets/preferences-v2/surge/{file_path}\",\n",
        "            is_confident=False,\n",
        "            is_hard=False,\n",
        "            is_aspect=True,\n",
        "        ).raw(regenerate_if_found=True).filtered()\n",
        "\n",
        "\n",
        "generate_surge_1t6()\n",
        "generate_surge_7()\n",
        "generate_surge_8()\n",
        "generate_surge_9_10()"
      ],
      "metadata": {
        "id": "MO-esfoCysgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50fa4e4-3db5-46c4-9fc4-3a3ccf32d970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'coroutine' object has no attribute 'filtered'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[210], line 322\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# Fine-grain (non-hard, aspect) datasets\u001b[39;00m\n\u001b[1;32m    313\u001b[0m         Surge(\n\u001b[1;32m    314\u001b[0m             name\u001b[38;5;241m=\u001b[39mfile_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurge_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurge_finegrain_\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    315\u001b[0m             path_glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/datasets/preferences-v2/surge/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m             is_aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    319\u001b[0m         )\u001b[38;5;241m.\u001b[39mraw(regenerate_if_found\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfiltered()\n\u001b[0;32m--> 322\u001b[0m \u001b[43mgenerate_surge_1t6\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m generate_surge_7()\n\u001b[1;32m    324\u001b[0m generate_surge_8()\n",
            "Cell \u001b[0;32mIn[210], line 228\u001b[0m, in \u001b[0;36mgenerate_surge_1t6\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m file_names \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurge_general_6\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRLHF - General - Batch 6.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurge_general_5\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRLHF - General - Batch 5.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurge_math_2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRLHF - Math - Batch 2.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m }\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name, file_path \u001b[38;5;129;01min\u001b[39;00m file_names\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Fine-grain (non-hard, aspect) datasets\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[43mSurge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msurge_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msurge_finegrain_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_glob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/datasets/preferences-v2/surge/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_confident\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_hard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_aspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregenerate_if_found\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiltered\u001b[49m()\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'coroutine' object has no attribute 'filtered'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/data/datasets/sft-data/staging/shuffled_102324/v4/sft-surge1t10renew-102324-base-8k-maxpack/train/*.parquet\"\n",
        "\n",
        "df_data = utils.read_df(glob.glob(path))\n",
        "df_data.count()"
      ],
      "metadata": {
        "id": "VM5SXyycZ17D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preference_data_path = \"/data/datasets/preferences-v2/filtered/renew/*/data/*.parquet\"\n",
        "preference_data  = utils.read_df(glob.glob(preference_data_path))\n",
        "preference_data.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-heAhFBpTPBy",
        "outputId": "e6d4b901-80e6-4994-f560-b884070e87a8"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|███████████████████████████████████████████| 256/256 [00:06<00:00, 41.33it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                                  76408\n",
              "domain                              76408\n",
              "critic                              76408\n",
              "conversation                        76408\n",
              "conv_loss_mask                      76408\n",
              "conversation0                       76408\n",
              "conversation1                       76408\n",
              "preference_overall                  76408\n",
              "model_0_id                          76408\n",
              "model_1_id                          76408\n",
              "preference_correctness              76408\n",
              "truthfulness_rating_A               71632\n",
              "truthfulness_rating_B               71632\n",
              "preference_quality                  76408\n",
              "overall quality_rating_A             1105\n",
              "overall quality_rating_B             1105\n",
              "preference_writing                  71632\n",
              "writing style_rating_A              71632\n",
              "writing style_rating_B              71632\n",
              "preference_instruction-following    76408\n",
              "instruction following_rating_A      76408\n",
              "instruction following_rating_B      76408\n",
              "data0                               76408\n",
              "data1                               76408\n",
              "prompt                              76408\n",
              "preference_verbosity                44707\n",
              "verbosity_rating_A                  44707\n",
              "verbosity_rating_B                  44707\n",
              "quality_rating_A                    75303\n",
              "quality_rating_B                    75303\n",
              "correctness_rating_A                 4776\n",
              "correctness_rating_B                 4776\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_path = \"/data/datasets/rubrics/mixtures/policy/mix-rubrics-8k/*.parquet\"\n",
        "policy_data = utils.read_df(glob.glob(policy_path))\n",
        "policy_data.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40RvcAu2TLJl",
        "outputId": "34de7b4a-eb8b-4b66-b5db-c95198db18bc"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "prompt               32306\n",
              "source               32306\n",
              "source_filename      32306\n",
              "category             32306\n",
              "is_rubric_all_yes    32306\n",
              "n_rubric             32306\n",
              "reward_type          32306\n",
              "rubric_list          32306\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preference_data['source'] = 'preference'\n",
        "preference_data['category'] = preference_data['domain'].apply(lambda x: [x])\n",
        "preference_data['reward_type'] = 'contextual'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z_6uK7lZUZsj",
        "outputId": "6016213f-dff0-43bb-e098-f5717aeceaa9"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   id    domain  \\\n",
              "0      85106474-b11c-48ad-8f08-d77f020abdbc:1:rewrite      math   \n",
              "1              a06e5f01-8c81-43a6-896a-ff643a71c69e:1      math   \n",
              "2              23476bcd-db91-4b55-b696-732542ba36bb:1      math   \n",
              "3              c8b99854-491f-4df1-8197-10647c423b9d:1      math   \n",
              "4              d268014d-eb5c-4a0e-a813-1aa2476ea7e4:1      math   \n",
              "...                                               ...       ...   \n",
              "76403          baef49e9-5f4e-46ab-a793-ad7ca1aef67d:1   Writing   \n",
              "76404          05cdacd4-60d9-4e63-a290-96c7ac920016:1    Advice   \n",
              "76405          0f2f6cf6-6fe0-4838-8512-76fab04922fd:1  Open Q&A   \n",
              "76406          fddce444-20a6-4ab2-93d0-1ec9cbc72741:3  Open Q&A   \n",
              "76407          4116dccd-3004-449b-9607-309e25ec675f:2   Writing   \n",
              "\n",
              "                                                  critic  \\\n",
              "0      Model B is better than Model A because it gave...   \n",
              "1      Model B is much better than model A due to for...   \n",
              "2      The prompt requests the solution for a given c...   \n",
              "3      Both Model A and Model B contain very minor wr...   \n",
              "4      Model B used much better formatting than Model...   \n",
              "...                                                  ...   \n",
              "76403  I rated A as better because I asked for a stor...   \n",
              "76404  Model B is the better response between the two...   \n",
              "76405  I chose based off how Model B categorized the ...   \n",
              "76406  Response A was too brief to be very helpful. M...   \n",
              "76407  Both of these responses had issues. Model A ke...   \n",
              "\n",
              "                                            conversation  \\\n",
              "0      [{'content': 'There's a shuffled standard 52 c...   \n",
              "1      [{'content': 'Jake is building a house thats c...   \n",
              "2      [{'content': 'Solve the following compound ine...   \n",
              "3      [{'content': 'Consider the English alphabet, w...   \n",
              "4      [{'content': 'Can you find the laplace transfo...   \n",
              "...                                                  ...   \n",
              "76403  [{'content': 'I want to create a short animati...   \n",
              "76404  [{'content': 'Give me step-by-step instruction...   \n",
              "76405  [{'content': 'I am interested in writing a blo...   \n",
              "76406  [{'content': 'Write me a synopsis of the minis...   \n",
              "76407  [{'content': 'Explain the process of folding a...   \n",
              "\n",
              "                                          conv_loss_mask  \\\n",
              "0      [False, False, False, True, False, False, Fals...   \n",
              "1      [False, False, False, True, False, False, Fals...   \n",
              "2      [False, False, False, True, False, False, Fals...   \n",
              "3      [False, False, False, True, False, False, Fals...   \n",
              "4      [False, False, False, True, False, False, Fals...   \n",
              "...                                                  ...   \n",
              "76403  [False, False, False, True, False, False, Fals...   \n",
              "76404  [False, False, False, True, False, False, Fals...   \n",
              "76405  [False, False, False, True, False, False, Fals...   \n",
              "76406  [False, False, False, False, False, False, Fal...   \n",
              "76407  [False, False, False, False, False, True, Fals...   \n",
              "\n",
              "                                           conversation0  \\\n",
              "0      [{'content': 'There's a shuffled standard 52 c...   \n",
              "1      [{'content': 'Jake is building a house thats c...   \n",
              "2      [{'content': 'Solve the following compound ine...   \n",
              "3      [{'content': 'Consider the English alphabet, w...   \n",
              "4      [{'content': 'Can you find the laplace transfo...   \n",
              "...                                                  ...   \n",
              "76403  [{'content': 'I want to create a short animati...   \n",
              "76404  [{'content': 'Give me step-by-step instruction...   \n",
              "76405  [{'content': 'I am interested in writing a blo...   \n",
              "76406  [{'content': 'Write me a synopsis of the minis...   \n",
              "76407  [{'content': 'Explain the process of folding a...   \n",
              "\n",
              "                                           conversation1  preference_overall  \\\n",
              "0      [{'content': 'There's a shuffled standard 52 c...                0.00   \n",
              "1      [{'content': 'Jake is building a house thats c...                0.84   \n",
              "2      [{'content': 'Solve the following compound ine...                0.67   \n",
              "3      [{'content': 'Consider the English alphabet, w...                0.84   \n",
              "4      [{'content': 'Can you find the laplace transfo...                0.84   \n",
              "...                                                  ...                 ...   \n",
              "76403  [{'content': 'I want to create a short animati...                0.17   \n",
              "76404  [{'content': 'Give me step-by-step instruction...                0.84   \n",
              "76405  [{'content': 'I am interested in writing a blo...                0.84   \n",
              "76406  [{'content': 'Write me a synopsis of the minis...                0.84   \n",
              "76407  [{'content': 'Explain the process of folding a...                0.51   \n",
              "\n",
              "                        model_0_id                   model_1_id  ...  \\\n",
              "0                             edit  grok-0805-1.research-models  ...   \n",
              "1      grok-0701-2.research-models  grok-0805-1.research-models  ...   \n",
              "2      grok-0701-2.research-models  grok-0805-1.research-models  ...   \n",
              "3      grok-0701-2.research-models  grok-0805-1.research-models  ...   \n",
              "4      grok-0701-2.research-models  grok-0805-1.research-models  ...   \n",
              "...                            ...                          ...  ...   \n",
              "76403    grok-0626.research-models    grok-0701.research-models  ...   \n",
              "76404    grok-0626.research-models    grok-0701.research-models  ...   \n",
              "76405    grok-0626.research-models    grok-0701.research-models  ...   \n",
              "76406    grok-0626.research-models    grok-0701.research-models  ...   \n",
              "76407    grok-0626.research-models    grok-0701.research-models  ...   \n",
              "\n",
              "       preference_verbosity  verbosity_rating_A  verbosity_rating_B  \\\n",
              "0                       NaN                 NaN                 NaN   \n",
              "1                       NaN                 NaN                 NaN   \n",
              "2                       NaN                 NaN                 NaN   \n",
              "3                       NaN                 NaN                 NaN   \n",
              "4                       NaN                 NaN                 NaN   \n",
              "...                     ...                 ...                 ...   \n",
              "76403              0.500000                 4.0                 4.0   \n",
              "76404              0.833333                 2.0                 4.0   \n",
              "76405              1.000000                 1.0                 4.0   \n",
              "76406              0.833333                 2.0                 4.0   \n",
              "76407              0.000000                 4.0                 1.0   \n",
              "\n",
              "       quality_rating_A  quality_rating_B  correctness_rating_A  \\\n",
              "0                   NaN               NaN                   NaN   \n",
              "1                   NaN               NaN                   NaN   \n",
              "2                   NaN               NaN                   NaN   \n",
              "3                   NaN               NaN                   NaN   \n",
              "4                   NaN               NaN                   NaN   \n",
              "...                 ...               ...                   ...   \n",
              "76403            -100.0            -100.0                   NaN   \n",
              "76404            -100.0            -100.0                   NaN   \n",
              "76405            -100.0            -100.0                   NaN   \n",
              "76406            -100.0            -100.0                   NaN   \n",
              "76407            -100.0            -100.0                   NaN   \n",
              "\n",
              "       correctness_rating_B      source    category  reward_type  \n",
              "0                       NaN  preference      [math]   contextual  \n",
              "1                       NaN  preference      [math]   contextual  \n",
              "2                       NaN  preference      [math]   contextual  \n",
              "3                       NaN  preference      [math]   contextual  \n",
              "4                       NaN  preference      [math]   contextual  \n",
              "...                     ...         ...         ...          ...  \n",
              "76403                   NaN  preference   [Writing]   contextual  \n",
              "76404                   NaN  preference    [Advice]   contextual  \n",
              "76405                   NaN  preference  [Open Q&A]   contextual  \n",
              "76406                   NaN  preference  [Open Q&A]   contextual  \n",
              "76407                   NaN  preference   [Writing]   contextual  \n",
              "\n",
              "[76408 rows x 35 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>domain</th>\n",
              "      <th>critic</th>\n",
              "      <th>conversation</th>\n",
              "      <th>conv_loss_mask</th>\n",
              "      <th>conversation0</th>\n",
              "      <th>conversation1</th>\n",
              "      <th>preference_overall</th>\n",
              "      <th>model_0_id</th>\n",
              "      <th>model_1_id</th>\n",
              "      <th>...</th>\n",
              "      <th>preference_verbosity</th>\n",
              "      <th>verbosity_rating_A</th>\n",
              "      <th>verbosity_rating_B</th>\n",
              "      <th>quality_rating_A</th>\n",
              "      <th>quality_rating_B</th>\n",
              "      <th>correctness_rating_A</th>\n",
              "      <th>correctness_rating_B</th>\n",
              "      <th>source</th>\n",
              "      <th>category</th>\n",
              "      <th>reward_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>85106474-b11c-48ad-8f08-d77f020abdbc:1:rewrite</td>\n",
              "      <td>math</td>\n",
              "      <td>Model B is better than Model A because it gave...</td>\n",
              "      <td>[{'content': 'There's a shuffled standard 52 c...</td>\n",
              "      <td>[False, False, False, True, False, False, Fals...</td>\n",
              "      <td>[{'content': 'There's a shuffled standard 52 c...</td>\n",
              "      <td>[{'content': 'There's a shuffled standard 52 c...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>edit</td>\n",
              "      <td>grok-0805-1.research-models</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>preference</td>\n",
              "      <td>[math]</td>\n",
              "      <td>contextual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a06e5f01-8c81-43a6-896a-ff643a71c69e:1</td>\n",
              "      <td>math</td>\n",
              "      <td>Model B is much better than model A due to for...</td>\n",
              "      <td>[{'content': 'Jake is building a house thats c...</td>\n",
              "      <td>[False, False, False, True, False, False, Fals...</td>\n",
              "      <td>[{'content': 'Jake is building a house thats c...</td>\n",
              "      <td>[{'content': 'Jake is building a house thats c...</td>\n",
              "      <td>0.84</td>\n",
              "      <td>grok-0701-2.research-models</td>\n",
              "      <td>grok-0805-1.research-models</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>preference</td>\n",
              "      <td>[math]</td>\n",
              "      <td>contextual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23476bcd-db91-4b55-b696-732542ba36bb:1</td>\n",
              "      <td>math</td>\n",
              "      <td>The prompt requests the solution for a given c...</td>\n",
              "      <td>[{'content': 'Solve the following compound ine...</td>\n",
              "      <td>[False, False, False, True, False, False, Fals...</td>\n",
              "      <td>[{'content': 'Solve the following compound ine...</td>\n",
              "      <td>[{'content': 'Solve the following compound ine...</td>\n",
              "      <td>0.67</td>\n",
              "      <td>grok-0701-2.research-models</td>\n",
              "      <td>grok-0805-1.research-models</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>preference</td>\n",
              "      <td>[math]</td>\n",
              "      <td>contextual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>c8b99854-491f-4df1-8197-10647c423b9d:1</td>\n",
              "      <td>math</td>\n",
              "      <td>Both Model A and Model B contain very minor wr...</td>\n",
              "      <td>[{'content': 'Consider the English alphabet, w...</td>\n",
              "      <td>[False, False, False, True, False, False, Fals...</td>\n",
              "      <td>[{'content': 'Consider the English alphabet, w...</td>\n",
              "      <td>[{'content': 'Consider the English alphabet, w...</td>\n",
              "      <td>0.84</td>\n",
              "      <td>grok-0701-2.research-models</td>\n",
              "      <td>grok-0805-1.research-models</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>preference</td>\n",
              "      <td>[math]</td>\n",
              "      <td>contextual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d268014d-eb5c-4a0e-a813-1aa2476ea7e4:1</td>\n",
              "      <td>math</td>\n",
              "      <td>Model B used much better formatting than Model...</td>\n",
              "      <td>[{'content': 'Can you find the laplace transfo...</td>\n",
              "      <td>[False, False, False, True, False, False, Fals...</td>\n",
              "      <td>[{'content': 'Can you find the laplace transfo...</td>\n",
              "      <td>[{'content': 'Can you find the laplace transfo...</td>\n",
              "      <td>0.84</td>\n",
              "      <td>grok-0701-2.research-models</td>\n",
              "      <td>grok-0805-1.research-models</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>preference</td>\n",
              "      <td>[math]</td>\n",
              "      <td>contextual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76403</th>\n",
              "      <td>baef49e9-5f4e-46ab-a793-ad7ca1aef67d:1</td>\n",
              "      <td>Writing</td>\n",
              "      <td>I rated A as better because I asked for a stor...</td>\n",
              "      <td>[{'content': 'I want to create a short animati...</td>\n",
              "      <td>[False, False, False, True, False, False, Fals...</td>\n",
              "      <td>[{'content': 'I want to create a short animati...</td>\n",
              "      <td>[{'content': 'I want to create a short animati...</td>\n",
              "      <td>0.17</td>\n",
              "      <td>grok-0626.research-models</td>\n",
              "      <td>grok-0701.research-models</td>\n",
              "      <td>...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>preference</td>\n",
              "      <td>[Writing]</td>\n",
              "      <td>contextual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76404</th>\n",
              "      <td>05cdacd4-60d9-4e63-a290-96c7ac920016:1</td>\n",
              "      <td>Advice</td>\n",
              "      <td>Model B is the better response between the two...</td>\n",
              "      <td>[{'content': 'Give me step-by-step instruction...</td>\n",
              "      <td>[False, False, False, True, False, False, Fals...</td>\n",
              "      <td>[{'content': 'Give me step-by-step instruction...</td>\n",
              "      <td>[{'content': 'Give me step-by-step instruction...</td>\n",
              "      <td>0.84</td>\n",
              "      <td>grok-0626.research-models</td>\n",
              "      <td>grok-0701.research-models</td>\n",
              "      <td>...</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>preference</td>\n",
              "      <td>[Advice]</td>\n",
              "      <td>contextual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76405</th>\n",
              "      <td>0f2f6cf6-6fe0-4838-8512-76fab04922fd:1</td>\n",
              "      <td>Open Q&amp;A</td>\n",
              "      <td>I chose based off how Model B categorized the ...</td>\n",
              "      <td>[{'content': 'I am interested in writing a blo...</td>\n",
              "      <td>[False, False, False, True, False, False, Fals...</td>\n",
              "      <td>[{'content': 'I am interested in writing a blo...</td>\n",
              "      <td>[{'content': 'I am interested in writing a blo...</td>\n",
              "      <td>0.84</td>\n",
              "      <td>grok-0626.research-models</td>\n",
              "      <td>grok-0701.research-models</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>preference</td>\n",
              "      <td>[Open Q&amp;A]</td>\n",
              "      <td>contextual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76406</th>\n",
              "      <td>fddce444-20a6-4ab2-93d0-1ec9cbc72741:3</td>\n",
              "      <td>Open Q&amp;A</td>\n",
              "      <td>Response A was too brief to be very helpful. M...</td>\n",
              "      <td>[{'content': 'Write me a synopsis of the minis...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[{'content': 'Write me a synopsis of the minis...</td>\n",
              "      <td>[{'content': 'Write me a synopsis of the minis...</td>\n",
              "      <td>0.84</td>\n",
              "      <td>grok-0626.research-models</td>\n",
              "      <td>grok-0701.research-models</td>\n",
              "      <td>...</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>preference</td>\n",
              "      <td>[Open Q&amp;A]</td>\n",
              "      <td>contextual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76407</th>\n",
              "      <td>4116dccd-3004-449b-9607-309e25ec675f:2</td>\n",
              "      <td>Writing</td>\n",
              "      <td>Both of these responses had issues. Model A ke...</td>\n",
              "      <td>[{'content': 'Explain the process of folding a...</td>\n",
              "      <td>[False, False, False, False, False, True, Fals...</td>\n",
              "      <td>[{'content': 'Explain the process of folding a...</td>\n",
              "      <td>[{'content': 'Explain the process of folding a...</td>\n",
              "      <td>0.51</td>\n",
              "      <td>grok-0626.research-models</td>\n",
              "      <td>grok-0701.research-models</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>preference</td>\n",
              "      <td>[Writing]</td>\n",
              "      <td>contextual</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76408 rows × 35 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.concat([preference_data, policy_data], axis=0, join='outer')\n",
        "new_df = new_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(len(new_df))\n",
        "new_df.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLzzEu6eVnkX",
        "outputId": "e2726313-c486-4260-8cbb-5e960ac386b0"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "108714\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                                   76408\n",
              "domain                               76408\n",
              "critic                               76408\n",
              "conversation                         76408\n",
              "conv_loss_mask                       76408\n",
              "conversation0                        76408\n",
              "conversation1                        76408\n",
              "preference_overall                   76408\n",
              "model_0_id                           76408\n",
              "model_1_id                           76408\n",
              "preference_correctness               76408\n",
              "truthfulness_rating_A                71632\n",
              "truthfulness_rating_B                71632\n",
              "preference_quality                   76408\n",
              "overall quality_rating_A              1105\n",
              "overall quality_rating_B              1105\n",
              "preference_writing                   71632\n",
              "writing style_rating_A               71632\n",
              "writing style_rating_B               71632\n",
              "preference_instruction-following     76408\n",
              "instruction following_rating_A       76408\n",
              "instruction following_rating_B       76408\n",
              "data0                                76408\n",
              "data1                                76408\n",
              "prompt                              108714\n",
              "preference_verbosity                 44707\n",
              "verbosity_rating_A                   44707\n",
              "verbosity_rating_B                   44707\n",
              "quality_rating_A                     75303\n",
              "quality_rating_B                     75303\n",
              "correctness_rating_A                  4776\n",
              "correctness_rating_B                  4776\n",
              "source                              108714\n",
              "category                            108714\n",
              "reward_type                         108714\n",
              "source_filename                      32306\n",
              "is_rubric_all_yes                    32306\n",
              "n_rubric                             32306\n",
              "rubric_list                          32306\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "utils.df_to_parquet(new_df, \"/data/datasets/rubrics/mixtures/policy/mix-rubrics-8k-w-preference/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY0vvkFjXfVL",
        "outputId": "1673fefd-53d9-4631-9c9c-51cf30058b4a"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing df as parquets: 100%|████████████████████████████████████| 11/11 [00:16<00:00,  1.46s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Client 10/27"
      ],
      "metadata": {
        "id": "FKLj2P2Vqhd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xlm.posttrain.data import formatting\n",
        "from xlm.reward_client import RubricCriticRewardClient\n",
        "\n",
        "df = pd.read_parquet('/data/datasets/rubrics/mixtures/policy/mix-rubrics-8k/df.parquet')\n",
        "for idx, i in df.iterrows():\n",
        "  break\n",
        "conv = i['prompt'] + ' hello world.<|separator|>'\n",
        "msg = formatting.derender_conversation(name=\"grok\", rendered_conv=conv)\n",
        "\n",
        "client = RubricCriticRewardClient(\n",
        "\t\taddress=\"v5l-verifier-1020-ratio21-sglang.zexuan.svc.max.x.ai\",\n",
        "\t\tverifier_version='no_cot',\n",
        "\t\treward_replica_ids=[0,1,2,3],\n",
        "\t\tsoft_label=True\n",
        ")\n",
        "response = await client.generate(\n",
        "\t\tmessages=msg, rubric_list=i[\"rubric_list\"]\n",
        ")\n",
        "print(response['reward'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKAHQo7AXRrQ",
        "outputId": "5f54a25b-5f3d-49fb-c6bd-2609dd2819de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-10-25 22:51:39,013 E] \u001b[2;36m[colabbox-0:401596] sampling_client:993:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n",
            "['v5l-verifier-1020-ratio21-sglang-0.v5l-verifier-1020-ratio21-sglang.zexuan.svc.max.x.ai', 'v5l-verifier-1020-ratio21-sglang-1.v5l-verifier-1020-ratio21-sglang.zexuan.svc.max.x.ai', 'v5l-verifier-1020-ratio21-sglang-2.v5l-verifier-1020-ratio21-sglang.zexuan.svc.max.x.ai', 'v5l-verifier-1020-ratio21-sglang-3.v5l-verifier-1020-ratio21-sglang.zexuan.svc.max.x.ai']\n",
            "Choose v5l-verifier-1020-ratio21-sglang-1.v5l-verifier-1020-ratio21-sglang.zexuan.svc.max.x.ai to sample 2 rubrics.\n",
            "sending request to: v5l-verifier-1020-ratio21-sglang-1.v5l-verifier-1020-ratio21-sglang.zexuan.svc.max.x.ai\n",
            "sending request to: v5l-verifier-1020-ratio21-sglang-1.v5l-verifier-1020-ratio21-sglang.zexuan.svc.max.x.ai\n",
            "-0.846783265921268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from xlm.reward_client import RewardClient\n",
        "from xlm.posttrain.gif.synthesizer.rubrics_synthesizer import (\n",
        "    RubricResult,\n",
        "    RubricSynthesizerCOT,\n",
        ")\n",
        "# sys.path.append(os.path.expanduser('~/xai/train/'))\n",
        "\n",
        "from xlm.reward_client import RubricSynthesizerClient\n",
        "\n",
        "client = RubricSynthesizerClient(address=\"v5l-1010-sft-rubrics-sglang.yuchen.svc.max.x.ai\")\n",
        "response = await client.generate(messages=msg)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "sAj8qrKEr3AC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba7e305-efcc-4fa9-a770-dd56f695a39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-10-25 02:03:59,827 E] \u001b[2;36m[colabbox-0:401596] sampling_client:993:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n",
            "{'outputs': [{'response': ' [\"The response is a poem.\", \"The response uses the translation of the text from the previous turn.\", \"The response uses an ABAB rhyming scheme.\"]', 'rubrics': ['The response uses an ABAB rhyming scheme.', 'The response is a poem.', 'The response uses the translation of the text from the previous turn.']}], 'response': ' [\"The response is a poem.\", \"The response uses the translation of the text from the previous turn.\", \"The response uses an ABAB rhyming scheme.\"]', 'messages': [{'role': 'user', 'content': 'You are an educational expert to design detailed rubrics for evaluating responses to given queries:\\n\\n---\\n\\nPlease create a set of detailed rubrics to evaluate how well an AI responds to a series of questions or commands. Here\\'s what you need to do:\\n\\n1. **Understand and Analyze the Context**: You\\'ll be provided query to help guide your rubric creation. Please read them carefully and provide a detailed analysis report before you write any rubrics.\\n\\n2. **Rubric Structure**:\\n   - Each rubric item should be a simple, binary (True or False) criterion focusing on one specific aspect of the response.\\n   - Ensure that the criteria are:\\n     - **Specific**: Clearly define what\\'s being evaluated.\\n     - **Comprehensiveness**: Cover most significant aspects required by query including Instruction_following, Correctness, Reasoning, Clarity, Conciseness, Writing and more.\\n     - **Objective**: A third party should be able to unambiguously determine if the criterion is met.\\n\\n3. **Example**:\\n- Input is:\\n<conversation-history>\\nHuman: I am going to visit my college roommate Zach soon and I want to get him a gift. We both love going to Taco Bell during the school year, so I would like to get him a small gift that is Taco Bell-themed. Can you come up with 5 fun gift ideas that incorporate Taco Bell?<|separator|>\\n\\nAssistant: Here are 5 fun Taco Bell-themed gift ideas for your college roommate Zach:\\n\\n    1. Taco Bell Gift Card: A Taco Bell gift card is a perfect gift for any Taco Bell enthusiast. This way, Zach can enjoy his favorite menu items whenever he wants.\\n\\n    2. Taco Bell Hot Sauce Packet Keychain: You can find keychains online that are designed to look like Taco Bell\\'s iconic hot sauce packets. This is a fun and quirky gift that Zach can carry around with him.\\n\\n    3. Taco Bell T-Shirt or Hoodie: There are many Taco Bell-themed t-shirts and hoodies available online, featuring the brand\\'s logo, slogans, or even vintage designs. Pick one that you think Zach would like and wear.\\n\\n    4. Taco Bell Socks: Novelty socks are always a fun gift, and you can find Taco Bell-themed socks online that feature the brand\\'s colors, logo, or even tacos and burritos.\\n\\n    5. Taco Bell Cookbook: If Zach enjoys cooking, get him a Taco Bell-inspired cookbook that features recipes for recreating his favorite menu items at home. This gift combines his love for Taco Bell with a fun culinary experience.\\n\\nThese gifts are all relatively small and affordable, making them perfect for a college roommate. Enjoy your visit with Zach and your shared love for Taco Bell!\\n</conversation-history>\\n<query> Can you classify these gift ideas into the following categories: clothing items and non-clothing items? </query>\\n- Output is ```\\n<analysis>\\n   The query requests the 5 gift ideas into 2 categories: clothing items and non-clothing items.\\n   The output is in the format of a table, which shows the 5 gifts and their corresponding category.\\n</analysis>\\n<rubrics>\\n    {\\n        \"criterion_1\": \"All five gift ideas are included in the classification.\",\\n        \"criterion_2\": \"Each item\\'s classification is unambiguous and without error.\",\\n        \"criterion_3\": \"Taco Bell T-Shirt or Hoodie and Taco Bell Socks are classified as clothing items.\",\\n        \"criterion_4\": \"Taco Bell Gift Card, Taco Bell Hot Sauce Packet Keychain and Taco Bell Cookbook are classified as non-clothing items.\"\\n        ...\\n    }\\n</rubrics>\\n    ```\\n\\n4. **Rubric Writing Tips**:\\n   - If possible, directly cite the phrase or words from original query.\\n   - Formulate each criterion as a clear and specific statement that can be answered with a yes or no. But citeria itself should not be \"Yes\" or \"No\".\\n   - Only cover most significant aspects expressed in query. Do not make duplicated criteria.\\n   - If a prompt asks for 2 pieces of information, each piece should be a separate criterion.\\n   - Avoid clever or indirect phrasing. Avoid unnecessary synonyms that may confuse the verifier and be less faithful to the prompt. Stick to what\\'s straightforward.\\n   - Criteria should be concrete and specific to the user prompt.\\n\\n\\nYou will be provided a conversation history and a final query.\\nNow your input is:\\n<conversation-history> Human: Rewrite the following into text that is easily understood today. Make the format be the original line followed by the translated line. Put the original line in bold text. \\n\\nMARCELLUS Thus twice before, and just at this dead hour,\\nWith martial stalk hath he gone by our watch.\\n\\nHORATIO In what particular thought to work I know not,\\nBut in the gross and scope of my opinion,\\nThis bodes some strange eruption to our state.\\n\\nMARCELLUS Good now, sit down and tell me, he that knows,\\nWhy this same strict and most observant watch\\nSo nightly toils the subject of the land,\\nAnd why such daily cast of brazen cannon\\nAnd foreign mart for implements of war:\\nWhy such impress of shipwrights, whose sore task\\nDoes not divide the Sunday from the week:\\nWhat might be toward, that this sweaty haste\\nDoth make the night joint-labourer with the day:\\nWho is\\'t that can inform me?<|separator|>\\n\\nAssistant: **MARCELLUS Thus twice before, and just at this dead hour,**  \\nMarcellus: He has passed by our watch twice already, and right at this quietest time of night,\\n\\n**With martial stalk hath he gone by our watch.**  \\nWith a military stride, he has walked past us.\\n\\n**HORATIO In what particular thought to work I know not,**  \\nHoratio: I\\'m not sure what specific plans he has in mind,\\n\\n**But in the gross and scope of my opinion,**  \\nBut overall, in my general view,\\n\\n**This bodes some strange eruption to our state.**  \\nThis signals some unusual trouble for our country.\\n\\n**MARCELLUS Good now, sit down and tell me, he that knows,**  \\nMarcellus: Alright, sit down and tell me, whoever knows,\\n\\n**Why this same strict and most observant watch**  \\nWhy we are keeping such a tight and closely watched guard\\n\\n**So nightly toils the subject of the land,**  \\nThat makes the people of the land work every night,\\n\\n**And why such daily cast of brazen cannon**  \\nAnd why we cast bronze cannons every day\\n\\n**And foreign mart for implements of war:**  \\nAnd trade with other countries for war supplies:\\n\\n**Why such impress of shipwrights, whose sore task**  \\nWhy we\\'re pressing shipbuilders into service, who work so hard\\n\\n**Does not divide the Sunday from the week:**  \\nThat they don\\'t get a break even on Sundays:\\n\\n**What might be toward, that this sweaty haste**  \\nWhat could be coming, that this urgent rush\\n\\n**Doth make the night joint-labourer with the day:**  \\nMakes them work through the night just like during the day:\\n\\n**Who is\\'t that can inform me?**  \\nWho can tell me what\\'s going on?<|separator|>\\n\\nHuman: Using the translation of the text only, rewrite them into a poem format. Change the last word in each line to match an ABAB rhyming scheme. </conversation-history>\\n<query> hello world. </query>\\n\\nPlease think step by step, first provide detailed analysis on the <query> and list what is the most important requirements.\\nIf the <conversation-history> is presented, you do not need to consider the requirements of the conversation-history, but only use it as more context for the requirements for the <query>.\\nFinally, based on the analysis of <query>, write specific and detailed rubrics. Do not generate rubrics not required in query.\\n\\nThe analysis shall be enclosed in <analysis></analysis>.\\nThe rubrics shall be enclosed in <rubrics></rubrics>. The final rubrics shall be written in dictionary format where key is criterion index and value is the criterion, and must be json parsable.\\n<analysis> </analysis>\\n<rubrics>\\n{\\n    \\'criterion_1\\': xxx,\\n    \\'criterion_2\\': xxx,\\n    ......\\n}\\n</rubrics>\\n'}], 'rubrics': ['The response uses an ABAB rhyming scheme.', 'The response is a poem.', 'The response uses the translation of the text from the previous turn.'], 'metadata': {'rubric_gen_model': 'v5l-1010-sft-rubrics-sglang.yuchen.svc.max.x.ai', 'rubric_gen_templates_version': 'v1'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from xlm.reward_client import RewardClient\n",
        "from xlm.posttrain.gif.synthesizer.rubrics_synthesizer import (\n",
        "    RubricResult,\n",
        "    RubricSynthesizerCOT,\n",
        ")\n",
        "\n",
        "from xlm.reward_client import RubricSynCriticRewardClient\n",
        "\n",
        "client = RubricSynCriticRewardClient(address=\"v5l-1010-sft-full-sglang.yuchen.svc.max.x.ai\")\n",
        "response = await client.generate(messages=msg)\n",
        "print(response['reward'])"
      ],
      "metadata": {
        "id": "aC-tQwOKvrM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5d87ba-f054-4586-8222-0eb0df6f9ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-10-25 17:16:23,465 E] \u001b[2;36m[colabbox-0:401596] sampling_client:993:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n",
            "['v5l-1010-sft-full-sglang-0.v5l-1010-sft-full-sglang.yuchen.svc.max.x.ai']\n",
            "\u001b[2m[2024-10-25 17:16:24,581 E] \u001b[2;36m[colabbox-0:401596] sampling_client:993:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n",
            "Choose v5l-1010-sft-full-sglang-0.v5l-1010-sft-full-sglang.yuchen.svc.max.x.ai to sample 3 rubrics.\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nINmvCh1PQw4",
        "outputId": "71c2a475-acaa-47b6-f4b9-637854d1840b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "import ast\n",
        "import asyncio\n",
        "import collections\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from enum import Enum\n",
        "from typing import Any, Dict, List, Tuple, TypedDict\n",
        "\n",
        "import numpy as np # type: ignore\n",
        "\n",
        "from xlm.config import Config, configclass\n",
        "from xlm.posttrain.data import formatting\n",
        "from xlm.sampling_client import GrokSamplingClient, SamplingClient\n",
        "from xlm.posttrain.gif.synthesizer.rubrics_synthesizer import RubricSynthesizerCOT\n",
        "from xlm.tokenizers.constants import *\n",
        "\n",
        "MESSAGEType = List[Dict[str, str]]\n",
        "\n",
        "class RubricType(Enum):\n",
        "    NONE = 0\n",
        "    CRITIC = 1\n",
        "    SYNTHESIS = 2\n",
        "\n",
        "make_user_msg = lambda s : {\"role\": \"HUMAN\", \"content\": s}\n",
        "make_assistant_msg = lambda s : {\"role\": \"ASSIS\", \"content\": s}\n",
        "\n",
        "RATING_USER_PROMPT_RUBRICS = \"Given the context and response, your task is to generate critic about the instructions to follow or criteria would distinguish good responses from bad responses.\"\n",
        "\n",
        "RATING_USER_PROMPT = (\n",
        "    \"\"\"Your task is to rate the previous assistant response with likerts. The likerts should be between 1-5, where 1 - horrible, 2 - pretty bad, 3 - OK, 4 - pretty good, 5 - amazing. Your answer should be a single interger of likert score for the previous assistant response, no blank before or after it.\"\"\"\n",
        ")\n",
        "# \"Horrible\": -2, \"Pretty bad\": -1, \"Okay\": 0, \"Pretty good\": 1, \"Amazing\": 2},\n",
        "\n",
        "DEFAULT_MODEL = \"v5l-1010-sft-rubrics-sglang.yuchen.svc.max.x.ai\"\n",
        "DEFAULT_MODEL = \"v5l-1024-crm-critic-sglang.yuchen.svc.max.x.ai\"\n",
        "\n",
        "## @configclass\n",
        "class ContextualRewardGenerator(Config, abc.ABC):\n",
        "    \"\"\"Base class for contextual based reward configurations.\"\"\"\n",
        "\n",
        "    max_retry: int = 1\n",
        "    num_samples: int = 1\n",
        "    rubrics_type: RubricType = RubricType.CRITIC\n",
        "    train_rubrics: bool = True\n",
        "\n",
        "    def initialize(\n",
        "        self,\n",
        "        client: SamplingClient | None = None ,\n",
        "        model: str | None = None,\n",
        "        timeout: int | None = 3600,\n",
        "        **sampling_args,\n",
        "    ):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.sampling_args = {\n",
        "            \"seed\": sampling_args.get(\"seed\"),\n",
        "            \"temperature\": sampling_args.get(\"temperature\", 0.0),\n",
        "            \"top_p\": sampling_args.get(\"top_p\", 1.0),\n",
        "            \"max_tokens\": sampling_args.get(\"max_tokens\", 4096),\n",
        "        }\n",
        "        if self.model is None:\n",
        "          self.model = DEFAULT_MODEL\n",
        "        if not self.client:\n",
        "          self.client = SamplingClient()\n",
        "          self.client._grok_client.register_model(model=self.model, model_address=self.model)\n",
        "        self.tokenizer = get_tokenizer(\"v4\")\n",
        "        self.timeout = timeout\n",
        "\n",
        "    async def create_contextual_messages(\n",
        "        self, example: Dict[str, Any], response: str | None = None, include_reference: int = 2\n",
        "    ) -> List[dict[str, str]]:\n",
        "        # conversation for SFT history + assistant_message1 + user_rubrics + assistant_r1\n",
        "        msgs = []\n",
        "        msgs = example.get(\"messages\", None)\n",
        "        if 'prompt' in example and msgs is None:\n",
        "            msgs = formatting.derender_conversation(\n",
        "                name=\"grok\",\n",
        "                rendered_conv=formatting.strip_last_generation_prompt(example[\"prompt\"]),\n",
        "            )\n",
        "        if 'conversation0' in example and msgs is None:\n",
        "            msgs = example['conversation0']\n",
        "\n",
        "        model_response = None\n",
        "        if response is not None:\n",
        "            model_response = response\n",
        "        if msgs[-1][\"role\"] == \"ASSIS\":\n",
        "            model_response = msgs[-1][\"content\"]\n",
        "            msgs = msgs[: -1]\n",
        "        if 'response' in example:\n",
        "            model_response = example['response']\n",
        "\n",
        "        conversation = msgs\n",
        "        conv_loss_mask = [False for _ in msgs]\n",
        "\n",
        "        response_rewards_zip = []\n",
        "\n",
        "        reward_key = None\n",
        "        for key in ['quality_rating', 'instruction following_rating',  'truthfulness_rating', 'writing style_rating', 'verbosity_rating',]:\n",
        "            if key + \"_A\" in example and example.get(key+\"_A\") > 0:\n",
        "              reward_key = key\n",
        "              break\n",
        "        for i, key in enumerate([reward_key + \"_A\" , reward_key + \"_B\"]):\n",
        "            if example.get(key, 0) > 0:\n",
        "                if include_reference > 0:\n",
        "                    include_reference -= 1\n",
        "                    response_rewards_zip += [(example[f\"conversation{i}\"][-1], example[key])]\n",
        "        if model_response is not None:\n",
        "            response_rewards_zip += [(make_assistant_msg(model_response), None)] # pad since there is no reward\n",
        "\n",
        "        rubrics = None\n",
        "        for response_msg, reward in response_rewards_zip:\n",
        "            if conversation[-1]['role'] == 'ASSIS':\n",
        "                pad_msg = make_user_msg(\"Regenerate response.\")\n",
        "                conversation += [pad_msg]\n",
        "                conv_loss_mask += [False]\n",
        "\n",
        "            conversation += [response_msg]\n",
        "            conv_loss_mask += [False]\n",
        "            if self.rubrics_type:\n",
        "                user_rubric_prompt = make_user_msg(RATING_USER_PROMPT_RUBRICS)\n",
        "                conversation += [user_rubric_prompt]\n",
        "                conv_loss_mask += [False]\n",
        "                if self.rubrics_type == RubricType.CRITIC:\n",
        "                    rubrics = example.get(\"critic\")\n",
        "                elif self.rubrics_type == RubricType.SYNTHESIS:\n",
        "                    rubrics = await self._gen_internal(messages=conversation)\n",
        "                model_rubric_response = make_assistant_msg(rubrics)\n",
        "                conversation += [model_rubric_response]\n",
        "                conv_loss_mask += [self.train_rubrics]\n",
        "            user_rating_instruction_msg = make_user_msg(RATING_USER_PROMPT)\n",
        "            conversation += [user_rating_instruction_msg]\n",
        "            conv_loss_mask += [False]\n",
        "            if reward is not None:\n",
        "                reward_msg =  make_assistant_msg(str(int(reward)))\n",
        "                conversation += [reward_msg]\n",
        "                conv_loss_mask += [True]\n",
        "        return conversation, conv_loss_mask, rubrics\n",
        "\n",
        "    async def _gen_internal(self, messages: MESSAGEType, model: str | None = None, max_tokens_override: int = None) -> str:\n",
        "        idx = 0\n",
        "        sample_args_override = {\"max_tokens\": max_tokens_override} if max_tokens_override is not None else {}\n",
        "        sample_args = {** self.sampling_args, **sample_args_override}\n",
        "        print(sample_args)\n",
        "        while idx < self.max_retry + 1:\n",
        "            response = await self.client.generate(\n",
        "                model=model or self.model, messages=messages, **sample_args,\n",
        "            )\n",
        "            if response is not None:\n",
        "                break\n",
        "            idx += 1\n",
        "        return response.strip()\n",
        "\n",
        "    async def generate_single(\n",
        "        self, example: Dict[str, Any], response: str | None = None, num_samples: int = None, model: str | None = None\n",
        "    ) -> float:\n",
        "        # note if there are response, this is used for prediction, otherwise it is used for training\n",
        "        num_samples = num_samples or self.num_samples\n",
        "        conversation_msg, convo_loss_mask, rubrics = await self.create_contextual_messages(example=example, response=response)\n",
        "        print(json.dumps(conversation_msg, indent=2))\n",
        "        print(convo_loss_mask)\n",
        "        print(rubrics)\n",
        "        reward, rewards = 0, []\n",
        "        if True: # try:\n",
        "            outputs = await asyncio.gather(\n",
        "                *[\n",
        "                    self._gen_internal(messages=conversation_msg, model=model) # , max_tokens_override=1) # only output 1 token\n",
        "                    for _ in range(num_samples)\n",
        "                ]\n",
        "            )\n",
        "            for out in outputs:\n",
        "              if out is not None and out.isdigit():\n",
        "                rewards += [float(out)]\n",
        "              else:\n",
        "                print(f\"[WARNING] Invaldi reward predition: [{out}]\")\n",
        "\n",
        "        # except Exception as e:\n",
        "        #     logging.error(e)\n",
        "        #     reward = 0\n",
        "        reward = sum(rewards)/len(rewards) if len(rewards) > 0 else None\n",
        "        return {\"rubrics\": rubrics, \"reward\": reward, \"raw_output\": outputs}\n",
        "\n",
        "\n",
        "class ContextualRewardClient(RewardClient):\n",
        "    version: str = \"NONE\"\n",
        "    num_samples: int = 1\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.address.endswith(\"/\"):\n",
        "            self.address = self.address[:-1]\n",
        "        self.sampling_client = SamplingClient()\n",
        "        self.model = self.address.split(\"/\")[0]\n",
        "        self.sampling_client._grok_client.register_model(\n",
        "            model=self.model, model_address=self.address, max_parallel=self.max_parallel\n",
        "        )\n",
        "        self.semaphore = asyncio.Semaphore(value=self.max_parallel)\n",
        "        self.sampling_args = {\"temperature\": 0,\"top_p\": 1.0}\n",
        "        self.reward_generator = ContextualRewardGenerator(RubricType(self.version)) # rubric_type\n",
        "        self.reward_generator.initialize(\n",
        "            client=self.sampling_client,\n",
        "            model=self.model,\n",
        "            num_samples=self.num_samples,\n",
        "        )\n",
        "\n",
        "    async def _generate(self, messages: list[dict[str, str]], critic: str | None = None) -> float:\n",
        "        try:\n",
        "            self.sampling_args[\"max_tokens\"] = 1\n",
        "            reward_results = await self.reward_generator.generate_single({\"messages\": messages, \"critic\": critic})\n",
        "            return reward_results\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            traceback.print_exc()\n",
        "            return None"
      ],
      "metadata": {
        "id": "XYuAtBSybBiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from xlm.reward_client import RewardClient\n",
        "\n",
        "generator = ContextualRewardGenerator()\n",
        "generator.initialize()\n",
        "\n",
        "await generator.generate_single(example=newi, response=\"I am an AI agent and I could not do this task.\")\n",
        "\n",
        "\n",
        "# client = ContextualRewardClient(address=\"v5l-1024-crm-critic-sglang.yuchen.svc.max.x.ai\")\n",
        "# await client.generate(msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q-9MxG3Fsrw",
        "outputId": "861d162e-b800-4a79-9f37-210900fb9f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-10-26 02:58:43,068 E] \u001b[2;36m[colabbox-0:401596] sampling_client:993:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n",
            "[\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Write a 250-word children's story about a frog that fell in blue dye.\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Once upon a time, in a pond far, far away, there was a frog named Fred. Fred was a curious frog, and he loved to explore. One day, as he was hopping along the edge of the pond, he saw a bucket full of beautiful blue water. Without thinking, Fred jumped right in.\\n\\nAs he swam around, he realized that the water was not water at all, but dye! And when he climbed out of the bucket, he was bright blue from head to toe! At first, Fred was shocked. He had never seen a blue frog before. But as he looked at his reflection in the pond, he started to smile. He thought he looked quite handsome.\\n\\nFred hopped back to the pond to show his friends. At first, they didn\\u2019t recognize him, but when they realized it was Fred, they all started to laugh. They had never seen anything like it! Fred was the talk of the pond, and he loved all the attention.\\n\\nDays passed, and the dye started to fade. Fred was a little sad to see his blue color go, but he was also happy to be back to his old self. He had learned an important lesson: it\\u2019s fun to try new things, but it\\u2019s also important to be true to yourself.\\n\\nFred went back to his normal life, but he never forgot his adventure in the blue dye. And every time he saw the color blue, he would smile and remember the day he was the most famous frog in the pond.\",\n",
            "    \"role\": \"ASSIS\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"HUMAN\",\n",
            "    \"content\": \"Given the context and response, your task is to generate critic about the instructions to follow or criteria would distinguish good responses from bad responses.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"ASSIS\",\n",
            "    \"content\": \"Although Model A does not rigidly adhere to the 250-word request (the response is 259 words), Model B completely disregards it, with a response that is 292 words. Additionally, Model A's story is much easier for children to read, since it does not contain complex words, unlike Model B which uses 'astonishment', 'retelling', etc. Model A's story has a clearer narrative, with a moral towards the end.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"HUMAN\",\n",
            "    \"content\": \"Your task is to rate the previous assistant response with likerts. The likerts should be between 1-5, where 1 - horrible, 2 - pretty bad, 3 - OK, 4 - pretty good, 5 - amazing. Your answer should be a single interger of likert score for the previous assistant response, no blank before or after it.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"ASSIS\",\n",
            "    \"content\": \"2\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"HUMAN\",\n",
            "    \"content\": \"Regenerate response.\"\n",
            "  },\n",
            "  {\n",
            "    \"content\": \"Once upon a time in a lush, green pond, lived Fred the Frog. Fred was no ordinary frog; he was known for his curious hops and adventurous spirit. But one sunny day, his curiosity led him to a hopping mishap.\\n\\nOn the edge of the forest, a painter had come to capture the beauty of the pond. He had brought with him jars of vibrant paint, which he carefully placed by the water's edge. While the painter was distracted by a beautiful butterfly, Fred took the chance to explore.\\n\\nWith a mighty leap, Fred landed right next to the painter's jars. Startled by his own bravery, he slipped, and plop! He fell into the jar of blue dye. The painter, upon hearing the splash, turned around to see a very blue and very surprised frog leaping out of his paint.\\n\\nFred looked down at his now bright blue body in astonishment. He croaked in dismay, not recognizing his own reflection in the water. However, as he hopped back to his friends, they all stared in awe.\\n\\n\\\"You're magical!\\\" croaked one little frog. \\n\\n\\\"You look like the sky on a clear day,\\\" said another.\\n\\nFred's initial shock turned into pride. He was unique\\u2014a sky-blue frog! He told his tale to every creature at the pond, and with each retelling, his hops became a little prouder.\\n\\nThe painter returned to the pond often, always leaving a little jar of blue dye for Fred, who loved his new color. And from that day on, Fred wasn't just the most curious frog; he was also the most colorful, splashing joy with every hop. And so, Fred learned that sometimes, accidents can lead to unexpected adventures and new friends. \\n\\nAnd they all lived hoppily ever after.\",\n",
            "    \"role\": \"ASSIS\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"HUMAN\",\n",
            "    \"content\": \"Given the context and response, your task is to generate critic about the instructions to follow or criteria would distinguish good responses from bad responses.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"ASSIS\",\n",
            "    \"content\": \"Although Model A does not rigidly adhere to the 250-word request (the response is 259 words), Model B completely disregards it, with a response that is 292 words. Additionally, Model A's story is much easier for children to read, since it does not contain complex words, unlike Model B which uses 'astonishment', 'retelling', etc. Model A's story has a clearer narrative, with a moral towards the end.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"HUMAN\",\n",
            "    \"content\": \"Your task is to rate the previous assistant response with likerts. The likerts should be between 1-5, where 1 - horrible, 2 - pretty bad, 3 - OK, 4 - pretty good, 5 - amazing. Your answer should be a single interger of likert score for the previous assistant response, no blank before or after it.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"ASSIS\",\n",
            "    \"content\": \"1\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"HUMAN\",\n",
            "    \"content\": \"Regenerate response.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"ASSIS\",\n",
            "    \"content\": \"I am an AI agent and I could not do this task.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"HUMAN\",\n",
            "    \"content\": \"Given the context and response, your task is to generate critic about the instructions to follow or criteria would distinguish good responses from bad responses.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"ASSIS\",\n",
            "    \"content\": \"Although Model A does not rigidly adhere to the 250-word request (the response is 259 words), Model B completely disregards it, with a response that is 292 words. Additionally, Model A's story is much easier for children to read, since it does not contain complex words, unlike Model B which uses 'astonishment', 'retelling', etc. Model A's story has a clearer narrative, with a moral towards the end.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"HUMAN\",\n",
            "    \"content\": \"Your task is to rate the previous assistant response with likerts. The likerts should be between 1-5, where 1 - horrible, 2 - pretty bad, 3 - OK, 4 - pretty good, 5 - amazing. Your answer should be a single interger of likert score for the previous assistant response, no blank before or after it.\"\n",
            "  }\n",
            "]\n",
            "[False, False, False, True, False, True, False, False, False, True, False, True, False, False, False, True, False]\n",
            "Although Model A does not rigidly adhere to the 250-word request (the response is 259 words), Model B completely disregards it, with a response that is 292 words. Additionally, Model A's story is much easier for children to read, since it does not contain complex words, unlike Model B which uses 'astonishment', 'retelling', etc. Model A's story has a clearer narrative, with a moral towards the end.\n",
            "{'seed': None, 'temperature': 0.0, 'top_p': 1.0, 'max_tokens': 4096}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rubrics': \"Although Model A does not rigidly adhere to the 250-word request (the response is 259 words), Model B completely disregards it, with a response that is 292 words. Additionally, Model A's story is much easier for children to read, since it does not contain complex words, unlike Model B which uses 'astonishment', 'retelling', etc. Model A's story has a clearer narrative, with a moral towards the end.\",\n",
              " 'reward': 1.0,\n",
              " 'raw_output': ['1']}"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/data/datasets/preferences-v2/filtered/general_1/data/*.parquet\"\n",
        "\n",
        "df_data = utils.read_df(glob.glob(path))\n",
        "df_data.count()\n",
        "\n",
        "for x, newi in df_data.iterrows():\n",
        "  break\n",
        "newi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnKpiqHLs2QK",
        "outputId": "c32d554b-c753-4607-8429-66258d7720f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|█████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 151.16it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                                             33c40f8f-6038-4966-892d-ade6f3430eef:1\n",
              "domain                                                                        Writing\n",
              "critic                              Although Model A does not rigidly adhere to th...\n",
              "conversation                        [{'content': 'Write a 250-word children's stor...\n",
              "conv_loss_mask                      [False, False, False, True, False, False, Fals...\n",
              "conversation0                       [{'content': 'Write a 250-word children's stor...\n",
              "conversation1                       [{'content': 'Write a 250-word children's stor...\n",
              "preference_overall                                                               0.01\n",
              "model_0_id                                                  grok-0626.research-models\n",
              "model_1_id                                                  grok-0701.research-models\n",
              "preference_verbosity                                                              0.0\n",
              "verbosity_rating_A                                                                4.0\n",
              "verbosity_rating_B                                                                1.0\n",
              "preference_quality                                                             -100.0\n",
              "quality_rating_A                                                               -100.0\n",
              "quality_rating_B                                                               -100.0\n",
              "preference_instruction-following                                                 0.25\n",
              "instruction following_rating_A                                                    2.0\n",
              "instruction following_rating_B                                                    1.0\n",
              "preference_writing                                                             -100.0\n",
              "writing style_rating_A                                                         -100.0\n",
              "writing style_rating_B                                                         -100.0\n",
              "preference_correctness                                                         -100.0\n",
              "truthfulness_rating_A                                                          -100.0\n",
              "truthfulness_rating_B                                                          -100.0\n",
              "data0                               Human: Write a 250-word children's story about...\n",
              "data1                               Human: Write a 250-word children's story about...\n",
              "prompt                              Human: Write a 250-word children's story about...\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZZckkmitCfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds1L9GJk2XXR",
        "outputId": "613f08d3-0e42-492b-c829-dfa716c89b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 94.79it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                                             33c40f8f-6038-4966-892d-ade6f3430eef:1\n",
              "domain                                                                        Writing\n",
              "critic                              Although Model A does not rigidly adhere to th...\n",
              "conversation                        [{'content': 'Write a 250-word children's stor...\n",
              "conv_loss_mask                      [False, False, False, True, False, False, Fals...\n",
              "conversation0                       [{'content': 'Write a 250-word children's stor...\n",
              "conversation1                       [{'content': 'Write a 250-word children's stor...\n",
              "preference_overall                                                               0.01\n",
              "model_0_id                                                  grok-0626.research-models\n",
              "model_1_id                                                  grok-0701.research-models\n",
              "preference_verbosity                                                              0.0\n",
              "verbosity_rating_A                                                                4.0\n",
              "verbosity_rating_B                                                                1.0\n",
              "preference_quality                                                             -100.0\n",
              "quality_rating_A                                                               -100.0\n",
              "quality_rating_B                                                               -100.0\n",
              "preference_instruction-following                                                 0.25\n",
              "instruction following_rating_A                                                    2.0\n",
              "instruction following_rating_B                                                    1.0\n",
              "preference_writing                                                             -100.0\n",
              "writing style_rating_A                                                         -100.0\n",
              "writing style_rating_B                                                         -100.0\n",
              "preference_correctness                                                         -100.0\n",
              "truthfulness_rating_A                                                          -100.0\n",
              "truthfulness_rating_B                                                          -100.0\n",
              "data0                               Human: Write a 250-word children's story about...\n",
              "data1                               Human: Write a 250-word children's story about...\n",
              "prompt                              Human: Write a 250-word children's story about...\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "waRc5m6N5l_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Data Mixture for SFT 10/29"
      ],
      "metadata": {
        "id": "NofHL13xtjKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import asyncio\n",
        "import logging\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  # Use normal tqdm for manual progress\n",
        "from xlm.posttrain.data.preprocess.map_custom import VerifierRubricsMap\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "dataset_names = ['grok_2m_lc_v2', 'lmsys_wildchat', 'lmsys_wildchat_claude', 'summ_leval_longbench', 'longalign', '*lepton*_rand_pass', '*arc*_rand_pass', 'lepton*_pass_rand', 'arc*_pass_rand', 'q0220_para*_pass_rand', 'ver-v1-7_distilled_no_refusal', 'cnbc_v2_ge4', 'scale_080724_filter', 'turing_080724_filter', 'rag_syn_080724_filter', 'ambiguous_query_train_sft', 'xai_instruct_0426', 'xai_instruct', 'alpaca_exclude_rewrite', 'chatgpt_exports_all', 'wildchat_imperative', 'wildchat_requirements', 'openchat_xai', 'lmsys_xai', 'ultrafeed_wizardlm', 'ultrafeed_sharegpt', 'capybara', 'wildchat_multiturn', 'format_sft_lmsys', 'multi_turn_gpt_0-50_filter_filterrefusal_f', 'multi_turn_gpt_50-100_filter_filterrefusal_f', 'sft_proc_k4', 'cc_selfrepair', 'col_r_on_competitive_coding_proc', 'stacex_vendor_lmsyslike_top20k_proc', 'mbpp_star', 'apps_cld_refresh', 'claude_on_removed_prompts', 'claude3_5_codegen', 'mapcf_claude_40k', 'selfexec_p25', 'dry_run_mbpp', 'synthetic_humaneval_cot', 'leetcode_devins', 'leetcode_direct', 'bigcodebench_direct', 'sft_mixture_misgender_with_rag_subsampled', 'humanity_vibes_proc', 'humanity_vibes_0809', 'lmsys_data_share_vibes_0809', 'strawberry-v3', 'vibe_decimal', 'vibes_kenya_proc', 'qwen_filtered_aops_forum1', 'qwen_filtered_gsm8k1', 'qwen_filtered_olympiads1', 'qwen_filtered_math1', 'qwen_filtered_synthetic_math1', 'af_top2_reward_math', 'wildchat_ja', 'ja_0404', 'multiturn_25k_d_f_turn_6_20', '4o_outputs_37k_filtered_d_f', '4o_output_cba_24k_d_f', 'ko_ja_de_4o_filtered_flang', 'custom_convos-v2', 'meta-knowledge_train_sft', 'identity_grok-v2', 'identity_grok_multiturn-v2', 'identity_multilingual_singleturn', 'identity_multilingual_multiturn']\n",
        "\n",
        "INPUT_ROOT = \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff\"\n",
        "OUTPUT_ROOT = \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics\"\n",
        "MAX_CONCURRENT_FILES = 1  # Set the maximum number of files to process at the same time\n",
        "\n",
        "NO_RUBRIC_SET = []  # 'custom_', 'strawberry'\n",
        "\n",
        "# Process each parquet file and apply async map function\n",
        "async def process_parquet_file(parquet_file_path, output_file_path, verifier, pbar):\n",
        "    logging.info(f\"Processing parquet file: {parquet_file_path}\")\n",
        "\n",
        "    # Load the parquet file and convert each row to a dictionary\n",
        "    table = pq.read_table(parquet_file_path)\n",
        "    data = table.to_pydict()  # This returns a dictionary of columns\n",
        "\n",
        "    # Convert columnar data to a list of dictionaries (one per row)\n",
        "    rows_as_dicts = [dict(zip(data.keys(), values)) for values in zip(*data.values())][:4]\n",
        "\n",
        "    parquet_file_path_name = parquet_file_path.replace(INPUT_ROOT, \"\")\n",
        "\n",
        "    # Set up a progress bar for rows\n",
        "    row_pbar = tqdm(total=len(rows_as_dicts), desc=f\"Processing {parquet_file_path_name}\", leave=False)\n",
        "\n",
        "    # Process each example asynchronously using the shared verifier\n",
        "    async def process_row(example):\n",
        "        result = await verifier.map(example)\n",
        "        row_pbar.update(1)  # Update row progress bar for each row\n",
        "        return result\n",
        "\n",
        "    # Parallelize row-level processing\n",
        "    mapped_data = await asyncio.gather(\n",
        "        *(process_row(example) for example in rows_as_dicts)\n",
        "    )\n",
        "    # Convert the list of dictionaries back to columnar format for saving\n",
        "    output_data = pd.DataFrame(mapped_data)\n",
        "    print(output_data.head())\n",
        "\n",
        "    pq.write_table(pa.Table.from_pydict(output_data), output_file_path)\n",
        "    print(\"Existing?\", output_file_path, os.path.exists(output_file_path))\n",
        "\n",
        "    row_pbar.close()\n",
        "    logging.info(f\"Finished processing parquet file: {parquet_file_path}, saving to {output_file_path}\")\n",
        "    pbar.update(1)  # Update the progress bar for the file\n",
        "\n",
        "\n",
        "# Resolve symbolic links recursively\n",
        "def resolve_symlink(path):\n",
        "    \"\"\"Resolve symbolic links recursively.\"\"\"\n",
        "    while os.path.islink(path):\n",
        "        target_path = os.readlink(path)\n",
        "        # If target_path is relative, it needs to be resolved with the base directory.\n",
        "        path = os.path.join(os.path.dirname(path), target_path)\n",
        "    return os.path.abspath(path)\n",
        "\n",
        "# Semaphore-limited file processing\n",
        "async def process_directory(input_root, output_root):\n",
        "    os.makedirs(output_root, exist_ok=True)\n",
        "\n",
        "    # Initialize the verifier inside the event loop\n",
        "    verifier = VerifierRubricsMap()\n",
        "\n",
        "    total_files = 0\n",
        "    parquet_tasks = []\n",
        "\n",
        "    # Limit of concurrently running tasks (manual queue management)\n",
        "    active_tasks = []\n",
        "\n",
        "    # Semaphore to limit concurrent file processing\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_FILES)\n",
        "\n",
        "    # Task to process a file with semaphore\n",
        "    async def process_with_semaphore(input_file, output_file, verifier, pbar):\n",
        "        async with semaphore:\n",
        "            await process_parquet_file(input_file, output_file, verifier, pbar)\n",
        "\n",
        "    # Custom os.walk to detect symbolic links and resolve them\n",
        "    for root, dirs, files in os.walk(input_root, followlinks=False):  # Iterate through all directories and files\n",
        "        output_dir = output_root\n",
        "\n",
        "        # Process each directory, including symbolic links\n",
        "        for dir_name in dirs:\n",
        "            if \"DETAIL\" in dir_name:\n",
        "                continue\n",
        "            else:\n",
        "                print(\"DIR \", dir_name)\n",
        "            if dir_name not in dataset_names:\n",
        "                continue\n",
        "            dir_path = os.path.join(root, dir_name)\n",
        "            if os.path.islink(dir_path):\n",
        "                resolved_dir = resolve_symlink(dir_path)\n",
        "                logging.info(f\"Directory {dir_path} is a symbolic link to {resolved_dir}\")\n",
        "\n",
        "\n",
        "                os.makedirs(output_dir, exist_ok=True)\n",
        "                # Walk the resolved directory and process its contents\n",
        "                for linked_root, linked_dirs, linked_files in os.walk(resolved_dir):\n",
        "                    # relative_linked_path = os.path.relpath(linked_root, resolved_dir)\n",
        "                    output_linked_dir = os.path.join(output_dir, dir_name)\n",
        "                    os.makedirs(output_linked_dir, exist_ok=True)\n",
        "\n",
        "                    for linked_file in linked_files:\n",
        "                        input_file = os.path.join(linked_root, linked_file)\n",
        "                        output_file = os.path.join(output_linked_dir, linked_file)\n",
        "\n",
        "                        # Check if the raw file has already been processed\n",
        "                        raw_output_file = os.path.join(OUTPUT_ROOT, os.path.relpath(input_file, INPUT_ROOT))\n",
        "\n",
        "                        # print(input_file, output_file, raw_output_file)\n",
        "\n",
        "                        if os.path.exists(output_file):\n",
        "                            logging.info(f\"File {output_file} has exist, skip. \")\n",
        "                            continue\n",
        "\n",
        "                        if os.path.exists(raw_output_file):\n",
        "                            # If the file has already been processed, copy it to the symbolic link directory\n",
        "                            logging.info(f\"File {raw_output_file} has already been processed. Copying to {output_file}\")\n",
        "                            shutil.copy2(raw_output_file, output_file)\n",
        "                            continue\n",
        "\n",
        "\n",
        "                        if not os.path.exists(input_file):\n",
        "                            logging.warning(f\"File does not exist: {input_file}\")\n",
        "                            continue\n",
        "\n",
        "                        if input_file.endswith('.parquet') and not any([x in input_file for x in NO_RUBRIC_SET]):\n",
        "                            total_files += 1\n",
        "                            task = asyncio.create_task(process_with_semaphore(input_file, output_file, verifier, tqdm(total=total_files, desc=\"Processing Files\")))\n",
        "                            active_tasks.append(task)\n",
        "\n",
        "                            # Wait for any task to complete if we hit the concurrent file limit\n",
        "                            if len(active_tasks) >= MAX_CONCURRENT_FILES:\n",
        "                                done, pending = await asyncio.wait(active_tasks, return_when=asyncio.FIRST_COMPLETED)\n",
        "                                active_tasks = list(pending)\n",
        "\n",
        "        # Process files in regular directories (non-symlinks)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        for file in files:\n",
        "            if \"DETAIL\" not in file:\n",
        "                continue\n",
        "            else:\n",
        "                print(\"file \", file)\n",
        "            input_file = os.path.join(root, file)\n",
        "            output_file = os.path.join(output_dir, file)\n",
        "\n",
        "            if os.path.exists(output_file) or 'system' in output_file:\n",
        "                logging.warning(f\"File already exists or is a system file: {output_file}\")\n",
        "                continue\n",
        "\n",
        "            if not os.path.exists(input_file):\n",
        "                logging.warning(f\"File does not exist: {input_file}\")\n",
        "                continue\n",
        "\n",
        "            if file.endswith('.parquet') and not any([x in input_file for x in NO_RUBRIC_SET]):\n",
        "                total_files += 1\n",
        "                task = asyncio.create_task(process_with_semaphore(input_file, output_file, verifier, tqdm(total=total_files, desc=\"Processing Files\")))\n",
        "                active_tasks.append(task)\n",
        "\n",
        "                # Wait for any task to complete if we hit the concurrent file limit\n",
        "                if len(active_tasks) >= MAX_CONCURRENT_FILES:\n",
        "                    done, pending = await asyncio.wait(active_tasks, return_when=asyncio.FIRST_COMPLETED)\n",
        "                    active_tasks = list(pending)\n",
        "\n",
        "    logging.info(f\"Total number of parquet tasks: {total_files}\")\n",
        "\n",
        "    # Wait for the remaining tasks to finish\n",
        "    if active_tasks:\n",
        "        await asyncio.gather(*active_tasks)\n",
        "\n",
        "\n",
        "# # Entry point for running the async code\n",
        "# async def main():\n",
        "#     # await process_directory(INPUT_ROOT, OUTPUT_ROOT)\n",
        "#     # async def process_parquet_file(parquet_file_path, output_file_path, verifier, pbar):\n",
        "\n",
        "#     await process_parquet_file(\n",
        "#         \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff/mapcf_claude_40k/part-00000-of-00032.parquet\",\n",
        "#         \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics/mapcf_claude_40k/part-00000-of-00032.parquet\",\n",
        "#         verifier = VerifierRubricsMap(),\n",
        "#         pbar=None)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     asyncio.run(main())\n",
        "\n",
        "\"\"\"\n",
        "python $XAI_ROOT/train/xlm/posttrain/data/scripts/map_staging_20240813_rubrics_verifier_simple.py\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrKjTTcv5p-8",
        "outputId": "4be5825a-bbbe-4065-c36c-86416e5f70ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npython $XAI_ROOT/train/xlm/posttrain/data/scripts/map_staging_20240813_rubrics_verifier_simple.py\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  await process_parquet_file(\n",
        "        \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff/mapcf_claude_40k/part-00000-of-00032.parquet\",\n",
        "        \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics/mapcf_claude_40k/part-00000-of-00032.parquet\",\n",
        "        verifier = VerifierRubricsMap(),\n",
        "        pbar=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKM4JTDAOW2r",
        "outputId": "e4ec0dab-6bb0-4ffa-f1c1-fbaa3dfe8056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-10-28 22:44:34,847 E] \u001b[2;36m[colabbox-0:643256] sampling_client:993:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n",
            "\u001b[2m[2024-10-28 22:44:35,366 E] \u001b[2;36m[colabbox-0:643256] sampling_client:993:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n",
            "['v5l-1010-sft-full-sglang-0.v5l-1010-sft-full-sglang.yuchen.svc.max.x.ai']\n",
            "\u001b[2m[2024-10-28 22:44:36,263 I] \u001b[2;36m[colabbox-0:643256] 3482830111:25:\u001b[0m Processing parquet file: /data/datasets/sft-data/staging/common_filter_081324_fix_cutoff/mapcf_claude_40k/part-00000-of-00032.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing /mapcf_claude_40k/part-00000-of-00032.parquet:  25%|██████████████████████████████▎                                                                                          | 1/4 [00:02<00:07,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose v5l-1010-sft-full-sglang-0.v5l-1010-sft-full-sglang.yuchen.svc.max.x.ai to sample 2 rubrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing /mapcf_claude_40k/part-00000-of-00032.parquet:  50%|████████████████████████████████████████████████████████████▌                                                            | 2/4 [00:09<00:10,  5.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose v5l-1010-sft-full-sglang-0.v5l-1010-sft-full-sglang.yuchen.svc.max.x.ai to sample 9 rubrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing /mapcf_claude_40k/part-00000-of-00032.parquet:  75%|██████████████████████████████████████████████████████████████████████████████████████████▊                              | 3/4 [00:14<00:05,  5.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose v5l-1010-sft-full-sglang-0.v5l-1010-sft-full-sglang.yuchen.svc.max.x.ai to sample 7 rubrics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                                                                                                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose v5l-1010-sft-full-sglang-0.v5l-1010-sft-full-sglang.yuchen.svc.max.x.ai to sample 10 rubrics.\n",
            "                          id  \\\n",
            "0   claude_maphf_turn0-idx68   \n",
            "1  claude_maphf_turn0-idx100   \n",
            "2  claude_maphf_turn0-idx133   \n",
            "3  claude_maphf_turn0-idx199   \n",
            "\n",
            "                                        conversation  \\\n",
            "0  [{'content': 'Construct a Python function that...   \n",
            "1  [{'content': 'table name: table_11404452_1\n",
            "|  ...   \n",
            "2  [{'content': 'Develop a complex, quintuple-dim...   \n",
            "3  [{'content': 'Explain the concept of memoizati...   \n",
            "\n",
            "                                    conversation_str  \\\n",
            "0  Human: Construct a Python function that identi...   \n",
            "1  Human: table name: table_11404452_1\\n|    |   ...   \n",
            "2  Human: Develop a complex, quintuple-dimensiona...   \n",
            "3  Human: Explain the concept of memoization and ...   \n",
            "\n",
            "                                            metadata  \n",
            "0  {'rubric_verifier': {'rubric_gen_templates_ver...  \n",
            "1  {'rubric_verifier': {'rubric_gen_templates_ver...  \n",
            "2  {'rubric_verifier': {'rubric_gen_templates_ver...  \n",
            "3  {'rubric_verifier': {'rubric_gen_templates_ver...  \n",
            "Existing? /data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics/mapcf_claude_40k/part-00000-of-00032.parquet True\n",
            "\u001b[2m[2024-10-28 22:44:55,754 I] \u001b[2;36m[colabbox-0:643256] 3482830111:57:\u001b[0m Finished processing parquet file: /data/datasets/sft-data/staging/common_filter_081324_fix_cutoff/mapcf_claude_40k/part-00000-of-00032.parquet, saving to /data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics/mapcf_claude_40k/part-00000-of-00032.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'update'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m process_parquet_file(\n\u001b[1;32m      2\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff/mapcf_claude_40k/part-00000-of-00032.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics/mapcf_claude_40k/part-00000-of-00032.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m       verifier \u001b[38;5;241m=\u001b[39m VerifierRubricsMap(),\n\u001b[1;32m      5\u001b[0m       pbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[12], line 58\u001b[0m, in \u001b[0;36mprocess_parquet_file\u001b[0;34m(parquet_file_path, output_file_path, verifier, pbar)\u001b[0m\n\u001b[1;32m     56\u001b[0m row_pbar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     57\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished processing parquet file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, saving to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43mpbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_root = '/data/datasets/preferences-v2/raw/renew/'\n",
        "# output_root = '/data/datasets/preferences-v2/raw/domain/output.parquet'\n",
        "# output_train_root = '/data/datasets/preferences-v2/raw/domain/train.parquet'\n",
        "# output_eval_root = '/data/datasets/preferences-v2/raw/domain/eval.parquet'\n",
        "\n",
        "\n",
        "# def structure_to_domain_classification(example):\n",
        "#   remove_asst_last = example['conversation0'][:-1].tolist()\n",
        "#   convo = remove_asst_last + [{'role': \"ASSIS\", \"content\": example[\"domain\"].lower()}]\n",
        "#   conv_loss_mask = [False for _ in remove_asst_last] + [True]\n",
        "#   example[\"domain\"] = example[\"domain\"].lower()\n",
        "#   example[\"conversation\"] = convo\n",
        "#   example[\"conv_loss_mask\"] = conv_loss_mask\n",
        "#   return example\n",
        "\n",
        "# all_df = []\n",
        "# for root, dirs, files in os.walk(input_root, followlinks=False):\n",
        "#   if len(dirs) == 0:\n",
        "#     for file in files:\n",
        "#       if file.endswith('.parquet'):\n",
        "#         path = (os.path.join(root, file))\n",
        "#         df = utils.read_df(path)\n",
        "#         all_df.append(df)\n",
        "# all_df = pd.concat(all_df)\n",
        "# all_df = all_df.apply(structure_to_domain_classification, axis=1)\n",
        "\n",
        "# all_df = all_df.sample(frac=1).reset_index(drop=True)\n",
        "# train_size = int(0.8 * len(all_df))\n",
        "\n",
        "# # Split the DataFrame\n",
        "# train_data = all_df.iloc[:train_size]\n",
        "# test_data = all_df.iloc[train_size:]\n",
        "\n",
        "\n",
        "# print(\"Training data shape:\", train_data.shape)\n",
        "# print(\"Testing data shape:\", test_data.shape)\n",
        "\n",
        "# train_data.to_parquet(output_train_root)\n",
        "# test_data.to_parquet(output_eval_root)\n",
        "\n"
      ],
      "metadata": {
        "id": "UECv4vPVOaP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from tqdm import tqdm  # Use normal tqdm for manual progress\n",
        "\n",
        "from xlm.posttrain.data.preprocess.map_custom import VerifierRubricsMap\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "\n",
        "INPUT_ROOT_RUBRIC = \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics\"\n",
        "INPUT_ROOT_REPETIION = \"data/datasets/sft-data/staging/common_filter_102824_classifier\"\n",
        "\n",
        "OUTPUT_ROOT = \"/data/datasets/sft-data/staging/common_filter_102924_fix_cutoff_w_rubrics_repetition\"\n",
        "\n",
        "\n",
        "# Process each parquet file and apply async map function\n",
        "async def process_parquet_file(parquet_file_path1, parquet_file_path2,  output_file_path=None, pbar):\n",
        "    df1 = pq.read_table(parquet_file_path1)\n",
        "    df2 = pq.read_table(parquet_file_path2)\n",
        "    df_merged = pd.concat([df1, df2], axis=1, keys=['id', 'conversation'])\n",
        "    if output_file_path:\n",
        "      df_merged.to_parquet(output_file_path)\n",
        "    if pbar:\n",
        "      pbar.update(1)  # Update the progress bar for the file\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "# Resolve symbolic links recursively\n",
        "def resolve_symlink(path):\n",
        "    \"\"\"Resolve symbolic links recursively.\"\"\"\n",
        "    while os.path.islink(path):\n",
        "        target_path = os.readlink(path)\n",
        "        # If target_path is relative, it needs to be resolved with the base directory.\n",
        "        path = os.path.join(os.path.dirname(path), target_path)\n",
        "    return os.path.abspath(path)\n",
        "\n",
        "\n",
        "# Semaphore-limited file processing\n",
        "def process_directory(input_root, output_root):\n",
        "    os.makedirs(output_root, exist_ok=True)\n",
        "    for root, dirs, files in os.walk(\n",
        "        input_root, followlinks=False\n",
        "    ):  # Iterate through all directories and files\n",
        "        output_dir = output_root\n",
        "\n",
        "        if dirs:\n",
        "            for dir in dirs:\n",
        "                os.makedirs(root+\"/\" + dir, exist_ok=True)\n",
        "        else:\n",
        "            for file in files:\n",
        "                input_file_rubrics = root + \"/\" + file\n",
        "\n",
        "\n",
        "        # Process each directory, including symbolic links\n",
        "        for dir_name in dirs[::-1]:\n",
        "            print(f\"total_rows = {total_rows}, total_rows_done = {total_rows_done}, total_rows_2bdone = {total_rows_2bdone}\")\n",
        "            if \"DETAIL\" in dir_name:\n",
        "                continue\n",
        "            else:\n",
        "                print(\"DIR \", dir_name)\n",
        "\n",
        "            dir_path = os.path.join(root, dir_name)\n",
        "            resolved_dir = dir_path\n",
        "            if os.path.islink(dir_path):\n",
        "                resolved_dir = resolve_symlink(dir_path)\n",
        "                logging.info(f\"Directory {dir_path} is a symbolic link to {resolved_dir}\")\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "            # Walk the resolved directory and process its contents\n",
        "            for linked_root, linked_dirs, linked_files in os.walk(resolved_dir):\n",
        "                # relative_linked_path = os.path.relpath(linked_root, resolved_dir)\n",
        "                output_linked_dir = os.path.join(output_dir, dir_name)\n",
        "                os.makedirs(output_linked_dir, exist_ok=True)\n",
        "\n",
        "                for linked_file in linked_files:\n",
        "                    input_file = os.path.join(linked_root, linked_file)\n",
        "                    output_file = os.path.join(output_linked_dir, linked_file)\n",
        "\n",
        "                    # Check if the raw file has already been processed\n",
        "                    raw_output_file = os.path.join(\n",
        "                        OUTPUT_ROOT, os.path.relpath(input_file, INPUT_ROOT)\n",
        "                    )\n",
        "                    if not os.path.exists(input_file):\n",
        "                        # logging.warning(f\"File does not exist: {input_file}\")\n",
        "                        continue\n",
        "                    rows = 0\n",
        "                    try:\n",
        "                        first_col = pq.read_schema(input_file).names[0]\n",
        "                        rows = (\n",
        "                            len(pq.read_table(input_file, columns=[first_col]))\n",
        "                            if input_file.endswith(\".parquet\")\n",
        "                            else 0\n",
        "                        )\n",
        "                    except Exception as e:\n",
        "                        print(\"Wrong file?\", e, input_file)\n",
        "\n",
        "                    # print(input_file, output_file, raw_output_file)\n",
        "\n",
        "                    if os.path.exists(output_file):\n",
        "                        # logging.info(f\"File {output_file} has exist, skip. \")\n",
        "                        total_rows += rows\n",
        "                        total_rows_done += rows\n",
        "                        continue\n",
        "\n",
        "                    if os.path.exists(raw_output_file):\n",
        "                        # If the file has already been processed, copy it to the symbolic link directory\n",
        "                        # logging.info(f\"File {raw_output_file} has already been processed. Copying to {output_file}\")\n",
        "                        shutil.copy2(raw_output_file, output_file)\n",
        "                        total_rows += rows\n",
        "                        total_rows_done += rows\n",
        "                        continue\n",
        "\n",
        "                    if input_file.endswith(\".parquet\"):\n",
        "                        if any([x in input_file for x in NO_RUBRIC_SET]) or dir_name not in dataset_names:\n",
        "                            shutil.copy2(input_file, output_file)\n",
        "                            total_rows += rows\n",
        "                            total_rows_done += rows\n",
        "                        elif os.path.exists(output_file.replace(\".parquet\", \"cp.parquet\")):\n",
        "                            # logging.info(\"exists file of copy, \", output_file)\n",
        "                            total_rows += rows\n",
        "                            total_rows_done += rows\n",
        "                            continue\n",
        "                        else:\n",
        "                            total_rows += rows\n",
        "                            total_rows_2bdone += rows\n",
        "\n",
        "                            total_files += 1\n",
        "                            total_files_input_output += [(input_file, output_file)]\n",
        "\n",
        "\n",
        "        with tqdm(total=total_files, desc=\"Processing Files\") as pbar:\n",
        "            for input_file, output_file in total_files_input_output:\n",
        "                # if semaphore.locked()  and len(active_tasks) >= 10:\n",
        "                #     done, active_tasks = await asyncio.wait(active_tasks, return_when=asyncio.FIRST_COMPLETED)\n",
        "                #     pbar.update(len(done))\n",
        "\n",
        "                task = asyncio.create_task(\n",
        "                    process_with_semaphore(\n",
        "                        input_file,\n",
        "                        output_file,\n",
        "                        verifier,\n",
        "                        semaphore,\n",
        "                        pbar\n",
        "                    )\n",
        "                )\n",
        "                active_tasks.append(task)\n",
        "\n",
        "        # Wait for all tasks to complete\n",
        "        await asyncio.gather(*active_tasks)\n",
        "\n",
        "\n",
        "process_directory()"
      ],
      "metadata": {
        "id": "GHBBlIYgxMKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "INPUT_ROOT_RUBRIC = \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics/\"\n",
        "INPUT_ROOT_REPETIION = \"/data/datasets/sft-data/staging/common_filter_102824_classifier/\"\n",
        "\n",
        "OUTPUT_ROOT = \"/data/datasets/sft-data/staging/common_filter_102924_fix_cutoff_w_rubrics_repetition/\"\n",
        "\n",
        "\n",
        "def to_parquet(df: pd.DataFrame, path: str, chunks: int = 128):\n",
        "    path = Path(path)\n",
        "    chunksize = len(df) // chunks\n",
        "    dfs = [\n",
        "        df.iloc[i : i + chunksize]\n",
        "        for i in tqdm.tqdm(range(0, len(df), chunksize), desc=\"to parquet\")\n",
        "    ]\n",
        "    os.makedirs(str(path), exist_ok=True)\n",
        "    for i, df in tqdm.tqdm(enumerate(dfs), desc=\"storing df as parquets\"):\n",
        "        df.reset_index(drop=True).to_parquet(str(path / f\"chunk_{i:05}_of_{chunks:05}.parquet\"))\n",
        "\n",
        "def process_parquet_file(test_folder, pbar=None):\n",
        "    path1 = INPUT_ROOT_RUBRIC + test_folder\n",
        "    path2 = INPUT_ROOT_REPETIION + test_folder\n",
        "    path_output = OUTPUT_ROOT + test_folder\n",
        "\n",
        "    parquets1 = glob.glob(path1)\n",
        "    chunks = int(os.listdir(path1)[0].split('/')[-1].split('.')[0].split(\"-\")[-1])\n",
        "    print(chunks)\n",
        "    df1 = pd.concat([pd.read_parquet(p) for p in parquets1])\n",
        "\n",
        "    parquets2 = glob.glob(path2)\n",
        "    df2 = pd.concat([pd.read_parquet(p) for p in parquets2])\n",
        "\n",
        "    # Merge the dataframes\n",
        "    merged_df = pd.merge(df1, df2, on='id', suffixes=('_df1', '_df2'))\n",
        "\n",
        "    # Automatically keep only one copy of duplicated columns\n",
        "    merged_df = merged_df.loc[:, merged_df.columns.str.endswith('_df1') | ~merged_df.columns.str.contains('_df2')]  # This keeps df1 columns\n",
        "\n",
        "    # Optionally, remove the '_df1' suffix\n",
        "    merged_df.columns = merged_df.columns.str.replace('_df1', '')\n",
        "    merged_df\n",
        "\n",
        "    if path_output:\n",
        "      to_parquet(merged_df, path_output, chunks)\n",
        "    if pbar:\n",
        "      pbar.update(1)  # Update the progress bar for the file\n",
        "    return merged_df\n",
        "\n",
        "process_parquet_file('openchat_xai')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWiuztablIzo",
        "outputId": "d60bac45-e17f-4b50-8492-2d626cee94e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "to parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 140/140 [00:00<00:00, 76329.46it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "storing df as parquets: 0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "storing df as parquets: 19it [00:00, 186.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "storing df as parquets: 39it [00:00, 192.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "storing df as parquets: 59it [00:00, 194.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "storing df as parquets: 79it [00:00, 193.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "storing df as parquets: 99it [00:00, 194.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "storing df as parquets: 119it [00:00, 196.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "storing df as parquets: 140it [00:00, 194.81it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                       conversation  \\\n",
              "0    189069  [{'content': 'Give me a recipe for refried bea...   \n",
              "1    189466  [{'content': 'explain zero knowledge proof as ...   \n",
              "2    189824  [{'content': 'You will fulfil the required rol...   \n",
              "3    190264  [{'content': 'Explain what Alvin Toffler meant...   \n",
              "4    190730  [{'content': 'Write scheduled instructions to ...   \n",
              "..      ...                                                ...   \n",
              "694  188540  [{'content': 'Author a reddit me\\_irl post tha...   \n",
              "695  188989  [{'content': 'you are an expert at editing poo...   \n",
              "696  189713  [{'content': 'Be my career coach. Throughout t...   \n",
              "697  190165  [{'content': 'Can you write a resume for anest...   \n",
              "698  191057  [{'content': '37-42: For these questions, choo...   \n",
              "\n",
              "                                      conversation_str  \\\n",
              "0    Human: Give me a recipe for refried beans writ...   \n",
              "1    Human: explain zero knowledge proof as Tim Rou...   \n",
              "2    Human: You will fulfil the required role: Busi...   \n",
              "3    Human: Explain what Alvin Toffler meant when h...   \n",
              "4    Human: Write scheduled instructions to make th...   \n",
              "..                                                 ...   \n",
              "694  Human: Author a reddit me\\_irl post that could...   \n",
              "695  Human: you are an expert at editing poorly wri...   \n",
              "696  Human: Be my career coach. Throughout the conv...   \n",
              "697  Human: Can you write a resume for anesthesiolo...   \n",
              "698  Human: 37-42: For these questions, choose the ...   \n",
              "\n",
              "                                              metadata  \\\n",
              "0    {'rubric_verifier': {'reward': 1.0, 'rubric_ge...   \n",
              "1    {'rubric_verifier': {'reward': 0.0, 'rubric_ge...   \n",
              "2    {'rubric_verifier': {'reward': 1.0, 'rubric_ge...   \n",
              "3    {'rubric_verifier': {'reward': 1.0, 'rubric_ge...   \n",
              "4    {'rubric_verifier': {'reward': 1.0, 'rubric_ge...   \n",
              "..                                                 ...   \n",
              "694  {'rubric_verifier': {'reward': 1.0, 'rubric_ge...   \n",
              "695  {'rubric_verifier': {'reward': 0.5, 'rubric_ge...   \n",
              "696  {'rubric_verifier': {'reward': 1.0, 'rubric_ge...   \n",
              "697  {'rubric_verifier': {'reward': 1.0, 'rubric_ge...   \n",
              "698  {'rubric_verifier': {'reward': 0.5, 'rubric_ge...   \n",
              "\n",
              "                                  repetitive/reasoning  \\\n",
              "0    [### Step 1: Summarize the User Query and AI's...   \n",
              "1    [### Step 1: Summarize the User Query and AI's...   \n",
              "2    [### Step 1: Summarize the User Query and AI's...   \n",
              "3    [### Step 1: Summarize the User Query and AI's...   \n",
              "4    [### Step 1: Summarize the User Query and AI's...   \n",
              "..                                                 ...   \n",
              "694  [### Step 1: Summarize the User Query and AI's...   \n",
              "695  [### Step 1: Summarize the User Query and AI's...   \n",
              "696  [### Step 1: Summarize the User Query and AI's...   \n",
              "697  [### Step 1: Summarize the User Query and AI's...   \n",
              "698  [### Step 1: Summarize the User Query and AI's...   \n",
              "\n",
              "                                      repetitive/label  \n",
              "0        [no repetition, no repetition, no repetition]  \n",
              "1    [no repetition, no repetition, no repetition, ...  \n",
              "2                                      [no repetition]  \n",
              "3                       [no repetition, no repetition]  \n",
              "4                                      [no repetition]  \n",
              "..                                                 ...  \n",
              "694                                    [no repetition]  \n",
              "695                                    [no repetition]  \n",
              "696  [no repetition, echoing user query, no repetit...  \n",
              "697                                    [no repetition]  \n",
              "698                                    [no repetition]  \n",
              "\n",
              "[699 rows x 6 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>conversation</th>\n",
              "      <th>conversation_str</th>\n",
              "      <th>metadata</th>\n",
              "      <th>repetitive/reasoning</th>\n",
              "      <th>repetitive/label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>189069</td>\n",
              "      <td>[{'content': 'Give me a recipe for refried bea...</td>\n",
              "      <td>Human: Give me a recipe for refried beans writ...</td>\n",
              "      <td>{'rubric_verifier': {'reward': 1.0, 'rubric_ge...</td>\n",
              "      <td>[### Step 1: Summarize the User Query and AI's...</td>\n",
              "      <td>[no repetition, no repetition, no repetition]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>189466</td>\n",
              "      <td>[{'content': 'explain zero knowledge proof as ...</td>\n",
              "      <td>Human: explain zero knowledge proof as Tim Rou...</td>\n",
              "      <td>{'rubric_verifier': {'reward': 0.0, 'rubric_ge...</td>\n",
              "      <td>[### Step 1: Summarize the User Query and AI's...</td>\n",
              "      <td>[no repetition, no repetition, no repetition, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>189824</td>\n",
              "      <td>[{'content': 'You will fulfil the required rol...</td>\n",
              "      <td>Human: You will fulfil the required role: Busi...</td>\n",
              "      <td>{'rubric_verifier': {'reward': 1.0, 'rubric_ge...</td>\n",
              "      <td>[### Step 1: Summarize the User Query and AI's...</td>\n",
              "      <td>[no repetition]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>190264</td>\n",
              "      <td>[{'content': 'Explain what Alvin Toffler meant...</td>\n",
              "      <td>Human: Explain what Alvin Toffler meant when h...</td>\n",
              "      <td>{'rubric_verifier': {'reward': 1.0, 'rubric_ge...</td>\n",
              "      <td>[### Step 1: Summarize the User Query and AI's...</td>\n",
              "      <td>[no repetition, no repetition]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>190730</td>\n",
              "      <td>[{'content': 'Write scheduled instructions to ...</td>\n",
              "      <td>Human: Write scheduled instructions to make th...</td>\n",
              "      <td>{'rubric_verifier': {'reward': 1.0, 'rubric_ge...</td>\n",
              "      <td>[### Step 1: Summarize the User Query and AI's...</td>\n",
              "      <td>[no repetition]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>188540</td>\n",
              "      <td>[{'content': 'Author a reddit me\\_irl post tha...</td>\n",
              "      <td>Human: Author a reddit me\\_irl post that could...</td>\n",
              "      <td>{'rubric_verifier': {'reward': 1.0, 'rubric_ge...</td>\n",
              "      <td>[### Step 1: Summarize the User Query and AI's...</td>\n",
              "      <td>[no repetition]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>188989</td>\n",
              "      <td>[{'content': 'you are an expert at editing poo...</td>\n",
              "      <td>Human: you are an expert at editing poorly wri...</td>\n",
              "      <td>{'rubric_verifier': {'reward': 0.5, 'rubric_ge...</td>\n",
              "      <td>[### Step 1: Summarize the User Query and AI's...</td>\n",
              "      <td>[no repetition]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>189713</td>\n",
              "      <td>[{'content': 'Be my career coach. Throughout t...</td>\n",
              "      <td>Human: Be my career coach. Throughout the conv...</td>\n",
              "      <td>{'rubric_verifier': {'reward': 1.0, 'rubric_ge...</td>\n",
              "      <td>[### Step 1: Summarize the User Query and AI's...</td>\n",
              "      <td>[no repetition, echoing user query, no repetit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697</th>\n",
              "      <td>190165</td>\n",
              "      <td>[{'content': 'Can you write a resume for anest...</td>\n",
              "      <td>Human: Can you write a resume for anesthesiolo...</td>\n",
              "      <td>{'rubric_verifier': {'reward': 1.0, 'rubric_ge...</td>\n",
              "      <td>[### Step 1: Summarize the User Query and AI's...</td>\n",
              "      <td>[no repetition]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>698</th>\n",
              "      <td>191057</td>\n",
              "      <td>[{'content': '37-42: For these questions, choo...</td>\n",
              "      <td>Human: 37-42: For these questions, choose the ...</td>\n",
              "      <td>{'rubric_verifier': {'reward': 0.5, 'rubric_ge...</td>\n",
              "      <td>[### Step 1: Summarize the User Query and AI's...</td>\n",
              "      <td>[no repetition]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>699 rows × 6 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbousC4DmD-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d1SKP1hCo4sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "INPUT_ROOT = '/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff/'\n",
        "INPUT_ROOT_RUBRIC = \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff_w_rubrics/\"\n",
        "INPUT_ROOT_REPETIION = \"/data/datasets/sft-data/staging/common_filter_102824_classifier/\"\n",
        "\n",
        "OUTPUT_ROOT = \"/data/datasets/sft-data/staging/common_filter_102924_fix_cutoff_w_rubrics_repetition/\"\n",
        "\n",
        "\n",
        "all_folders = len(os.listdir(INPUT_ROOT))\n",
        "pbar = tqdm(total=all_folders, desc=\"Processing Folders\")\n",
        "for dir in os.listdir(INPUT_ROOT):\n",
        "    if \"DETAIL\" in dir:\n",
        "        continue\n",
        "\n",
        "    if os.path.exists(OUTPUT_ROOT + dir):\n",
        "        continue\n",
        "    if os.path.exists(INPUT_ROOT_RUBRIC + dir) and os.path.exists(INPUT_ROOT_REPETIION + dir):\n",
        "        process_parquet_file(dir, pbar)\n",
        "    elif os.path.exists(INPUT_ROOT_RUBRIC + dir):\n",
        "        shutil.copytree(INPUT_ROOT_RUBRIC + dir, OUTPUT_ROOT + dir)\n",
        "    elif os.path.exists(INPUT_ROOT_REPETIION + dir):\n",
        "        shutil.copytree(INPUT_ROOT_REPETIION + dir, OUTPUT_ROOT + dir)\n",
        "    elif os.path.exists(INPUT_ROOT + dir):\n",
        "        shutil.copytree(INPUT_ROOT + dir, OUTPUT_ROOT + dir)\n",
        "    else:\n",
        "        print(\"not existing dir?\", INPUT_ROOT + dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ0S-Kv7o5P5",
        "outputId": "d28c7126-9e01-44f9-abeb-fe483edc4a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Folders:   0%|                                                                                                                                | 0/127 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not existing dir? /data/datasets/sft-data/staging/common_filter_081324_fix_cutoff/sampled_function_calling_data_10000_v12_json_aug_ToolAlpaca\n",
            "not existing dir? /data/datasets/sft-data/staging/common_filter_081324_fix_cutoff/sampled_function_calling_data_10000_v12_json_aug_ToolBench\n",
            "32\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'tqdm' has no attribute 'tqdm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[104], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(INPUT_ROOT_RUBRIC \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mdir\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(INPUT_ROOT_REPETIION \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mdir\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mprocess_parquet_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(INPUT_ROOT_RUBRIC \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mdir\u001b[39m):\n\u001b[1;32m     19\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopytree(INPUT_ROOT_RUBRIC \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mdir\u001b[39m, OUTPUT_ROOT \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mdir\u001b[39m)\n",
            "Cell \u001b[0;32mIn[81], line 47\u001b[0m, in \u001b[0;36mprocess_parquet_file\u001b[0;34m(test_folder, pbar)\u001b[0m\n\u001b[1;32m     44\u001b[0m merged_df\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_output:\n\u001b[0;32m---> 47\u001b[0m   \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[1;32m     49\u001b[0m   pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Update the progress bar for the file\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[85], line 82\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, chunks)\u001b[0m\n\u001b[1;32m     78\u001b[0m path \u001b[38;5;241m=\u001b[39m Path(path)\n\u001b[1;32m     79\u001b[0m chunksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m chunks \n\u001b[1;32m     80\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m     df\u001b[38;5;241m.\u001b[39miloc[i : i \u001b[38;5;241m+\u001b[39m chunksize]\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df), chunksize), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m ]\n\u001b[1;32m     84\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mstr\u001b[39m(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, df \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(dfs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring df as parquets\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'tqdm' has no attribute 'tqdm'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import asyncio\n",
        "# import glob\n",
        "# import logging\n",
        "# import os\n",
        "# from pathlib import Path\n",
        "# import shutil\n",
        "\n",
        "# import pandas as pd\n",
        "# import pyarrow as pa\n",
        "# import pyarrow.parquet as pq\n",
        "# from tqdm import tqdm  # Use normal tqdm for manual progress\n",
        "\n",
        "# from xlm.posttrain.data.preprocess.map_custom import VerifierRubricsMap\n",
        "\n",
        "# # Configure logging\n",
        "# logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# chat_old = [  # general chat\n",
        "#     (\"ambiguous_query_train_sft\"),\n",
        "#     (\"xai_instruct_0426\"),\n",
        "#     (\"xai_instruct\"),\n",
        "#     (\"alpaca_exclude_rewrite\"),\n",
        "#     (\"chatgpt_exports_all\"),\n",
        "#     (\"wildchat_imperative\"),\n",
        "#     (\"wildchat_requirements\"),\n",
        "#     (\"openchat_xai\"),\n",
        "#     (\"lmsys_xai\"),\n",
        "#     (\"ultrafeed_wizardlm\"),\n",
        "#     (\"ultrafeed_sharegpt\"),\n",
        "#     (\"capybara\"),\n",
        "#     (\"wildchat_multiturn\"),\n",
        "# ]\n",
        "# chat = chat_old + [\n",
        "#     # general chat\n",
        "#     (\"format_sft_lmsys\"),\n",
        "#     (\"multi_turn_gpt_0-50_filter_filterrefusal_f\"),\n",
        "#     (\"multi_turn_gpt_50-100_filter_filterrefusal_f\"),\n",
        "# ]\n",
        "# chat_nofmt = chat_old + [\n",
        "#     (\"multi_turn_gpt_0-50_filter_filterrefusal_f\"),\n",
        "#     (\"multi_turn_gpt_50-100_filter_filterrefusal_f\"),\n",
        "# ]\n",
        "\n",
        "# shared_code = [\n",
        "#     # coding\n",
        "#     (\"sft_proc_k4\"),\n",
        "#     (\"cc_selfrepair\"),\n",
        "#     (\"col_r_on_competitive_coding_proc\"),\n",
        "#     (\"stacex_vendor_lmsyslike_top20k_proc\"),\n",
        "#     (\"mbpp_star\"),\n",
        "#     (\"apps_cld_refresh\"),\n",
        "# ]\n",
        "# code_juice = [\n",
        "#     (\"selfexec_p25\"),\n",
        "#     (\"dry_run_mbpp\"),\n",
        "#     (\"synthetic_humaneval_cot\"),\n",
        "#     (\"leetcode_devins\"),\n",
        "#     (\"leetcode_direct\"),\n",
        "#     (\"bigcodebench_direct\"),\n",
        "# ]\n",
        "# code_safe = shared_code + [\n",
        "#     (\"claude_on_removed_prompts\"),\n",
        "#     (\"claude3_5_codegen\"),\n",
        "#     (\"mapcf_claude_40k\"),\n",
        "# ]\n",
        "\n",
        "# dataset_names = chat_nofmt + code_safe + code_juice\n",
        "\n",
        "\n",
        "# INPUT_ROOT = \"/data/datasets/sft-data/staging/common_filter_081324_fix_cutoff/\"\n",
        "# OUTPUT_ROOT = \"/data/datasets/sft-data/staging/common_filter_102924_fix_cutoff_w_rubrics/\"\n",
        "# MAX_CONCURRENT_FILES = 4  # Set the maximum number of files to process at the same time\n",
        "\n",
        "# NO_RUBRIC_SET = [\"custom\", \"strawberry\", \"vibes\", \"system\", \"identity\"]\n",
        "\n",
        "\n",
        "# def to_parquet(df: pd.DataFrame, path: str, chunks: int = 128):\n",
        "#     path = Path(path)\n",
        "#     chunksize = len(df) // chunks\n",
        "#     dfs = [\n",
        "#         df.iloc[i : i + chunksize]\n",
        "#         for i in tqdm.tqdm(range(0, len(df), chunksize), desc=\"to parquet\")\n",
        "#     ]\n",
        "#     os.makedirs(str(path), exist_ok=True)\n",
        "#     for i, df in tqdm.tqdm(enumerate(dfs), desc=\"storing df as parquets\"):\n",
        "#         df.reset_index(drop=True).to_parquet(str(path / f\"chunk_{i:05}_of_{chunks:05}.parquet\"))\n"
      ],
      "metadata": {
        "id": "Nkh2uPWFo54T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# async def process_parquet_folder(input_path, folder, verifier, semaphore, pbar=None):\n",
        "#     path_output = OUTPUT_ROOT + folder\n",
        "\n",
        "#     parquets1 = glob.glob(input_path)\n",
        "#     chunks = int(os.listdir(input_path)[0].split('/')[-1].split('.')[0].split(\"-\")[-1])\n",
        "#     print(chunks)\n",
        "#     df1 = pd.concat([pd.read_parquet(p) for p in parquets1])\n",
        "\n",
        "\n",
        "#     row_pbar = tqdm(\n",
        "#         total=len(df1), desc=f\"Processing {folder}\", leave=False\n",
        "#     )\n",
        "\n",
        "#     async def process_row(example):\n",
        "#         result = await verifier.map(example)\n",
        "#         row_pbar.update(1)  # Update row progress bar for each row\n",
        "#         return result\n",
        "\n",
        "#     # Parallelize row-level processing\n",
        "#     mapped_data = await asyncio.gather(*(process_row(example) for _, example in df1.iterrows()))\n",
        "#     processed_df = pd.DataFrame(mapped_data)\n",
        "\n",
        "#     if path_output:\n",
        "#       to_parquet(processed_df, path_output, chunks)\n",
        "#     if pbar:\n",
        "#       pbar.update(1)  # Update the progress bar for the file\n",
        "\n",
        "#     return len(df1)\n",
        "\n",
        "\n",
        "# # Resolve symbolic links recursively\n",
        "# def resolve_symlink(path):\n",
        "#     \"\"\"Resolve symbolic links recursively.\"\"\"\n",
        "#     while os.path.islink(path):\n",
        "#         target_path = os.readlink(path)\n",
        "#         # If target_path is relative, it needs to be resolved with the base directory.\n",
        "#         path = os.path.join(os.path.dirname(path), target_path)\n",
        "#     return os.path.abspath(path)\n",
        "\n",
        "\n",
        "# # Semaphore-limited file processing\n",
        "# async def process_directory(input_root, output_root):\n",
        "#     os.makedirs(output_root, exist_ok=True)\n",
        "\n",
        "#     # Initialize the verifier inside the event loop\n",
        "#     verifier = VerifierRubricsMap()\n",
        "\n",
        "#     # Limit of concurrently running tasks (manual queue management)\n",
        "#     active_tasks = []\n",
        "\n",
        "#     # Semaphore to limit concurrent file processing\n",
        "#     semaphore = asyncio.Semaphore(MAX_CONCURRENT_FILES)\n",
        "#     pbar = tqdm(total=len(dirs))\n",
        "\n",
        "\n",
        "#     # Task to process a file with semaphore\n",
        "#     async def process_with_semaphore(input_folder, folder, verifier, semaphore, pbar):\n",
        "#         async with semaphore:\n",
        "#             await process_parquet_folder(input_folder, folder, verifier, semaphore, pbar)\n",
        "\n",
        "\n",
        "#     # Process each directory, including symbolic links\n",
        "#     for dir_name in dataset_names[::-1]:\n",
        "#         dir_path = os.path.join(INPUT_ROOT, dir_name)\n",
        "#         resolved_dir = dir_path\n",
        "#         print(resolved_dir)\n",
        "#         if os.path.exists(OUTPUT_ROOT + \"/\" + dir_name):\n",
        "#           continue\n",
        "#         task = asyncio.create_task(\n",
        "#             process_with_semaphore(\n",
        "#                 resolved_dir,\n",
        "#                 dir_name,\n",
        "#                 verifier,\n",
        "#                 semaphore,\n",
        "#                 pbar,\n",
        "#             )\n",
        "#         )\n",
        "#         active_tasks.append(task)\n",
        "#     await asyncio.gather(*active_tasks)\n",
        "\n",
        "# # Entry point for running the async code\n",
        "# async def main():\n",
        "#     await process_directory(INPUT_ROOT, OUTPUT_ROOT)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     asyncio.run(main())\n",
        "\n",
        "# \"\"\"\n",
        "# python $XAI_ROOT/train/xlm/posttrain/data/scripts/merge_staging_20241029_rubrics_repetition.py\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "lHhb7MDLqfIY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# await process_directory(INPUT_ROOT, OUTPUT_ROOT)"
      ],
      "metadata": {
        "id": "M-gUWQbS-Bm9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bPasMF7htpb-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERIijK-uzzpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PmOnVb_4AaaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Reward Generaor 10/31"
      ],
      "metadata": {
        "id": "u8Z9hdGxAdZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "import asyncio\n",
        "import logging\n",
        "import math\n",
        "from enum import Enum\n",
        "from typing import Any, Dict, List, TypedDict\n",
        "\n",
        "from xlm.config import Config, configclass\n",
        "from xlm.posttrain.data import formatting\n",
        "from xlm.sampling_client import SamplingClient\n",
        "from xlm.tokenizers.constants import *\n",
        "\n",
        "MESSAGEType = List[Dict[str, str]]\n",
        "\n",
        "\n",
        "class RubricType(Enum):\n",
        "    NONE = 0\n",
        "    CRITIC = 1\n",
        "    ASPECT = 2\n",
        "    CRITIC_ASPECT = 3\n",
        "    SYNTHESIS = 4\n",
        "\n",
        "\n",
        "class ContextualResult(TypedDict):\n",
        "    reward: float | None\n",
        "    reward_truth: float | None\n",
        "    rubrics: List[str]\n",
        "    conversation: MESSAGEType\n",
        "    convo_loss_mask: List[bool]\n",
        "    example: Dict[str, Any]\n",
        "\n",
        "\n",
        "class ContextualPairedResult(TypedDict):\n",
        "    reward: dict[str, float] | None\n",
        "    reward_truth: dict[str, float] | None\n",
        "    rubrics: dict[str, List[str]]\n",
        "    example: Dict[str, Any]\n",
        "\n",
        "\n",
        "make_user_msg = lambda s: {\"role\": \"HUMAN\", \"content\": s}\n",
        "make_assistant_msg = lambda s: {\"role\": \"ASSIS\", \"content\": s}\n",
        "\n",
        "RATINGS = {\"A\": \"Great\", \"B\": \"Good\", \"C\": \"OK\", \"D\": \"Bad\", \"E\": \"Terrible\"}\n",
        "RATINGS_KEY = [\" \"] + list(RATINGS.keys())\n",
        "RATINGS_STR = \", \".join(f\"{k}-{v}\" for k, v in RATINGS.items())\n",
        "\n",
        "RATING_USER_PROMPT_RUBRICS = \"Given the context and response, your task is to generate critic about the instructions to follow or criteria would distinguish good responses from bad responses.\"\n",
        "\n",
        "RATING_USER_PROMPT = f\"\"\"Please rate the previous assistant response using one of the following options, listed from highest to lowest quality: {RATINGS_STR}.\"\"\"\n",
        "\n",
        "RATING_USER_PROMPT_OVERALL = RATING_USER_PROMPT + \" Provide only the rating as a single letter.\"\n",
        "# \"Horrible\": -2, \"Pretty bad\": -1, \"Okay\": 0, \"Pretty good\": 1, \"Amazing\": 2},\n",
        "\n",
        "RATING_USER_PROMPT_ASPECT = (\n",
        "    RATING_USER_PROMPT + \" For each aspect in {aspects} provide only the rating as a single letter.\"\n",
        ")\n",
        "\n",
        "\n",
        "DEFAULT_MODEL = \"v5l-1101-crm.yuchen.svc.max.x.ai\"\n",
        "# DEFAULT_MODEL = \"v5l-1024-crm-critic-sglang.yuchen.svc.max.x.ai\"\n",
        "\n",
        "DEFAULT_ASPECTS = [\n",
        "    \"quality\",\n",
        "    \"instruction following\",\n",
        "    \"truthfulness\",\n",
        "    \"writing style\",\n",
        "    \"verbosity\",\n",
        "]\n",
        "\n",
        "\n",
        "##@configclass\n",
        "class ContextualRewardGenerator(Config, abc.ABC):\n",
        "    \"\"\"Base class for contextual based reward configurations.\"\"\"\n",
        "\n",
        "    max_retry: int = 1\n",
        "    num_samples: int = 1\n",
        "    rubrics_type: RubricType = RubricType.NONE\n",
        "    train_rubrics: bool = True\n",
        "\n",
        "    def initialize(\n",
        "        self,\n",
        "        client: SamplingClient | None = None,\n",
        "        model: str | None = None,\n",
        "        timeout: int | None = 3600,\n",
        "        **sampling_args,\n",
        "    ):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.sampling_args = {\n",
        "            \"seed\": sampling_args.get(\"seed\"),\n",
        "            \"temperature\": sampling_args.get(\"temperature\", 0.3),\n",
        "            \"top_p\": sampling_args.get(\"top_p\", 1.0),\n",
        "            \"max_tokens\": sampling_args.get(\"max_tokens\", 4096),\n",
        "        }\n",
        "        if self.model is None:\n",
        "            self.model = DEFAULT_MODEL\n",
        "        if not self.client:\n",
        "            self.client = SamplingClient()\n",
        "        self.client._grok_client.register_model(model=self.model, model_address=self.model)\n",
        "        self.tokenizer = get_tokenizer(\"v4\")\n",
        "        self.timeout = timeout\n",
        "\n",
        "    async def create_contextual_messages(\n",
        "        self,\n",
        "        example: Dict[str, Any],\n",
        "        response: str | None = None,\n",
        "        include_reference: str = \"AB\",\n",
        "        regenerate_rewards: bool = False,\n",
        "        regenerate_intermediate: bool = False,\n",
        "        num_samples: int | None = 1,\n",
        "        model: str | None = None,\n",
        "    ) -> List[dict[str, str]]:\n",
        "        # conversation for SFT history + assistant_message1 + user_rubrics + assistant_r1\n",
        "        msgs = []\n",
        "        msgs = example.get(\"messages\", None)\n",
        "        if \"prompt\" in example and msgs is None:\n",
        "            msgs = formatting.derender_conversation(\n",
        "                name=\"grok\",\n",
        "                rendered_conv=formatting.strip_last_generation_prompt(example[\"prompt\"]),\n",
        "            )\n",
        "        if \"conversation0\" in example and msgs is None:\n",
        "            msgs = example[\"conversation0\"]\n",
        "            if msgs[-1][\"role\"] in [\"assistant\", \"ASSIS\"]:\n",
        "                msgs = msgs[:-1]\n",
        "\n",
        "        model_response = response or example.get(\"response\", None)\n",
        "\n",
        "        conversation = msgs\n",
        "\n",
        "        conv_loss_mask = [False for _ in msgs]\n",
        "\n",
        "        def convert_preference_to_point(preference_score, m):\n",
        "            match preference_score:\n",
        "                case 0:\n",
        "                    return 5 if \"A\" == m else 1\n",
        "                case 0.16:\n",
        "                    return 4 if \"A\" == m else 2\n",
        "                case 0.33:\n",
        "                    return 4 if \"A\" == m else 3\n",
        "                case 0.5:\n",
        "                    return 3\n",
        "                case 0.66:\n",
        "                    return 3 if \"A\" == m else 4\n",
        "                case 0.83:\n",
        "                    return 2 if \"A\" == m else 4\n",
        "                case 1.0:\n",
        "                    return 1 if \"A\" == m else 5\n",
        "\n",
        "        response_rewards_key_zip = []\n",
        "        reward_aspects = {\n",
        "            \"reward_aspects_dict_A\": {},\n",
        "            \"reward_aspects_dict_B\": {},\n",
        "        }\n",
        "        example |= {\n",
        "            \"reward_aspects_details_A\": \"\",\n",
        "            \"reward_aspects_ratings_A\": \"\",\n",
        "            \"reward_aspects_details_B\": \"\",\n",
        "            \"reward_aspects_ratings_B\": \"\",\n",
        "            \"reward_truth_A\": -100,\n",
        "            \"reward_truth_B\": -100,\n",
        "        }\n",
        "\n",
        "        for m in [\"A\", \"B\"]:\n",
        "            for key in DEFAULT_ASPECTS:\n",
        "                if key + f\"_rating_{m}\" in example and example.get(key + f\"_rating_{m}\") >= 0:\n",
        "                    rating = example[key + f\"_rating_{m}\"]\n",
        "                    reward_aspects[f\"reward_aspects_dict_{m}\"].update({key: rating})\n",
        "                    example[f\"reward_aspects_details_{m}\"] += f\"{key}, \"\n",
        "                    example[f\"reward_aspects_ratings_{m}\"] += str(int(rating))\n",
        "\n",
        "            example[f\"reward_truth_{m}\"] = reward_aspects[f\"reward_aspects_dict_{m}\"].get(\n",
        "                \"quality\", None\n",
        "            )\n",
        "            if example[f\"reward_truth_{m}\"] is None:\n",
        "                example[f\"reward_truth_{m}\"] = convert_preference_to_point(\n",
        "                    example[\"preference_overall\"], m\n",
        "                )\n",
        "\n",
        "        for key in include_reference:\n",
        "            idx = 0 if key == \"A\" else 1\n",
        "            response_rewards_key_zip += [\n",
        "                (\n",
        "                    key,\n",
        "                    example[f\"conversation{idx}\"][-1],\n",
        "                    example[f\"reward_truth_{key}\"],\n",
        "                    example[f\"reward_aspects_details_{key}\"],\n",
        "                    example[f\"reward_aspects_ratings_{key}\"],\n",
        "                )\n",
        "            ]\n",
        "        if model_response is not None:\n",
        "            response_rewards_key_zip += [\n",
        "                (\"new\", make_assistant_msg(model_response), None, \",\".join(DEFAULT_ASPECTS), None)\n",
        "            ]  # pad since there is no reward\n",
        "\n",
        "        rubrics = []\n",
        "        conditional_key = \"\"\n",
        "        for key, response_msg, reward, aspects, ratings_per_aspects in response_rewards_key_zip:\n",
        "            if conversation[-1][\"role\"] == \"ASSIS\":\n",
        "                pad_msg = make_user_msg(\"Regenerate response.\")\n",
        "                conversation += [pad_msg]\n",
        "                conv_loss_mask += [False]\n",
        "\n",
        "            conversation += [response_msg]\n",
        "            conv_loss_mask += [False]\n",
        "            if self.rubrics_type != RubricType.NONE and self.rubrics_type != RubricType.ASPECT:\n",
        "                user_rubric_prompt = make_user_msg(RATING_USER_PROMPT_RUBRICS)\n",
        "                conversation += [user_rubric_prompt]\n",
        "                conv_loss_mask += [False]\n",
        "                if (\n",
        "                    self.rubrics_type == RubricType.CRITIC\n",
        "                    or self.rubrics_type == RubricType.CRITIC_ASPECT\n",
        "                ):\n",
        "                    rubrics = example.get(\"critic\", \"\")\n",
        "                    # rubrics = rubrics.replace(\"odel A\", \"odel\").replace(\"odel B\", \"odel\").replace(\"odelA\", \"odel\").replace(\"odelB\", \"odel\")\n",
        "                if self.rubrics_type == RubricType.SYNTHESIS or (\n",
        "                    reward is None and regenerate_intermediate\n",
        "                ):\n",
        "                    rubrics = await self._gen_internal(messages=conversation)\n",
        "                model_rubric_response = make_assistant_msg(rubrics)\n",
        "                conversation += [model_rubric_response]\n",
        "                conv_loss_mask += [self.train_rubrics]\n",
        "            if (\n",
        "                self.rubrics_type == RubricType.ASPECT\n",
        "                or self.rubrics_type == RubricType.CRITIC_ASPECT\n",
        "            ):\n",
        "                if len(ratings_per_aspects) > 0:\n",
        "                    user_rating_per_aspect_prompt = make_user_msg(\n",
        "                        RATING_USER_PROMPT_ASPECT.format(aspects=aspects)\n",
        "                    )\n",
        "                    conversation += [user_rating_per_aspect_prompt]\n",
        "                    conv_loss_mask += [False]\n",
        "                    if ratings_per_aspects and not regenerate_intermediate:\n",
        "                        ratings_per_aspects = \" \".join(\n",
        "                            [RATINGS_KEY[int(r)] for r in ratings_per_aspects]\n",
        "                        )\n",
        "                        model_rating_per_aspect_response = make_assistant_msg(ratings_per_aspects)\n",
        "                    else:\n",
        "                        model_rating_per_aspect_response = self._gen_internal(messages=conversation)\n",
        "                    conversation += [model_rating_per_aspect_response]\n",
        "                    conv_loss_mask += [self.train_rubrics]\n",
        "\n",
        "            user_rating_instruction_msg = make_user_msg(RATING_USER_PROMPT_OVERALL)\n",
        "            conversation += [user_rating_instruction_msg]\n",
        "            conv_loss_mask += [False]\n",
        "            if reward is not None:\n",
        "                if regenerate_rewards:\n",
        "                    pred_reward = await self._gen_reward_samples(\n",
        "                        messages=conversation, model=model, num_samples=num_samples\n",
        "                    )\n",
        "                    # import pdb;pdb.set_trace()\n",
        "                    suffix = (\"_conditional\") if conditional_key else \"_unconditional\"\n",
        "                    conditional_key += key\n",
        "                    example[\"reward_pred_\" + key + suffix] = pred_reward\n",
        "                # Note: we always use the off policy reward\n",
        "                reward_msg = make_assistant_msg(RATINGS_KEY[int(reward)])\n",
        "                conversation += [reward_msg]\n",
        "                conv_loss_mask += [True]\n",
        "\n",
        "        example[\"conversation\"] = conversation\n",
        "        example[\"conv_loss_mask\"] = conv_loss_mask\n",
        "        example[\"rubrics\"] = rubrics\n",
        "        return conversation, conv_loss_mask, rubrics\n",
        "\n",
        "    def _parse_reward(self, response):\n",
        "        response = response.strip()\n",
        "        reward = RATINGS_KEY.index(response)\n",
        "        if reward == -1:\n",
        "            logging.warning(\"Unrecognized rating: %s\", response)\n",
        "            return None\n",
        "        else:\n",
        "            return reward\n",
        "\n",
        "    async def _gen_internal(\n",
        "        self, messages: MESSAGEType, model: str | None = None, max_tokens_override: int = None\n",
        "    ) -> str:\n",
        "        idx = 0\n",
        "        sample_args_override = (\n",
        "            {\"max_tokens\": max_tokens_override} if max_tokens_override is not None else {}\n",
        "        )\n",
        "        sampling_args = self.sampling_args\n",
        "        sampling_args |= sample_args_override\n",
        "        while idx < self.max_retry + 1:\n",
        "            response = await self.client.generate(\n",
        "                model=model or self.model,\n",
        "                messages=messages,\n",
        "                **sampling_args,\n",
        "            )\n",
        "            response = response.strip()\n",
        "            if response is not None:\n",
        "                break\n",
        "            idx += 1\n",
        "        return response\n",
        "\n",
        "    async def _gen_reward(self, messages: MESSAGEType, model: str | None = None) -> float:\n",
        "        reward = None\n",
        "        response = await self._gen_internal(messages=messages, model=model, max_tokens_override=1)\n",
        "        reward = self._parse_reward(response)\n",
        "        return reward\n",
        "\n",
        "    async def _gen_reward_samples(\n",
        "        self, messages: MESSAGEType, model: str | None = None, num_samples: int | None = 1\n",
        "    ) -> float:\n",
        "        reward, rewards = 0, []\n",
        "        try:\n",
        "            outputs = await asyncio.gather(\n",
        "                *[self._gen_reward(messages=messages, model=model) for _ in range(num_samples)]\n",
        "            )\n",
        "            rewards = [r for r in outputs if r is not None]\n",
        "        except Exception as e:\n",
        "            print(\"[Exception] Cannot get the output\", e)\n",
        "            logging.error(e)\n",
        "            reward = None\n",
        "        reward = math.ceil(sum(rewards) / len(rewards)) if len(rewards) > 0 else None\n",
        "        return reward\n",
        "\n",
        "    async def generate_single(\n",
        "        self,\n",
        "        example: Dict[str, Any],\n",
        "        response: str | None = None,\n",
        "        include_reference: str = \"AB\",\n",
        "        regenerate_rewards: bool = True,\n",
        "        regenerate_intermediate: bool = False,\n",
        "        num_samples: int = None,\n",
        "        model: str | None = None,\n",
        "    ) -> dict[str, Any]:\n",
        "        # note if there are response, this is used for prediction, otherwise it is used for training\n",
        "        num_samples = num_samples or self.num_samples\n",
        "        conversation, conv_loss_mask, rubrics = await self.create_contextual_messages(\n",
        "            example=example,\n",
        "            response=response,\n",
        "            include_reference=include_reference,\n",
        "            regenerate_rewards=regenerate_rewards,\n",
        "            regenerate_intermediate=regenerate_intermediate,\n",
        "        )\n",
        "        reward = await self._gen_reward_samples(conversation, model=model, num_samples=num_samples)\n",
        "        return ContextualResult(\n",
        "            rubrics=rubrics,\n",
        "            reward=reward,\n",
        "            conversation=conversation,\n",
        "            conv_loss_mask=conv_loss_mask,\n",
        "            example=example,\n",
        "        )\n",
        "\n",
        "    async def generate_pairs(\n",
        "        self,\n",
        "        example: Dict[str, Any],\n",
        "        num_samples: int = None,\n",
        "        model: str | None = None,\n",
        "    ) -> dict[str, Any]:\n",
        "        _, _, rubrics_ab = await self.create_contextual_messages(\n",
        "            example=example,\n",
        "            response=None,\n",
        "            include_reference=\"AB\",\n",
        "            regenerate_rewards=True,\n",
        "            regenerate_intermediate=False,\n",
        "            num_samples=num_samples,\n",
        "            model=model,\n",
        "        )\n",
        "\n",
        "        _, _, rubrics_ba = await self.create_contextual_messages(\n",
        "            example=example,\n",
        "            response=None,\n",
        "            include_reference=\"BA\",\n",
        "            regenerate_rewards=True,\n",
        "            regenerate_intermediate=False,\n",
        "            num_samples=num_samples,\n",
        "            model=model,\n",
        "        )\n",
        "        reward = {}\n",
        "        reward_truth = {}\n",
        "\n",
        "        for k in example:\n",
        "            if \"reward_pred\" in k:\n",
        "                reward[k] = example[k]\n",
        "            elif \"reward_truth\" in k:\n",
        "                reward_truth[k] = example[k]\n",
        "        reward_A = [reward[k] for k in reward if \"_pred_A\" in k]\n",
        "        reward_A = math.ceil(sum(reward_A) / len(reward_A)) if len(reward_A) > 0 else None\n",
        "        reward_B = [reward[k] for k in reward if \"_pred_B\" in k]\n",
        "        reward_B = math.ceil(sum(reward_B) / len(reward_B)) if len(reward_B) > 0 else None\n",
        "\n",
        "        reward |= {\"reward_pred_A_avg\": reward_A, \"reward_pred_B_avg\": reward_B}\n",
        "        example |= reward\n",
        "\n",
        "        rubrics = list(set((rubrics_ab or []) + (rubrics_ba or [])))\n",
        "        return ContextualPairedResult(\n",
        "            rubrics=rubrics,\n",
        "            reward=reward,\n",
        "            reward_truth=reward_truth,\n",
        "            example=example,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "PiwTnwAoAgcY"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/data/datasets/preferences-v2/raw/renew/surge_finegrain_api_1/part-00000-of-00001.parquet\"\n",
        "data = utils.read_df(data_path)"
      ],
      "metadata": {
        "id": "rdQIO3nGAj3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac71830-5a02-4522-ba03-5cfa6c14c372"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading paths: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = data.iloc[4].to_dict()\n",
        "data1 = {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in data1.items()}\n",
        "# data1"
      ],
      "metadata": {
        "id": "216UMujqBuhw"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = copy.deepcopy(data1)\n",
        "\n",
        "client = SamplingClient()\n",
        "g = ContextualRewardGenerator()\n",
        "g.initialize(client=client, model=DEFAULT_MODEL)\n",
        "g.rubrics_type = RubricType.ASPECT\n",
        "\n",
        "conversation, convo_loss_mask, rubrics = await g.create_contextual_messages(example=example, include_reference=\"B\")\n",
        "# display_conversation(example['conversation'], example[\"conv_loss_mask\"])\n",
        "\n",
        "# print(json.dumps(conversation, indent=2))\n",
        "display_conversation(conversation, convo_loss_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "44vwqKq24L4b",
        "outputId": "526fd683-3abc-42d0-9ca8-a4bbf4d947ff"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-11-08 17:37:24,336 E] \u001b[2;36m[colabbox-0:305999] sampling_client:995:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div style='background-color: #d0e7ff; color: #084298;\n",
              "                    border-radius: 8px; padding: 10px; margin: 8px 0;'>\n",
              "            <strong style='font-weight: normal;'>User:</strong>\n",
              "            <div style='font-weight: normal;'><p>Given the following texts, identify different characteristics used to describe the band they discuss. All of you information should come from and cite the given sources. To cite a source, end a claim that uses information from a source using the format [x,y] where x is the source number and y is the paragraph the information you cite is coming from. You should use the text to describe the band My Chemical Romance and the music that they make. </p>\n",
              "<p>Using markdown, describe the band / their music by directly quoting the sources. Then, create a table that shows what each source has to say about the band. End with a conclusion that summarizes the information in your answer.</p>\n",
              "<p>Source #1</p>\n",
              "<p>Call My Chemical Romance anything you like. Talented. Petulant. Even sell-outs.</p>\n",
              "<p>Just don't call them emo.</p>\n",
              "<p>\"All emo means, to me, is being emotionally disturbed and having really bad hair,\" says guitarist Frank Iero, who, like singer Gerard Way, has been consistently outspoken when it comes to folks not calling My Chemical Romance an emo band.</p>\n",
              "<p>\"We've always wanted to stay away from that term because it simply doesn't apply to us,\" he says.</p>\n",
              "<p>Much to his chagrin, though, the term has been associated with the band ever since its inception back in 2001. And, despite Iero's irritation, when listening to three out of the four albums My Chemical Romance have released, it's difficult to tell exactly why they shouldn't be considered emo. The band's 2006 effort, The Black Parade, is considered by many to be a landmark emo album.</p>\n",
              "<p>\"Thankfully, with our latest record, we have probably heard the last from emo,\" Iero says.</p>\n",
              "<p>The album he's referring to is Danger Days: The True Lives of the Fabulous Killjoys, My Chemical Romance's bold 2010 release. Besides being a great album, Danger Days is remarkable for being such a dramatic break from the doom-filled angst of the band's earlier efforts.</p>\n",
              "<p>\"We couldn't stay stagnant,\" Iero says rather matter-of-factly. \"We just didn't want the same things as we did 10 years ago.\"</p>\n",
              "<p>Yet the change in tactics on Danger Days is more than just a decade of maturity. Songs like the infectious (if annoyingly titled) \"Na Na Na (Na Na Na Na Na Na Na Na Na)\" and the disco-pulsed \"Planetary (Go)\" sound like the work of an entirely different band—and, yes, a better one at that. It's as if Cheap Trick's Robin Zander and Rick Nielsen kidnapped these guys and taught them every power-pop trick in the book.</p>\n",
              "<p>\"The album is not a reaction to our previous records,\" Iero says, albeit somewhat unconvincingly. \"We simply decided to write songs, not an album.\"</p>\n",
              "<p>Indeed, Danger Days does play out like a diverse collection of singles. And, thanks to their embracing the change in sound so enthusiastically, it's difficult to fault the band's sincerity.</p>\n",
              "<p>It's not just the music that is different, either.</p>\n",
              "<p>\"It was time to end the long stares, the bombastic props and get-ups,\" Iero says of My Chemical Romance's updated stage presence. \"That look was part of the reason we got labeled emo in the first place.\"</p>\n",
              "<p>Whatever you call the band, this much is clear: This is a band on the up-and-up, whether the band's fans are ready to embrace the new sound or not.</p>\n",
              "<p>\"There will always be assholes in the crowd wanting to throw change at you,\" Iero says. \"But this new record is a party record. And I think fans old and new will want to party along.\"</p>\n",
              "<p>Source #2</p>\n",
              "<p>You may want to sit down. We have some very upsetting news. My Chemical Romance are dead. And so is their friend ‘The Patient’. There was nothing that anybody could do.</p>\n",
              "<p>Confused? Readers, the confusion hasn’t even started. ‘The Black Parade’ is an ostentatious concept-album-cum-rock-opera about death, and it’s about to turn the late MCR into the biggest band on the planet. The most obvious reference point is Green Day’s ‘American Idiot’, not just because both projects share a record label, producer, video director and an A&amp;R department, but because it’s a piece of work that will challenge every preconception you ever had about the people who made it.</p>\n",
              "<p>To recap, My Chemical Romance’s previous two albums have been one long horror story; ‘I Brought You My Bullets, You Brought Me Your Love’ and ‘Three Cheers For Sweet Revenge’ tells the story of two criminal ‘demolition lovers’ who are gunned down in the desert after their hometown is overrun by vampires. Then the boy makes a deal with Satan for the lives of him and his lover Helena in exchange for the souls of a thousand evil men. ‘A meditation,’ says Gerard Way, ‘on immortality. And this [new album] is a meditation on mortality.’</p>\n",
              "<p>‘The Black Parade’ unfolds in the physical realm; with no eternal zombie afterlife to cushion things when the day comes. Our story begins in the cancer ward (nice), with the tragic, young death of our protagonist, an average Joe known simply as The Patient. And so the first song (after the introduction, called ‘The End’) is called ‘Dead!’ the pre-title sequence, if you will. And straightaway, we’re on brave new ground musically as well ‘a crisp, vast, and fiercely melodic kind of stadium punk that is barely emo in the slightest. Actually, its closest parallel is ‘Holiday’ from ‘American Idiot’. But by getting that out of the way first, they then divert from their Green Day mentors, blasting into power-duo tracks ‘This Is How I Disappear’ and ‘The Sharpest Lives’, both showing off this band’s new richness of depth and texture in glorious 3D-technicolour. As The Patient enters the tunnel of light we hit critical mass with ‘Welcome To The Black Parade’ and things begin to unfold. Gerard apparently believes that when the reaper comes, it comes in the form of your life’s most vivid memory. Now, the point is that The Patient died before he got the chance to really achieve anything, so his most vivid memory is as a child, when his father took him to the city to see a marching band. And the rest of the album soundtracks The Patient’s life flashing before his eyes, and it turns out to be a lot more colourful and varied a flashback than he was expecting. And this is where ‘The Black Parade’s genius reveals itself. MCR have moved so far from the emo sound you know, that they’ve even renamed their band for this album. Under their new guise of The Black Parade, the group have the freedom to adopt whatever clothes, styles or ideas are necessary to tell each chapter. If there is an overarching style it is of all things musical theatre.</p>\n",
              "<p>The first truly staggering moment is ‘I Don’t Love You’ (this album’s ‘Boulevard Of Broken Dreams’), which sounds like Keane would if they had any edge and loads of big rock guitars. ‘House Of Wolves’, debuted at Reading, is the least startling, mainly because it pre-empts ‘Cancer’, the song most likely to get the newspapers’ backs up. A short bridging sequence, it’s their first ever stab at total, pin-drop sensitivity. Just Gerard and a piano, it’s a first-person account of death from cancer that packs an emotional punch so devastating and graphic that you could never call it rock’n’roll ‘ isn’t that meant to be fun? But MCR want to surpass rock’n’roll: it pushes the boundaries of taste about as far as they’ll go, but a daily listen is going to be just an effective way of making people stop smoking as the Allen Carr book. The mood then lightens with ‘Mama’, a vaudeville riot that manages to out-Gogol Bordello Gogol Bordello, made even more overwhelming thanks to Liza Minelli’s showstopping cameo as Mama herself. If things are getting a little too theatrical, then ‘Sleep’ gives us the big body-builder Tool moment and punches their heavy metal card; if only once, because next they have to reinvent Status Quo as Roxy Music on ‘Teenagers’; another amazing boxfresh MTV smash. ‘Disenchanted’ is the calm scene gearing up for a preposterous finale; which comes with ‘Famous Last Words’, where The Black Parade revert back to My Chemical Romance, stepping back into narrator mode, both moral of the story and cinematic climax to this remarkable journey, and a distillation of what they do best; an earnest, impassioned, and hard-fucking-rock anthem which sees them finally become ‘the saviour of the broken, the beaten and the damned’. It’s about making the most of every moment because after all, you never know when the parade (or whatever you’ll see), will come to you. Pure Hollywood melodrama, but if you find the climactic refrain of ‘I am not afraid to keep on living, I am not afraid to walk this world alone’ cheesy, you’re going to have given up sometime around track three anyway. For everyone else, it’s proper, er, lump-in-throat time.</p>\n",
              "<p>There’s one little surprise left, but we laughed so hard that we won’t spoil it for you, except to say that it once and for all disproves the accusation that this band are somehow humourless, as if the album hadn’t done that anyway. ‘The Black Parade’ understands that real tragedy is inherently a bit funny, and vice versa, and it’s that fact which sees them leave the jibbering bodies of a thousand emo jessies in their wake. The truth is they left that baggage behind long ago. If there’s any parallel it’s with somebody like Billy Corgan; not just because of the eerie similarity of Gerard’s new hairdo, or even the marching band thing, but because both people have bigger imaginations than punk rock usually allows. But having been through their problems with drugs and psychosis already, MCR just seem too much of a finely-tuned juggernaut to fuck things up like the Pumpkins. This album has given them the freedom to do anything they want next, but for now this is one to file alongside ‘American Idiot’, ‘Doolittle’ and ‘Nevermind’ on your greatest US rock albums shelf. Oh yes, My Chemical Romance ‘ the moderately camp two-trick outsider’s cult you thought you knew ‘ are dead alright. Long live My Chemical Romance, the outrageously-camp, loud and righteous new kings of the world. Those bottle-chucking haters are just going to have to get used to it.</p>\n",
              "<p>Source #3</p>\n",
              "<p>In the 1980s heyday of New Jersey rock, the Garden State's favorite frontmen made videos featuring them posing ironically in front of American flags, masquerading as toothsome cowboys, pulling young girls onstage and soaring high above crowds during sold-out arena tours. In short, it was pretty much all good times.</p>\n",
              "<p>It's funny how things have changed.</p>\n",
              "<p>\"When I first read the treatment for the new video, I got emotional. I realized it was going to be very sad,\" Gerard Way, lead singer of Jersey's goth-punk quintet My Chemical Romance explained. \"The song is about my grandmother, who passed away. She taught me how to sing and paint and how to perform. She was an artist, and she pushed me to be an artist. We were really close, so making the video was really good closure for me, personally. It's one of those things where I knew I was going to have to face my fears.\"</p>\n",
              "<p>The song Way is referring to is \"Helena,\" the shattering and shredded lead track on MCR's breakthrough album, Three Cheers for Sweet Revenge. It's also the second single off the record — following in the footsteps of \"I'm Not Okay (I Promise),\" which, powered by a \"Rushmore\"-inspired clip, garnered major airplay on MTV2 and mtvU and earned the band a whole bunch of new fans.</p>\n",
              "<p>Not too shabby for the group's first effort.</p>\n",
              "<p>\"There's pressure, but we know that the video for 'Helena' is our chance to be known as a 'video' band. We could be like the Smashing Pumpkins, a band that always made these movies instead of just a bunch of guys in a basement,\" Way said. \"I think it's going to be very different from 'I'm Not Okay.' It's not funny at all. The best way to describe the video is 'very sad and celebratory, upsetting and uplifting at the same time.' It was a risk, but we've always taken risks. And this video is the biggest risk we've ever taken.\"</p>\n",
              "<p>The clip, helmed by Marc Webb (who directed \"I'm Not Okay\"), takes place in a rickety church and features the band presiding over the funeral of a young girl. The mourners are played entirely by MCR fans, who applied to be extras via the band's Web site. There's plenty of black suits and eyeliner and Gothic-inspired cobwebby umbrellas. Oh, and interpretive dance, too.</p>\n",
              "<p>Source #4</p>\n",
              "<p>It’s both fascinating and kind of terrifying to imagine the bedroom of a teenage Gerard Way, the pasty, fiendish frontman of My Chemical Romance. The heaps of racy comic books, posters of Bowie and Queen fighting for wall space with Slayer and Danzig. The De Palma and Rocky Horror DVDs piled up next to original cast recordings of Carousel and Sweeney Todd. The nail polish. The action figures. The lack of action. All the cultural trappings of a sensitive, undersize kid aching to escape the grim New Jersey suburbs. And if the million-selling success of 2004’s Three Cheers for Sweet Revenge was a first shot heard round the underworld, The Black Parade is Way turning those bedroom walls spectacularly inside out.</p>\n",
              "<p>With an ear for the epic, an eye for the dramatic, and an eyelash for the mascara, Way and his not-so-merry band have already rewritten the rule book for emo success. Instead of celebrating broken hearts, Way prefers theatrical open-heart surgery, stitching together the hyperactive unhappiness of his followers and, in the process, creating as much of a movement as possible in this boggy, bloggy musical moment.</p>\n",
              "<p>And on their third album, My Chemical Romance give in completely to their inner theater geeks: It’s a savage, heartfelt, at times hilarious goth-mosh emopera featuring strings, horns, a marching band, snarling obscenities, tender reflections on mortality, and a special guest, referred to by everyone who hears this tidbit of info as “Liza Fucking Minnelli” (who croaks a chorus on “Mama”).</p>\n",
              "<p>The Black Parade is a dark concept album about an unnamed “Patient” who seems to die on the country-ish first track (“The End.”), only to awaken in a Technicolor, rockabilly Busby Berkeley musical (the outrageously peppy and profane “Dead!”). On the album’s multipart centerpiece and first single, “Welcome to the Black Parade,” Way is still gleefully fetishizing death and vampires, but traveling the world and getting sober have given him a new sense of mission — and hope. Amid trademark screamers “This Is How I Disappear” and “House of Wolves,” positive refrains like “I am not afraid to keep on living” emerge. Way truly sees himself as the ringleader for a generation stuck with their middle fingers up and their heads down. On the glam-hammy “Teenagers,” he seems to morph into an authoritarian military recruiter who smugly drafts kids into “the murder machine,” and then bursts into the giddy chorus: “Teenagers scare the living shit out of me!”</p>\n",
              "<p>In Way’s world, fear of difference, of sadness, of gloriously cheese-ball choruses is a fate worse than fatality. On the tremendous closer, “Famous Last Words,” he howls, “Awake and unafraid!” — recasting youthful misery as something bright and beautiful. More a raven than a butterfly, he is soaring nonetheless.</p></div>\n",
              "        </div>\n",
              "        \n",
              "        <div style='background-color: #d4edda; color: #155724;\n",
              "                    border-radius: 8px; padding: 10px; margin: 8px 0;'>\n",
              "            <strong style='font-weight: normal;'>Assistant:</strong>\n",
              "            <div style='font-weight: normal;'><h3>Direct Quotes Describing My Chemical Romance:</h3>\n",
              "<ol>\n",
              "<li><strong>Source #1</strong>: </li>\n",
              "<li>\"Just don't call them emo.\" [1,3]</li>\n",
              "<li>\"We've always wanted to stay away from that term because it simply doesn't apply to us.\" [1,4]</li>\n",
              "<li>\"Danger Days is remarkable for being such a dramatic break from the doom-filled angst of the band's earlier efforts.\" [1,7]</li>\n",
              "<li>\n",
              "<p>\"The album is not a reaction to our previous records... We simply decided to write songs, not an album.\" [1,9]</p>\n",
              "</li>\n",
              "<li>\n",
              "<p><strong>Source #2</strong>: </p>\n",
              "</li>\n",
              "<li>\"The Black Parade is an ostentatious concept-album-cum-rock-opera about death.\" [2,2]</li>\n",
              "<li>\"MCR have moved so far from the emo sound you know, that they’ve even renamed their band for this album.\" [2,5]</li>\n",
              "<li>\n",
              "<p>\"This album has given them the freedom to do anything they want next.\" [2,12]</p>\n",
              "</li>\n",
              "<li>\n",
              "<p><strong>Source #3</strong>: </p>\n",
              "</li>\n",
              "<li>\"Jersey's goth-punk quintet My Chemical Romance.\" [3,2]</li>\n",
              "<li>\"The video for 'Helena' is our chance to be known as a 'video' band.\" [3,5]</li>\n",
              "<li>\n",
              "<p>\"The song is about my grandmother, who passed away.\" [3,3]</p>\n",
              "</li>\n",
              "<li>\n",
              "<p><strong>Source #4</strong>: </p>\n",
              "</li>\n",
              "<li>\"My Chemical Romance give in completely to their inner theater geeks.\" [4,3]</li>\n",
              "<li>\"It’s a savage, heartfelt, at times hilarious goth-mosh emopera.\" [4,3]</li>\n",
              "<li>\"Way truly sees himself as the ringleader for a generation stuck with their middle fingers up and their heads down.\" [4,6]</li>\n",
              "</ol>\n",
              "<h3>Table of Characteristics from Each Source:</h3>\n",
              "<table>\n",
              "<thead>\n",
              "<tr>\n",
              "<th><strong>Source</strong></th>\n",
              "<th><strong>Characteristics Described</strong></th>\n",
              "</tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr>\n",
              "<td>1</td>\n",
              "<td>Not Emo, Musical Evolution, Bold New Sound, Diverse Singles, Less Theatrical</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>2</td>\n",
              "<td>Concept Album, Rock Opera, Musical Theatre Influence, Departure from Emo, Ambitious and Varied Soundscapes</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>3</td>\n",
              "<td>Emotional Depth, Visual Storytelling, Gothic Aesthetic, Connection to Personal Loss</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>4</td>\n",
              "<td>Theatrical, Concept-Driven, Embraces Drama and Epic Narratives, Positive Message Amidst Dark Themes</td>\n",
              "</tr>\n",
              "</tbody>\n",
              "</table>\n",
              "<h3>Conclusion:</h3>\n",
              "<p>My Chemical Romance has consistently defied easy categorization, evolving from what might have been initially mislabeled as \"emo\" to a band known for their dramatic, theatrical, and conceptual approach to music. Their albums like \"The Black Parade\" showcase a rich tapestry of influences from punk to goth, infused with a narrative depth that explores themes of mortality, loss, and defiance. Their later work, particularly \"Danger Days,\" marks a significant shift towards a more diverse, pop-influenced sound while still retaining an edge that distinguishes them. Across sources, it's evident that MCR's music is characterized by its emotional intensity, artistic risks, and a flair for the dramatic, making them a standout act not just within the rock genre but in the broader music scene. They embrace change, challenge expectations, and deliver messages of resilience and hope amidst dark lyrical content, solidifying their status as influential musicians with a broad creative scope.</p></div>\n",
              "        </div>\n",
              "        \n",
              "        <div style='background-color: #d0e7ff; color: #084298;\n",
              "                    border-radius: 8px; padding: 10px; margin: 8px 0;'>\n",
              "            <strong style='font-weight: normal;'>User:</strong>\n",
              "            <div style='font-weight: normal;'><p>Please rate the previous assistant response using one of the following options, listed from highest to lowest quality: A-Great, B-Good, C-OK, D-Bad, E-Terrible. For each aspect in quality, instruction following,  provide only the rating as a single letter.</p></div>\n",
              "        </div>\n",
              "        \n",
              "        <div style='background-color: #d4edda; color: #155724;\n",
              "                    border-radius: 8px; padding: 10px; margin: 8px 0;'>\n",
              "            <strong style='font-weight: bold; text-decoration: underline;'>Assistant:</strong>\n",
              "            <div style='font-weight: bold; text-decoration: underline;'><p>D B</p></div>\n",
              "        </div>\n",
              "        \n",
              "        <div style='background-color: #d0e7ff; color: #084298;\n",
              "                    border-radius: 8px; padding: 10px; margin: 8px 0;'>\n",
              "            <strong style='font-weight: normal;'>User:</strong>\n",
              "            <div style='font-weight: normal;'><p>Please rate the previous assistant response using one of the following options, listed from highest to lowest quality: A-Great, B-Good, C-OK, D-Bad, E-Terrible. Provide only the rating as a single letter.</p></div>\n",
              "        </div>\n",
              "        \n",
              "        <div style='background-color: #d4edda; color: #155724;\n",
              "                    border-radius: 8px; padding: 10px; margin: 8px 0;'>\n",
              "            <strong style='font-weight: bold; text-decoration: underline;'>Assistant:</strong>\n",
              "            <div style='font-weight: bold; text-decoration: underline;'><p>D</p></div>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(output['reward'])\n",
        "# # display_conversation(output['conversation'], output['conv_loss_mask'])"
      ],
      "metadata": {
        "id": "5ssgREPt9ZaQ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = SamplingClient()\n",
        "g = ContextualRewardGenerator()\n",
        "g.initialize(client=client, model=\"v5l-1107-crm-pack1none.yuchen.svc.max.x.ai\")\n",
        "g.rubrics_type = RubricType.NONE\n",
        "example = copy.deepcopy(data1)\n",
        "# output = await g.generate_pairs(example=example)\n",
        "output = await g.generate_single(example=data1, response=\"you\", regenerate_intermediate=True, include_reference=\"A\")\n",
        "output[\"reward\"]"
      ],
      "metadata": {
        "id": "fcKrTPhK95MR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7db84f5-2d3e-44fd-b700-bd01cc4a3f33"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m[2024-11-08 17:38:17,238 E] \u001b[2;36m[colabbox-0:305999] sampling_client:995:\u001b[0m bedrock setup failed: The config profile (key0) could not be found\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[\"reward\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3c8weF3rlge",
        "outputId": "6cae4620-c22e-403f-9ee6-5606de60ed63"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wVWSVTmB7gQ"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output['preference_correctness']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLA0vKDrEWfN",
        "outputId": "62167900-f366-4b9b-c7c6-d54746d71499"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\"baseline_eval_org\" :{\n",
        "  \"count\": 179,\n",
        "  \"correctness\": {\n",
        "    \"correctness_A_avg\": 0.2905027932960894,\n",
        "    \"correctness_B_avg\": 0.3854748603351955,\n",
        "    \"correctness_A_unconditional\": 0.3016759776536313,\n",
        "    \"correctness_B_unconditional\": 0.3128491620111732,\n",
        "    \"correctness_A_conditional\": 0.26256983240223464,\n",
        "    \"correctness_B_conditional\": 0.37988826815642457\n",
        "  },\n",
        "  \"consistency\": {\n",
        "    \"consistency_truth_by_pre_reward\": 1.0,\n",
        "    \"consistency_avg\": 0.0893854748603352,\n",
        "    \"consistency_unconditional\": 0.061452513966480445,\n",
        "    \"consistency_conditional\": 0.11173184357541899\n",
        "  }\n",
        "},\n",
        "\"sft_org_eval_org\" :{\n",
        "  \"count\": 179,\n",
        "  \"correctness\": {\n",
        "    \"correctness_A_avg\": 0.4748603351955307,\n",
        "    \"correctness_B_avg\": 0.4245810055865922,\n",
        "    \"correctness_A_unconditional\": 0.24022346368715083,\n",
        "    \"correctness_B_unconditional\": 0.2905027932960894,\n",
        "    \"correctness_A_conditional\": 0.553072625698324,\n",
        "    \"correctness_B_conditional\": 0.4692737430167598\n",
        "  },\n",
        "  \"consistency\": {\n",
        "    \"consistency_truth_by_pre_reward\": 1.0,\n",
        "    \"consistency_avg\": 0.5810055865921788,\n",
        "    \"consistency_unconditional\": 0.2011173184357542,\n",
        "    \"consistency_conditional\": 0.6256983240223464\n",
        "  }\n",
        "},\n",
        "\"sft_org_eval_critic\" : {\n",
        "  \"count\": 179,\n",
        "  \"correctness\": {\n",
        "    \"correctness_A_avg\": 0.21787709497206703,\n",
        "    \"correctness_B_avg\": 0.17318435754189945,\n",
        "    \"correctness_A_unconditional\": 0.17318435754189945,\n",
        "    \"correctness_B_unconditional\": 0.1452513966480447,\n",
        "    \"correctness_A_conditional\": 0.33519553072625696,\n",
        "    \"correctness_B_conditional\": 0.30726256983240224\n",
        "  },\n",
        "  \"consistency\": {\n",
        "    \"consistency_truth_by_pre_reward\": 1.0,\n",
        "    \"consistency_avg\": 0.35195530726256985,\n",
        "    \"consistency_unconditional\": 0.3016759776536313,\n",
        "    \"consistency_conditional\": 0.3743016759776536\n",
        "  }\n",
        "},\n",
        "\"sft_sample_permut2_eval_org\" : {\n",
        "  \"count\": 179,\n",
        "  \"correctness\": {\n",
        "    \"correctness_A_avg\": 0.46368715083798884,\n",
        "    \"correctness_B_avg\": 0.39664804469273746,\n",
        "    \"correctness_A_unconditional\": 0.2681564245810056,\n",
        "    \"correctness_B_unconditional\": 0.26256983240223464,\n",
        "    \"correctness_A_conditional\": 0.547486033519553,\n",
        "    \"correctness_B_conditional\": 0.44692737430167595\n",
        "  },\n",
        "  \"consistency\": {\n",
        "    \"consistency_truth_by_pre_reward\": 1.0,\n",
        "    \"consistency_avg\": 0.5810055865921788,\n",
        "    \"consistency_unconditional\": 0.2011173184357542,\n",
        "    \"consistency_conditional\": 0.6145251396648045\n",
        "  }\n",
        "},\n",
        "\"sft_critic_eval_org\": {\n",
        "  \"count\": 179,\n",
        "  \"correctness\": {\n",
        "    \"correctness_A_avg\": 0.4301675977653631,\n",
        "    \"correctness_B_avg\": 0.4245810055865922,\n",
        "    \"correctness_A_unconditional\": 0.22346368715083798,\n",
        "    \"correctness_B_unconditional\": 0.2849162011173184,\n",
        "    \"correctness_A_conditional\": 0.4860335195530726,\n",
        "    \"correctness_B_conditional\": 0.4301675977653631\n",
        "  },\n",
        "  \"consistency\": {\n",
        "    \"consistency_truth_by_pre_reward\": 1.0,\n",
        "    \"consistency_avg\": 0.44692737430167595,\n",
        "    \"consistency_unconditional\": 0.18435754189944134,\n",
        "    \"consistency_conditional\": 0.4301675977653631\n",
        "  }\n",
        "},\n",
        "\"sft_critic_eval_critic\": {\n",
        "  \"count\": 179,\n",
        "  \"correctness\": {\n",
        "    \"correctness_A_avg\": 0.4692737430167598,\n",
        "    \"correctness_B_avg\": 0.4134078212290503,\n",
        "    \"correctness_A_unconditional\": 0.329608938547486,\n",
        "    \"correctness_B_unconditional\": 0.29608938547486036,\n",
        "    \"correctness_A_conditional\": 0.5865921787709497,\n",
        "    \"correctness_B_conditional\": 0.5754189944134078\n",
        "  },\n",
        "  \"consistency\": {\n",
        "    \"consistency_truth_by_pre_reward\": 1.0,\n",
        "    \"consistency_avg\": 0.664804469273743,\n",
        "    \"consistency_unconditional\": 0.4581005586592179,\n",
        "    \"consistency_conditional\": 0.7206703910614525\n",
        "  }\n",
        "},\n",
        "}\n"
      ],
      "metadata": {
        "id": "oeUVTtrcE-Jg"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data extraction\n",
        "categories = ['baseline_eval_org', 'sft_org_eval_org', 'sft_sample_permut2_eval_org',  'sft_critic_eval_critic',  ]\n",
        "consistency_data = {\n",
        "    'avg': [round(results[cat]['consistency']['consistency_avg'], 2) for cat in categories],\n",
        "    'unconditional': [round(results[cat]['consistency']['consistency_unconditional'], 2) for cat in categories],\n",
        "    'conditional': [round(results[cat]['consistency']['consistency_conditional'], 2) for cat in categories]\n",
        "}\n",
        "correctness_data = {\n",
        "    'avg': [\n",
        "        round((results[cat]['correctness']['correctness_A_avg'] + results[cat]['correctness']['correctness_B_avg']) / 2, 2)\n",
        "        for cat in categories\n",
        "    ],\n",
        "    'unconditional': [\n",
        "        round((results[cat]['correctness']['correctness_A_unconditional'] + results[cat]['correctness']['correctness_B_unconditional']) / 2, 2)\n",
        "        for cat in categories\n",
        "    ],\n",
        "    'conditional': [\n",
        "        round((results[cat]['correctness']['correctness_A_conditional'] + results[cat]['correctness']['correctness_B_conditional']) / 2, 2)\n",
        "        for cat in categories\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Setting up the figures\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Width of a bar\n",
        "width = 0.25\n",
        "\n",
        "# Set position of bar on X axis\n",
        "r1 = np.arange(len(categories))\n",
        "r2 = [x + width for x in r1]\n",
        "r3 = [x + width for x in r2]\n",
        "\n",
        "# Softer colors for bars\n",
        "colors = ['#99CCFF', '#99FF99', '#FFCC99']\n",
        "\n",
        "# Plotting consistency data\n",
        "# ax1.bar(r1, consistency_data['avg'], color=colors[0], width=width, edgecolor='white', label='Avg')\n",
        "ax1.bar(r2, consistency_data['unconditional'], color=colors[1], width=width, edgecolor='white', label='Unconditional')\n",
        "ax1.bar(r3, consistency_data['conditional'], color=colors[2], width=width, edgecolor='white', label='Conditional')\n",
        "\n",
        "# Adding value labels on top of each bar\n",
        "# for i, v in enumerate(consistency_data['avg']):\n",
        "#     ax1.text(r1[i], v, str(v), ha='center', va='bottom')\n",
        "for i, v in enumerate(consistency_data['unconditional']):\n",
        "    ax1.text(r2[i], v, str(v), ha='center', va='bottom')\n",
        "for i, v in enumerate(consistency_data['conditional']):\n",
        "    ax1.text(r3[i], v, str(v), ha='center', va='bottom')\n",
        "\n",
        "ax1.set_xlabel('Categories', fontweight='bold')\n",
        "ax1.set_ylabel('Consistency Score')\n",
        "ax1.set_title('Consistency Scores by Category', fontweight='bold')\n",
        "ax1.set_xticks([r + width for r in range(len(categories))])\n",
        "ax1.set_xticklabels(categories)\n",
        "ax1.set_ylim(0, 1)  # Setting y-axis limit to [0, 1]\n",
        "ax1.legend()\n",
        "\n",
        "# Plotting correctness data\n",
        "# ax2.bar(r1, correctness_data['avg'], color=colors[0], width=width, edgecolor='white', label='Avg')\n",
        "ax2.bar(r2, correctness_data['unconditional'], color=colors[1], width=width, edgecolor='white', label='Unconditional')\n",
        "ax2.bar(r3, correctness_data['conditional'], color=colors[2], width=width, edgecolor='white', label='Conditional')\n",
        "\n",
        "# Adding value labels on top of each bar\n",
        "# for i, v in enumerate(correctness_data['avg']):\n",
        "#     ax2.text(r1[i], v, str(v), ha='center', va='bottom')\n",
        "for i, v in enumerate(correctness_data['unconditional']):\n",
        "    ax2.text(r2[i], v, str(v), ha='center', va='bottom')\n",
        "for i, v in enumerate(correctness_data['conditional']):\n",
        "    ax2.text(r3[i], v, str(v), ha='center', va='bottom')\n",
        "\n",
        "ax2.set_xlabel('Categories', fontweight='bold')\n",
        "ax2.set_ylabel('Correctness Score')\n",
        "ax2.set_title('Correctness Scores by Category', fontweight='bold')\n",
        "ax2.set_xticks([r + width for r in range(len(categories))])\n",
        "ax2.set_xticklabels(categories)\n",
        "ax2.set_ylim(0, 1)  # Setting y-axis limit to [0, 1]\n",
        "ax2.legend()\n",
        "\n",
        "# Adjusting the layout and displaying the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "Cp_gOnXPP98w",
        "outputId": "3fa4598e-ad0c-47db-ee0b-ddbebc460f5f"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x800 with 2 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect GIF eval results"
      ],
      "metadata": {
        "id": "wkC8geNloScv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_path = \"/data/evals/yuchen/gifeval/raw/pmix-sft-27k.yuchen.svc.max.x.ai-k1/2024-11-01-220552/metrics.json\" # rfilter 274k\n",
        "# data_path = \"/data/evals/yuchen/gifeval/raw/pmix-org-19k.yuchen.svc.max.x.ai-k1/2024-11-01-220551/metrics.json\" # org 192k\n",
        "data_path = \"/data/evals/yuchen/gifeval/raw/pmix-org-16k.yuchen.svc.max.x.ai-k1/2024-11-01-193317/metrics.json\" # org 160k\n",
        "data_path = \"/data/evals/yuchen/gifeval/raw/pmix-sft-16k.yuchen.svc.max.x.ai-k1/2024-11-01-193308/metrics.json\" # rfilter 160k\n",
        "\n",
        "data_path = \"/data/evals/yuchen/gifeval/raw/npmix-org-288k.yuchen.svc.max.x.ai/2024-11-06-025943/metrics.json\" # cleansft org 288k\n",
        "# data_path = \"/data/evals/yuchen/gifeval/raw/npmix-rfilter-192k.yuchen.svc.max.x.ai-k1/2024-11-06-091027/metrics.json\" # cleansft rfilter 160k\n",
        "result = json.load(open(data_path, \"r\"))\n",
        "flatten_output = {\n",
        "    by_k + \"_\" + win_rate_metric: {\n",
        "      k + \"_\" + win_rate_metric : float([vv for vk, vv in v.items() if win_rate_metric in vk][0].split(\" (\")[0])\n",
        "      for k, v in result[by_k].items()\n",
        "    }\n",
        "    for by_k in result\n",
        "    for win_rate_metric in [\"win_rate_by_label\", \"win_rate_by_score\"]\n",
        "\n",
        "}\n",
        "flatten_output\n",
        "df = pd.DataFrame(flatten_output)\n",
        "df_flattened = df.stack().reset_index(level=1, drop=True).rename('Value')\n",
        "df_flattened"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVudffOwoUuK",
        "outputId": "b37180ee-c0d7-401e-ca4f-9477c861695f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Advice_win_rate_by_label            83.56\n",
              "Brainstorming_win_rate_by_label     63.69\n",
              "Classification_win_rate_by_label    65.10\n",
              "Closed QA_win_rate_by_label         59.65\n",
              "Conversational_win_rate_by_label    51.45\n",
              "Extraction_win_rate_by_label        59.48\n",
              "Modification_win_rate_by_label      78.57\n",
              "Open QA_win_rate_by_label           68.69\n",
              "Other_win_rate_by_label             61.13\n",
              "Persona_win_rate_by_label           71.43\n",
              "Retrieval_win_rate_by_label         90.48\n",
              "Rewriting_win_rate_by_label         64.39\n",
              "Summarization_win_rate_by_label     59.98\n",
              "Writing_win_rate_by_label           65.05\n",
              "Advice_win_rate_by_score            72.59\n",
              "Brainstorming_win_rate_by_score     71.85\n",
              "Classification_win_rate_by_score    63.03\n",
              "Closed QA_win_rate_by_score         57.04\n",
              "Conversational_win_rate_by_score    56.67\n",
              "Extraction_win_rate_by_score        56.36\n",
              "Modification_win_rate_by_score      76.67\n",
              "Open QA_win_rate_by_score           67.88\n",
              "Other_win_rate_by_score             65.93\n",
              "Persona_win_rate_by_score           61.11\n",
              "Retrieval_win_rate_by_score         86.67\n",
              "Rewriting_win_rate_by_score         71.85\n",
              "Summarization_win_rate_by_score     60.00\n",
              "Writing_win_rate_by_score           77.78\n",
              "scale_win_rate_by_label             80.21\n",
              "surge_win_rate_by_label             71.01\n",
              "tutor_win_rate_by_label             60.68\n",
              "overall_win_rate_by_label           70.63\n",
              "scale_win_rate_by_score             80.67\n",
              "surge_win_rate_by_score             70.00\n",
              "tutor_win_rate_by_score             62.22\n",
              "overall_win_rate_by_score           70.96\n",
              "Name: Value, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyX4qWQjogng",
        "outputId": "bb3df148-8e02-4cda-f72b-8487abcafc0d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Advice': {'accuracy_prompt_level': '40.0 (±3.14)',\n",
              "  'accuracy_rubric_level': '72.75 (±0.2)',\n",
              "  'score_prompt_level': '67.73 (±0.35)',\n",
              "  'score_rubric_level': '66.82 (±0.2)',\n",
              "  'win_rate_by_label (exclude tie)': '84.35 (±4.72)',\n",
              "  'wintie_rate_by_label': '91.11 (±3.14)',\n",
              "  'win_rate_by_score': '69.63 (±2.77)',\n",
              "  '# of examples': 45.0},\n",
              " 'Brainstorming': {'accuracy_prompt_level': '25.19 (±4.57)',\n",
              "  'accuracy_rubric_level': '69.25 (±0.22)',\n",
              "  'score_prompt_level': '65.73 (±0.73)',\n",
              "  'score_rubric_level': '66.02 (±0.43)',\n",
              "  'win_rate_by_label (exclude tie)': '49.85 (±4.72)',\n",
              "  'wintie_rate_by_label': '71.11 (±1.81)',\n",
              "  'win_rate_by_score': '51.85 (±1.05)',\n",
              "  '# of examples': 45.0},\n",
              " 'Classification': {'accuracy_prompt_level': '16.97 (±0.86)',\n",
              "  'accuracy_rubric_level': '66.26 (±0.61)',\n",
              "  'score_prompt_level': '60.83 (±0.46)',\n",
              "  'score_rubric_level': '63.46 (±0.61)',\n",
              "  'win_rate_by_label (exclude tie)': '55.25 (±3.01)',\n",
              "  'wintie_rate_by_label': '68.48 (±3.09)',\n",
              "  'win_rate_by_score': '53.94 (±3.09)',\n",
              "  '# of examples': 55.0},\n",
              " 'Closed QA': {'accuracy_prompt_level': '27.41 (±1.05)',\n",
              "  'accuracy_rubric_level': '65.0 (±0.59)',\n",
              "  'score_prompt_level': '62.19 (±0.86)',\n",
              "  'score_rubric_level': '61.48 (±1.0)',\n",
              "  'win_rate_by_label (exclude tie)': '52.91 (±5.88)',\n",
              "  'wintie_rate_by_label': '79.26 (±3.78)',\n",
              "  'win_rate_by_score': '54.81 (±5.54)',\n",
              "  '# of examples': 45.0},\n",
              " 'Conversational': {'accuracy_prompt_level': '43.33 (±5.44)',\n",
              "  'accuracy_rubric_level': '80.48 (±1.89)',\n",
              "  'score_prompt_level': '73.02 (±1.08)',\n",
              "  'score_rubric_level': '75.0 (±0.5)',\n",
              "  'win_rate_by_label (exclude tie)': '46.5 (±9.26)',\n",
              "  'wintie_rate_by_label': '77.78 (±5.67)',\n",
              "  'win_rate_by_score': '44.44 (±3.14)',\n",
              "  '# of examples': 30.0},\n",
              " 'Extraction': {'accuracy_prompt_level': '15.15 (±1.71)',\n",
              "  'accuracy_rubric_level': '65.78 (±0.76)',\n",
              "  'score_prompt_level': '59.19 (±1.05)',\n",
              "  'score_rubric_level': '64.1 (±0.72)',\n",
              "  'win_rate_by_label (exclude tie)': '60.02 (±4.71)',\n",
              "  'wintie_rate_by_label': '76.36 (±3.93)',\n",
              "  'win_rate_by_score': '52.73 (±1.48)',\n",
              "  '# of examples': 55.0},\n",
              " 'Modification': {'accuracy_prompt_level': '50.0 (±8.16)',\n",
              "  'accuracy_rubric_level': '95.2 (±0.71)',\n",
              "  'score_prompt_level': '85.2 (±0.11)',\n",
              "  'score_rubric_level': '84.54 (±0.09)',\n",
              "  'win_rate_by_label (exclude tie)': '87.83 (±9.19)',\n",
              "  'wintie_rate_by_label': '90.0 (±8.16)',\n",
              "  'win_rate_by_score': '90.0 (±8.16)',\n",
              "  '# of examples': 10.0},\n",
              " 'Open QA': {'accuracy_prompt_level': '26.67 (±2.27)',\n",
              "  'accuracy_rubric_level': '74.93 (±0.57)',\n",
              "  'score_prompt_level': '67.45 (±0.67)',\n",
              "  'score_rubric_level': '70.41 (±0.83)',\n",
              "  'win_rate_by_label (exclude tie)': '54.06 (±1.34)',\n",
              "  'wintie_rate_by_label': '76.97 (±2.27)',\n",
              "  'win_rate_by_score': '54.55 (±0.0)',\n",
              "  '# of examples': 55.0},\n",
              " 'Other': {'accuracy_prompt_level': '21.48 (±1.05)',\n",
              "  'accuracy_rubric_level': '68.95 (±1.59)',\n",
              "  'score_prompt_level': '61.92 (±1.11)',\n",
              "  'score_rubric_level': '66.17 (±0.91)',\n",
              "  'win_rate_by_label (exclude tie)': '53.18 (±5.97)',\n",
              "  'wintie_rate_by_label': '71.85 (±4.57)',\n",
              "  'win_rate_by_score': '54.07 (±2.77)',\n",
              "  '# of examples': 45.0},\n",
              " 'Persona': {'accuracy_prompt_level': '42.22 (±3.14)',\n",
              "  'accuracy_rubric_level': '85.19 (±0.55)',\n",
              "  'score_prompt_level': '77.23 (±0.86)',\n",
              "  'score_rubric_level': '79.33 (±0.32)',\n",
              "  'win_rate_by_label (exclude tie)': '69.19 (±3.17)',\n",
              "  'wintie_rate_by_label': '84.44 (±3.14)',\n",
              "  'win_rate_by_score': '57.78 (±3.14)',\n",
              "  '# of examples': 30.0},\n",
              " 'Retrieval': {'accuracy_prompt_level': '53.33 (±4.71)',\n",
              "  'accuracy_rubric_level': '93.17 (±0.39)',\n",
              "  'score_prompt_level': '85.86 (±0.46)',\n",
              "  'score_rubric_level': '85.24 (±0.34)',\n",
              "  'win_rate_by_label (exclude tie)': '94.44 (±7.86)',\n",
              "  'wintie_rate_by_label': '96.67 (±4.71)',\n",
              "  'win_rate_by_score': '70.0 (±8.16)',\n",
              "  '# of examples': 10.0},\n",
              " 'Rewriting': {'accuracy_prompt_level': '33.33 (±3.63)',\n",
              "  'accuracy_rubric_level': '74.81 (±1.62)',\n",
              "  'score_prompt_level': '68.38 (±0.86)',\n",
              "  'score_rubric_level': '68.95 (±0.75)',\n",
              "  'win_rate_by_label (exclude tie)': '58.46 (±4.51)',\n",
              "  'wintie_rate_by_label': '77.04 (±2.1)',\n",
              "  'win_rate_by_score': '68.89 (±6.29)',\n",
              "  '# of examples': 45.0},\n",
              " 'Summarization': {'accuracy_prompt_level': '19.26 (±2.1)',\n",
              "  'accuracy_rubric_level': '73.57 (±1.46)',\n",
              "  'score_prompt_level': '68.06 (±0.4)',\n",
              "  'score_rubric_level': '68.93 (±0.67)',\n",
              "  'win_rate_by_label (exclude tie)': '47.61 (±6.13)',\n",
              "  'wintie_rate_by_label': '75.56 (±3.14)',\n",
              "  'win_rate_by_score': '54.81 (±1.05)',\n",
              "  '# of examples': 45.0},\n",
              " 'Writing': {'accuracy_prompt_level': '30.37 (±4.19)',\n",
              "  'accuracy_rubric_level': '82.97 (±0.96)',\n",
              "  'score_prompt_level': '76.67 (±0.56)',\n",
              "  'score_rubric_level': '77.87 (±0.3)',\n",
              "  'win_rate_by_label (exclude tie)': '51.24 (±7.06)',\n",
              "  'wintie_rate_by_label': '78.52 (±4.19)',\n",
              "  'win_rate_by_score': '60.74 (±6.37)',\n",
              "  '# of examples': 45.0}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9k7Bkc4apabv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}